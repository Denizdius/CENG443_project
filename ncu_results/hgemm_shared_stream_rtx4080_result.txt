==PROF== Connected to process 6682 (/home/deniz/Documents/CENG443_project/src/hgemm_2)
==PROF== Profiling "hgemm_normal" - 0: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_tensor_core" - 1: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 2: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 3: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 4: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 5: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 6: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 7: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 8: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 9: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 10: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_tensor_core" - 11: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 12: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_tensor_core" - 13: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 14: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_tensor_core" - 15: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 16: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_tensor_core" - 17: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 18: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 19: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 20: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_tensor_core" - 21: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 22: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_tensor_core" - 23: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 24: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 25: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 26: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 27: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 28: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 29: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 30: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_tensor_core" - 31: 0%....50%....100% - 38 passes
Total execution time: 14733.644531 ms
==PROF== Disconnected from process 6682
[6682] hgemm_2@127.0.0.1
  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       955131
    Memory Throughput                   %        90.95
    DRAM Throughput                     %         2.86
    Duration                      usecond       740.51
    L1/TEX Cache Throughput             %        93.27
    L2 Cache Throughput                 %        18.75
    SM Active Cycles                cycle    931321.74
    Compute (SM) Throughput             %        90.95
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.77
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.31
    Issued Ipc Active     inst/cycle         1.77
    SM Busy                        %        44.31
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.04
    Mem Busy                               %        50.75
    Max Bandwidth                          %        90.95
    L1/TEX Hit Rate                        %        13.55
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.44
    Mem Pipes Busy                         %        90.95
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.33
    Issued Warp Per Scheduler                        0.44
    No Eligible                            %        55.67
    Active Warps Per Scheduler          warp        10.58
    Eligible Warps Per Scheduler        warp         1.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.055%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.58 active warps per scheduler, but only an average of 1.82 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.86
    Warp Cycles Per Executed Instruction           cycle        23.86
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.055%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 43.4% of the total average of 23.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    412566.07
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    412688.63
    Issued Instructions                             inst     95743762
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.07
    Achieved Active Warps Per SM           warp        42.27
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.055%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     41721856
    Average L1 Active Cycles         cycle    931321.74
    Total L1 Elapsed Cycles          cycle     55396700
    Average L2 Active Cycles         cycle    785455.17
    Total L2 Elapsed Cycles          cycle     19457256
    Average SM Active Cycles         cycle    931321.74
    Total SM Elapsed Cycles          cycle     55396700
    Average SMSP Active Cycles       cycle    931016.93
    Total SMSP Elapsed Cycles        cycle    221586800
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       909273
    Memory Throughput                   %        92.44
    DRAM Throughput                     %         3.01
    Duration                      usecond       704.93
    L1/TEX Cache Throughput             %        94.07
    L2 Cache Throughput                 %        42.12
    SM Active Cycles                cycle    893426.29
    Compute (SM) Throughput             %        32.83
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.61
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        11.31
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.95%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.65
    Mem Busy                               %        92.44
    Max Bandwidth                          %        41.77
    L1/TEX Hit Rate                        %        78.28
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        96.62
    Mem Pipes Busy                         %        32.83
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 81.56%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 47.04%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097196 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 56.44%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.00
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.00
    Active Warps Per Scheduler          warp         3.77
    Eligible Warps Per Scheduler        warp         0.17
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.565%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.77 active warps per scheduler, but only an average of 0.17 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.71
    Warp Cycles Per Executed Instruction           cycle        37.71
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.565%                                                                                          
          On average, each warp of this kernel spends 12.9 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.3% of the total average of 37.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.565%                                                                                          
          On average, each warp of this kernel spends 12.0 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 32.0% of the total average of 37.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.64%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     85857.10
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst     85875.60
    Issued Instructions                             inst     19923139
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           33
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.20
    Achieved Active Warps Per SM           warp        14.50
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.565%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (30.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.565%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       185832
    Total DRAM Elapsed Cycles        cycle     39719936
    Average L1 Active Cycles         cycle    893426.29
    Total L1 Elapsed Cycles          cycle     52737390
    Average L2 Active Cycles         cycle    763184.54
    Total L2 Elapsed Cycles          cycle     18523392
    Average SM Active Cycles         cycle    893426.29
    Total SM Elapsed Cycles          cycle     52737390
    Average SMSP Active Cycles       cycle    858429.62
    Total SMSP Elapsed Cycles        cycle    210949560
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.079%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 5.38% above the average, while the minimum instance value is 5.36% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     564.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 87.17%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 57.32%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       954967
    Memory Throughput                   %        90.96
    DRAM Throughput                     %         2.87
    Duration                      usecond       740.35
    L1/TEX Cache Throughput             %        93.24
    L2 Cache Throughput                 %        18.83
    SM Active Cycles                cycle    931610.93
    Compute (SM) Throughput             %        90.96
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.77
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.30
    Issued Ipc Active     inst/cycle         1.77
    SM Busy                        %        44.30
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.06
    Mem Busy                               %        50.75
    Max Bandwidth                          %        90.96
    L1/TEX Hit Rate                        %        13.43
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.24
    Mem Pipes Busy                         %        90.96
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.31
    Issued Warp Per Scheduler                        0.44
    No Eligible                            %        55.69
    Active Warps Per Scheduler          warp        10.57
    Eligible Warps Per Scheduler        warp         1.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.039%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.57 active warps per scheduler, but only an average of 1.82 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.86
    Warp Cycles Per Executed Instruction           cycle        23.86
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.039%                                                                                          
          On average, each warp of this kernel spends 10.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 43.4% of the total average of 23.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    412566.07
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    412689.41
    Issued Instructions                             inst     95743943
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.06
    Achieved Active Warps Per SM           warp        42.27
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.039%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    186077.33
    Total DRAM Elapsed Cycles        cycle     41715712
    Average L1 Active Cycles         cycle    931610.93
    Total L1 Elapsed Cycles          cycle     55387258
    Average L2 Active Cycles         cycle    784177.58
    Total L2 Elapsed Cycles          cycle     19454064
    Average SM Active Cycles         cycle    931610.93
    Total SM Elapsed Cycles          cycle     55387258
    Average SMSP Active Cycles       cycle    931332.60
    Total SMSP Elapsed Cycles        cycle    221549032
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       909697
    Memory Throughput                   %        92.39
    DRAM Throughput                     %         3.01
    Duration                      usecond       705.28
    L1/TEX Cache Throughput             %        94.08
    L2 Cache Throughput                 %        42.15
    SM Active Cycles                cycle    893318.28
    Compute (SM) Throughput             %        32.82
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.61
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        11.31
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.95%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.67
    Mem Busy                               %        92.39
    Max Bandwidth                          %        41.71
    L1/TEX Hit Rate                        %        78.21
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        96.43
    Mem Pipes Busy                         %        32.82
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 81.52%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 47.04%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097202 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 56.45%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.01
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.99
    Active Warps Per Scheduler          warp         3.77
    Eligible Warps Per Scheduler        warp         0.17
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.609%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.77 active warps per scheduler, but only an average of 0.17 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.71
    Warp Cycles Per Executed Instruction           cycle        37.71
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.609%                                                                                          
          On average, each warp of this kernel spends 12.9 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.3% of the total average of 37.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.609%                                                                                          
          On average, each warp of this kernel spends 12.0 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 31.8% of the total average of 37.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.64%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     85857.10
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst     85874.82
    Issued Instructions                             inst     19922958
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           33
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.19
    Achieved Active Warps Per SM           warp        14.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.609%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (30.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.609%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       186216
    Total DRAM Elapsed Cycles        cycle     39736320
    Average L1 Active Cycles         cycle    893318.28
    Total L1 Elapsed Cycles          cycle     52761546
    Average L2 Active Cycles         cycle    763287.71
    Total L2 Elapsed Cycles          cycle     18531960
    Average SM Active Cycles         cycle    893318.28
    Total SM Elapsed Cycles          cycle     52761546
    Average SMSP Active Cycles       cycle    858215.07
    Total SMSP Elapsed Cycles        cycle    211046184
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.136%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 5.44% above the average, while the minimum instance value is 5.30% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     564.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 87.14%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 57.28%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 17, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       954789
    Memory Throughput                   %        90.98
    DRAM Throughput                     %         2.88
    Duration                      usecond       740.26
    L1/TEX Cache Throughput             %        93.28
    L2 Cache Throughput                 %        18.79
    SM Active Cycles                cycle    931197.66
    Compute (SM) Throughput             %        90.98
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.77
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.32
    Issued Ipc Active     inst/cycle         1.77
    SM Busy                        %        44.32
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.09
    Mem Busy                               %        50.76
    Max Bandwidth                          %        90.98
    L1/TEX Hit Rate                        %        13.71
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.18
    Mem Pipes Busy                         %        90.98
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.32
    Issued Warp Per Scheduler                        0.44
    No Eligible                            %        55.68
    Active Warps Per Scheduler          warp        10.57
    Eligible Warps Per Scheduler        warp         1.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.022%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.57 active warps per scheduler, but only an average of 1.82 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.85
    Warp Cycles Per Executed Instruction           cycle        23.86
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.022%                                                                                          
          On average, each warp of this kernel spends 10.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 43.5% of the total average of 23.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    412566.07
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    412689.63
    Issued Instructions                             inst     95743995
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.10
    Achieved Active Warps Per SM           warp        42.29
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.022%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    186418.67
    Total DRAM Elapsed Cycles        cycle     41707520
    Average L1 Active Cycles         cycle    931197.66
    Total L1 Elapsed Cycles          cycle     55376766
    Average L2 Active Cycles         cycle    785217.08
    Total L2 Elapsed Cycles          cycle     19450464
    Average SM Active Cycles         cycle    931197.66
    Total SM Elapsed Cycles          cycle     55376766
    Average SMSP Active Cycles       cycle    931130.19
    Total SMSP Elapsed Cycles        cycle    221507064
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 18, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       909741
    Memory Throughput                   %        92.39
    DRAM Throughput                     %         3.01
    Duration                      usecond       705.28
    L1/TEX Cache Throughput             %        94.08
    L2 Cache Throughput                 %        42.14
    SM Active Cycles                cycle    893371.03
    Compute (SM) Throughput             %        32.82
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.61
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        11.31
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.95%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.65
    Mem Busy                               %        92.39
    Max Bandwidth                          %        41.80
    L1/TEX Hit Rate                        %        78.26
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        96.32
    Mem Pipes Busy                         %        32.82
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 81.52%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 47.04%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097190 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 56.45%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.00
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.00
    Active Warps Per Scheduler          warp         3.77
    Eligible Warps Per Scheduler        warp         0.17
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.612%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.77 active warps per scheduler, but only an average of 0.17 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.70
    Warp Cycles Per Executed Instruction           cycle        37.71
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.612%                                                                                          
          On average, each warp of this kernel spends 12.9 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.2% of the total average of 37.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.612%                                                                                          
          On average, each warp of this kernel spends 12.1 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 32.1% of the total average of 37.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.64%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     85857.10
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst     85874.66
    Issued Instructions                             inst     19922920
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           33
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.21
    Achieved Active Warps Per SM           warp        14.50
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.612%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (30.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.612%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       185832
    Total DRAM Elapsed Cycles        cycle     39739392
    Average L1 Active Cycles         cycle    893371.03
    Total L1 Elapsed Cycles          cycle     52764396
    Average L2 Active Cycles         cycle    762205.92
    Total L2 Elapsed Cycles          cycle     18532848
    Average SM Active Cycles         cycle    893371.03
    Total SM Elapsed Cycles          cycle     52764396
    Average SMSP Active Cycles       cycle    858424.69
    Total SMSP Elapsed Cycles        cycle    211057584
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.153%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 5.46% above the average, while the minimum instance value is 5.35% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     564.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 87.01%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 57.28%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 19, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       954556
    Memory Throughput                   %        91.00
    DRAM Throughput                     %         2.87
    Duration                      usecond       740.03
    L1/TEX Cache Throughput             %        93.23
    L2 Cache Throughput                 %        18.84
    SM Active Cycles                cycle    931742.43
    Compute (SM) Throughput             %        91.00
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.77
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.29
    Issued Ipc Active     inst/cycle         1.77
    SM Busy                        %        44.29
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.06
    Mem Busy                               %        50.78
    Max Bandwidth                          %        91.00
    L1/TEX Hit Rate                        %        13.39
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.25
    Mem Pipes Busy                         %        91.00
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.31
    Issued Warp Per Scheduler                        0.44
    No Eligible                            %        55.69
    Active Warps Per Scheduler          warp        10.57
    Eligible Warps Per Scheduler        warp         1.81
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9%                                                                                        
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.57 active warps per scheduler, but only an average of 1.81 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.85
    Warp Cycles Per Executed Instruction           cycle        23.86
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9%                                                                                              
          On average, each warp of this kernel spends 10.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 43.5% of the total average of 23.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    412566.07
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    412688.78
    Issued Instructions                             inst     95743796
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.08
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 9%                                                                                              
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185869.33
    Total DRAM Elapsed Cycles        cycle     41696256
    Average L1 Active Cycles         cycle    931742.43
    Total L1 Elapsed Cycles          cycle     55363348
    Average L2 Active Cycles         cycle    784883.08
    Total L2 Elapsed Cycles          cycle     19445472
    Average SM Active Cycles         cycle    931742.43
    Total SM Elapsed Cycles          cycle     55363348
    Average SMSP Active Cycles       cycle    931442.62
    Total SMSP Elapsed Cycles        cycle    221453392
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 20, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       910153
    Memory Throughput                   %        92.35
    DRAM Throughput                     %         3.01
    Duration                      usecond       705.63
    L1/TEX Cache Throughput             %        94.00
    L2 Cache Throughput                 %        42.07
    SM Active Cycles                cycle    894160.41
    Compute (SM) Throughput             %        32.80
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.60
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        11.30
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.96%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.64
    Mem Busy                               %        92.35
    Max Bandwidth                          %        41.75
    L1/TEX Hit Rate                        %        78.16
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        96.14
    Mem Pipes Busy                         %        32.80
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 81.48%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 47%                                                                                             
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097194 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 56.4%                                                                                           
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.01
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.99
    Active Warps Per Scheduler          warp         3.77
    Eligible Warps Per Scheduler        warp         0.17
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.655%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.77 active warps per scheduler, but only an average of 0.17 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.72
    Warp Cycles Per Executed Instruction           cycle        37.73
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.655%                                                                                          
          On average, each warp of this kernel spends 13.0 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.3% of the total average of 37.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.655%                                                                                          
          On average, each warp of this kernel spends 12.0 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 31.7% of the total average of 37.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.63%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     85857.10
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst     85875.50
    Issued Instructions                             inst     19923116
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           33
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.19
    Achieved Active Warps Per SM           warp        14.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.655%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (30.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.655%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       185832
    Total DRAM Elapsed Cycles        cycle     39754752
    Average L1 Active Cycles         cycle    894160.41
    Total L1 Elapsed Cycles          cycle     52788386
    Average L2 Active Cycles         cycle       762179
    Total L2 Elapsed Cycles          cycle     18541320
    Average SM Active Cycles         cycle    894160.41
    Total SM Elapsed Cycles          cycle     52788386
    Average SMSP Active Cycles       cycle    858202.38
    Total SMSP Elapsed Cycles        cycle    211153544
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.033%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 5.34% above the average, while the minimum instance value is 5.40% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     564.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 86.97%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 57.31%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       955699
    Memory Throughput                   %        90.93
    DRAM Throughput                     %         2.86
    Duration                      usecond       740.96
    L1/TEX Cache Throughput             %        93.25
    L2 Cache Throughput                 %        18.81
    SM Active Cycles                cycle    931468.12
    Compute (SM) Throughput             %        90.93
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.77
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.31
    Issued Ipc Active     inst/cycle         1.77
    SM Busy                        %        44.31
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.03
    Mem Busy                               %        50.74
    Max Bandwidth                          %        90.93
    L1/TEX Hit Rate                        %        13.42
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.39
    Mem Pipes Busy                         %        90.93
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.32
    Issued Warp Per Scheduler                        0.44
    No Eligible                            %        55.68
    Active Warps Per Scheduler          warp        10.57
    Eligible Warps Per Scheduler        warp         1.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.069%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.57 active warps per scheduler, but only an average of 1.82 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.86
    Warp Cycles Per Executed Instruction           cycle        23.86
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.069%                                                                                          
          On average, each warp of this kernel spends 10.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 43.4% of the total average of 23.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    412566.07
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    412689.52
    Issued Instructions                             inst     95743968
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.08
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.069%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     41748480
    Average L1 Active Cycles         cycle    931468.12
    Total L1 Elapsed Cycles          cycle     55405380
    Average L2 Active Cycles         cycle    785213.79
    Total L2 Elapsed Cycles          cycle     19465992
    Average SM Active Cycles         cycle    931468.12
    Total SM Elapsed Cycles          cycle     55405380
    Average SMSP Active Cycles       cycle    931140.16
    Total SMSP Elapsed Cycles        cycle    221621520
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       909177
    Memory Throughput                   %        92.44
    DRAM Throughput                     %         3.01
    Duration                      usecond       704.86
    L1/TEX Cache Throughput             %        94.04
    L2 Cache Throughput                 %        42.04
    SM Active Cycles                cycle    893707.45
    Compute (SM) Throughput             %        32.84
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.61
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        11.31
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.96%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.65
    Mem Busy                               %        92.44
    Max Bandwidth                          %        41.65
    L1/TEX Hit Rate                        %        78.12
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        96.70
    Mem Pipes Busy                         %        32.84
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 81.57%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 47.02%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097206 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 56.43%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.01
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.99
    Active Warps Per Scheduler          warp         3.78
    Eligible Warps Per Scheduler        warp         0.17
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.556%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.78 active warps per scheduler, but only an average of 0.17 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.74
    Warp Cycles Per Executed Instruction           cycle        37.75
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.556%                                                                                          
          On average, each warp of this kernel spends 12.9 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.3% of the total average of 37.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.556%                                                                                          
          On average, each warp of this kernel spends 11.9 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 31.6% of the total average of 37.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.65%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     85857.10
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst     85875.31
    Issued Instructions                             inst     19923072
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           33
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.20
    Achieved Active Warps Per SM           warp        14.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.556%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (30.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.556%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       185832
    Total DRAM Elapsed Cycles        cycle     39713792
    Average L1 Active Cycles         cycle    893707.45
    Total L1 Elapsed Cycles          cycle     52731726
    Average L2 Active Cycles         cycle    763523.08
    Total L2 Elapsed Cycles          cycle     18521568
    Average SM Active Cycles         cycle    893707.45
    Total SM Elapsed Cycles          cycle     52731726
    Average SMSP Active Cycles       cycle    858301.29
    Total SMSP Elapsed Cycles        cycle    210926904
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.06%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 5.36% above the average, while the minimum instance value is 5.41% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     564.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 87.22%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 57.34%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       954442
    Memory Throughput                   %        91.01
    DRAM Throughput                     %         2.87
    Duration                      usecond       739.94
    L1/TEX Cache Throughput             %        93.28
    L2 Cache Throughput                 %        18.79
    SM Active Cycles                cycle    931245.38
    Compute (SM) Throughput             %        91.01
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.77
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.32
    Issued Ipc Active     inst/cycle         1.77
    SM Busy                        %        44.32
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.05
    Mem Busy                               %        50.78
    Max Bandwidth                          %        91.01
    L1/TEX Hit Rate                        %        13.59
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.23
    Mem Pipes Busy                         %        91.01
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.32
    Issued Warp Per Scheduler                        0.44
    No Eligible                            %        55.68
    Active Warps Per Scheduler          warp        10.57
    Eligible Warps Per Scheduler        warp         1.81
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.989%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.57 active warps per scheduler, but only an average of 1.81 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.86
    Warp Cycles Per Executed Instruction           cycle        23.86
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.989%                                                                                          
          On average, each warp of this kernel spends 10.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 43.4% of the total average of 23.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    412566.07
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    412689.53
    Issued Instructions                             inst     95743972
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.10
    Achieved Active Warps Per SM           warp        42.29
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.989%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     41691136
    Average L1 Active Cycles         cycle    931245.38
    Total L1 Elapsed Cycles          cycle     55356610
    Average L2 Active Cycles         cycle    785857.71
    Total L2 Elapsed Cycles          cycle     19443024
    Average SM Active Cycles         cycle    931245.38
    Total SM Elapsed Cycles          cycle     55356610
    Average SMSP Active Cycles       cycle    931257.22
    Total SMSP Elapsed Cycles        cycle    221426440
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       909730
    Memory Throughput                   %        92.39
    DRAM Throughput                     %         3.01
    Duration                      usecond       705.28
    L1/TEX Cache Throughput             %        94.02
    L2 Cache Throughput                 %        41.98
    SM Active Cycles                cycle    893954.64
    Compute (SM) Throughput             %        32.82
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.61
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        11.30
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.96%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.66
    Mem Busy                               %        92.39
    Max Bandwidth                          %        41.71
    L1/TEX Hit Rate                        %        78.15
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        96.38
    Mem Pipes Busy                         %        32.82
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 81.52%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 47.01%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097204 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 56.41%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.01
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.99
    Active Warps Per Scheduler          warp         3.77
    Eligible Warps Per Scheduler        warp         0.17
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.612%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.77 active warps per scheduler, but only an average of 0.17 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.68
    Warp Cycles Per Executed Instruction           cycle        37.69
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.612%                                                                                          
          On average, each warp of this kernel spends 12.9 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.3% of the total average of 37.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.612%                                                                                          
          On average, each warp of this kernel spends 12.0 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 31.8% of the total average of 37.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.64%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     85857.10
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst     85874.60
    Issued Instructions                             inst     19922907
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           33
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.21
    Achieved Active Warps Per SM           warp        14.50
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.612%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (30.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.612%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       186088
    Total DRAM Elapsed Cycles        cycle     39739392
    Average L1 Active Cycles         cycle    893954.64
    Total L1 Elapsed Cycles          cycle     52763784
    Average L2 Active Cycles         cycle    762559.21
    Total L2 Elapsed Cycles          cycle     18532608
    Average SM Active Cycles         cycle    893954.64
    Total SM Elapsed Cycles          cycle     52763784
    Average SMSP Active Cycles       cycle    858309.85
    Total SMSP Elapsed Cycles        cycle    211055136
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.08%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 5.38% above the average, while the minimum instance value is 5.19% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     564.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 87.05%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 57.32%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 17, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       953988
    Memory Throughput                   %        91.05
    DRAM Throughput                     %         2.87
    Duration                      usecond       739.62
    L1/TEX Cache Throughput             %        93.24
    L2 Cache Throughput                 %        18.84
    SM Active Cycles                cycle    931579.21
    Compute (SM) Throughput             %        91.05
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.77
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.30
    Issued Ipc Active     inst/cycle         1.77
    SM Busy                        %        44.30
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.06
    Mem Busy                               %        50.81
    Max Bandwidth                          %        91.05
    L1/TEX Hit Rate                        %        13.30
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.32
    Mem Pipes Busy                         %        91.05
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.31
    Issued Warp Per Scheduler                        0.44
    No Eligible                            %        55.69
    Active Warps Per Scheduler          warp        10.57
    Eligible Warps Per Scheduler        warp         1.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.946%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.57 active warps per scheduler, but only an average of 1.82 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.85
    Warp Cycles Per Executed Instruction           cycle        23.86
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.946%                                                                                          
          On average, each warp of this kernel spends 10.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 43.4% of the total average of 23.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    412566.07
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    412689.55
    Issued Instructions                             inst     95743976
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.09
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.946%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     41671680
    Average L1 Active Cycles         cycle    931579.21
    Total L1 Elapsed Cycles          cycle     55330616
    Average L2 Active Cycles         cycle    784572.46
    Total L2 Elapsed Cycles          cycle     19434024
    Average SM Active Cycles         cycle    931579.21
    Total SM Elapsed Cycles          cycle     55330616
    Average SMSP Active Cycles       cycle    931435.88
    Total SMSP Elapsed Cycles        cycle    221322464
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 18, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       908952
    Memory Throughput                   %        92.47
    DRAM Throughput                     %         3.01
    Duration                      usecond       704.70
    L1/TEX Cache Throughput             %        94.10
    L2 Cache Throughput                 %        42.09
    SM Active Cycles                cycle    893176.52
    Compute (SM) Throughput             %        32.85
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.61
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        11.31
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.95%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.66
    Mem Busy                               %        92.47
    Max Bandwidth                          %        41.77
    L1/TEX Hit Rate                        %        78.24
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        96.36
    Mem Pipes Busy                         %        32.85
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 81.59%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 47.05%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097203 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 56.46%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.01
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.99
    Active Warps Per Scheduler          warp         3.77
    Eligible Warps Per Scheduler        warp         0.17
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.532%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.77 active warps per scheduler, but only an average of 0.17 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.71
    Warp Cycles Per Executed Instruction           cycle        37.72
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.532%                                                                                          
          On average, each warp of this kernel spends 13.0 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.4% of the total average of 37.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.532%                                                                                          
          On average, each warp of this kernel spends 11.9 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 31.6% of the total average of 37.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.65%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     85857.10
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst     85874.73
    Issued Instructions                             inst     19922938
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           33
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.21
    Achieved Active Warps Per SM           warp        14.50
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.532%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (30.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.532%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       185832
    Total DRAM Elapsed Cycles        cycle     39702528
    Average L1 Active Cycles         cycle    893176.52
    Total L1 Elapsed Cycles          cycle     52718478
    Average L2 Active Cycles         cycle    762954.96
    Total L2 Elapsed Cycles          cycle     18516672
    Average SM Active Cycles         cycle    893176.52
    Total SM Elapsed Cycles          cycle     52718478
    Average SMSP Active Cycles       cycle    858178.81
    Total SMSP Elapsed Cycles        cycle    210873912
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.145%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 5.45% above the average, while the minimum instance value is 5.38% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     564.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 87.17%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 57.32%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 19, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       955588
    Memory Throughput                   %        90.90
    DRAM Throughput                     %         2.87
    Duration                      usecond       740.86
    L1/TEX Cache Throughput             %        93.21
    L2 Cache Throughput                 %        18.76
    SM Active Cycles                cycle    931862.16
    Compute (SM) Throughput             %        90.90
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.77
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.29
    Issued Ipc Active     inst/cycle         1.77
    SM Busy                        %        44.29
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.07
    Mem Busy                               %        50.72
    Max Bandwidth                          %        90.90
    L1/TEX Hit Rate                        %        13.57
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.25
    Mem Pipes Busy                         %        90.90
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.30
    Issued Warp Per Scheduler                        0.44
    No Eligible                            %        55.70
    Active Warps Per Scheduler          warp        10.57
    Eligible Warps Per Scheduler        warp         1.81
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.098%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.57 active warps per scheduler, but only an average of 1.81 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.86
    Warp Cycles Per Executed Instruction           cycle        23.86
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.098%                                                                                          
          On average, each warp of this kernel spends 10.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 43.4% of the total average of 23.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    412566.07
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    412689.31
    Issued Instructions                             inst     95743920
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.04
    Achieved Active Warps Per SM           warp        42.26
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.098%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    186290.67
    Total DRAM Elapsed Cycles        cycle     41742336
    Average L1 Active Cycles         cycle    931862.16
    Total L1 Elapsed Cycles          cycle     55422902
    Average L2 Active Cycles         cycle    784592.67
    Total L2 Elapsed Cycles          cycle     19466400
    Average SM Active Cycles         cycle    931862.16
    Total SM Elapsed Cycles          cycle     55422902
    Average SMSP Active Cycles       cycle    931603.45
    Total SMSP Elapsed Cycles        cycle    221691608
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 20, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.28
    Elapsed Cycles                  cycle       911125
    Memory Throughput                   %        92.53
    DRAM Throughput                     %         3.04
    Duration                      usecond       707.46
    L1/TEX Cache Throughput             %        94.12
    L2 Cache Throughput                 %        42.48
    SM Active Cycles                cycle    892993.95
    Compute (SM) Throughput             %        32.87
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.62
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        11.32
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.95%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.61
    Mem Busy                               %        92.53
    Max Bandwidth                          %        42.23
    L1/TEX Hit Rate                        %        78.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        96.66
    Mem Pipes Busy                         %        32.87
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 81.65%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 47.06%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097195 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 56.47%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.01
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.99
    Active Warps Per Scheduler          warp         3.77
    Eligible Warps Per Scheduler        warp         0.17
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.466%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.77 active warps per scheduler, but only an average of 0.17 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.70
    Warp Cycles Per Executed Instruction           cycle        37.71
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.466%                                                                                          
          On average, each warp of this kernel spends 12.9 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.2% of the total average of 37.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.466%                                                                                          
          On average, each warp of this kernel spends 12.0 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 31.8% of the total average of 37.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.66%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     85857.10
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst     85875.47
    Issued Instructions                             inst     19923108
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           33
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.24
    Achieved Active Warps Per SM           warp        14.52
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.466%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (30.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.466%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       185832
    Total DRAM Elapsed Cycles        cycle     39860224
    Average L1 Active Cycles         cycle    892993.95
    Total L1 Elapsed Cycles          cycle     52680778
    Average L2 Active Cycles         cycle    763175.50
    Total L2 Elapsed Cycles          cycle     18348120
    Average SM Active Cycles         cycle    892993.95
    Total SM Elapsed Cycles          cycle     52680778
    Average SMSP Active Cycles       cycle    858321.16
    Total SMSP Elapsed Cycles        cycle    210723112
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.096%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 5.39% above the average, while the minimum instance value is 5.25% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     564.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 88%                                                                                             
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 57.35%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       954204
    Memory Throughput                   %        91.03
    DRAM Throughput                     %         2.87
    Duration                      usecond       739.78
    L1/TEX Cache Throughput             %        93.25
    L2 Cache Throughput                 %        18.80
    SM Active Cycles                cycle    931471.17
    Compute (SM) Throughput             %        91.03
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.77
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.31
    Issued Ipc Active     inst/cycle         1.77
    SM Busy                        %        44.31
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.05
    Mem Busy                               %        50.79
    Max Bandwidth                          %        91.03
    L1/TEX Hit Rate                        %        13.50
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.29
    Mem Pipes Busy                         %        91.03
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.30
    Issued Warp Per Scheduler                        0.44
    No Eligible                            %        55.70
    Active Warps Per Scheduler          warp        10.57
    Eligible Warps Per Scheduler        warp         1.81
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.966%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.57 active warps per scheduler, but only an average of 1.81 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.85
    Warp Cycles Per Executed Instruction           cycle        23.86
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.966%                                                                                          
          On average, each warp of this kernel spends 10.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 43.4% of the total average of 23.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    412566.07
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    412688.85
    Issued Instructions                             inst     95743813
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.06
    Achieved Active Warps Per SM           warp        42.27
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.966%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     41681920
    Average L1 Active Cycles         cycle    931471.17
    Total L1 Elapsed Cycles          cycle     55342934
    Average L2 Active Cycles         cycle    784402.58
    Total L2 Elapsed Cycles          cycle     19438344
    Average SM Active Cycles         cycle    931471.17
    Total SM Elapsed Cycles          cycle     55342934
    Average SMSP Active Cycles       cycle    931596.95
    Total SMSP Elapsed Cycles        cycle    221371736
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       909434
    Memory Throughput                   %        92.42
    DRAM Throughput                     %         3.01
    Duration                      usecond       705.06
    L1/TEX Cache Throughput             %        94.05
    L2 Cache Throughput                 %        41.91
    SM Active Cycles                cycle    893619.47
    Compute (SM) Throughput             %        32.83
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.61
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        11.31
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.96%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.66
    Mem Busy                               %        92.42
    Max Bandwidth                          %        41.74
    L1/TEX Hit Rate                        %        78.21
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        96.65
    Mem Pipes Busy                         %        32.83
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 81.54%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 47.02%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097199 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 56.43%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.01
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.99
    Active Warps Per Scheduler          warp         3.78
    Eligible Warps Per Scheduler        warp         0.17
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.583%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.78 active warps per scheduler, but only an average of 0.17 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.72
    Warp Cycles Per Executed Instruction           cycle        37.73
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.583%                                                                                          
          On average, each warp of this kernel spends 12.9 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.3% of the total average of 37.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.583%                                                                                          
          On average, each warp of this kernel spends 12.0 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 31.8% of the total average of 37.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.64%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     85857.10
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst     85875.37
    Issued Instructions                             inst     19923086
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           33
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.19
    Achieved Active Warps Per SM           warp        14.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.583%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (30.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.583%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       185960
    Total DRAM Elapsed Cycles        cycle     39726080
    Average L1 Active Cycles         cycle    893619.47
    Total L1 Elapsed Cycles          cycle     52746590
    Average L2 Active Cycles         cycle    762621.46
    Total L2 Elapsed Cycles          cycle     18526608
    Average SM Active Cycles         cycle    893619.47
    Total SM Elapsed Cycles          cycle     52746590
    Average SMSP Active Cycles       cycle    858081.31
    Total SMSP Elapsed Cycles        cycle    210986360
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.076%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 5.38% above the average, while the minimum instance value is 5.22% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     564.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 87.09%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 57.32%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       955177
    Memory Throughput                   %        90.94
    DRAM Throughput                     %         2.86
    Duration                      usecond       740.51
    L1/TEX Cache Throughput             %        93.20
    L2 Cache Throughput                 %        18.80
    SM Active Cycles                cycle    932026.29
    Compute (SM) Throughput             %        90.94
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.77
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.28
    Issued Ipc Active     inst/cycle         1.77
    SM Busy                        %        44.28
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.04
    Mem Busy                               %        50.74
    Max Bandwidth                          %        90.94
    L1/TEX Hit Rate                        %        13.41
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.19
    Mem Pipes Busy                         %        90.94
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.30
    Issued Warp Per Scheduler                        0.44
    No Eligible                            %        55.70
    Active Warps Per Scheduler          warp        10.57
    Eligible Warps Per Scheduler        warp         1.81
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.059%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.57 active warps per scheduler, but only an average of 1.81 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.86
    Warp Cycles Per Executed Instruction           cycle        23.86
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.059%                                                                                          
          On average, each warp of this kernel spends 10.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 43.4% of the total average of 23.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    412566.07
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    412689.61
    Issued Instructions                             inst     95743990
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.05
    Achieved Active Warps Per SM           warp        42.26
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.059%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     41725952
    Average L1 Active Cycles         cycle    932026.29
    Total L1 Elapsed Cycles          cycle     55399630
    Average L2 Active Cycles         cycle    784068.12
    Total L2 Elapsed Cycles          cycle     19458360
    Average SM Active Cycles         cycle    932026.29
    Total SM Elapsed Cycles          cycle     55399630
    Average SMSP Active Cycles       cycle    931498.97
    Total SMSP Elapsed Cycles        cycle    221598520
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       908910
    Memory Throughput                   %        92.50
    DRAM Throughput                     %         3.01
    Duration                      usecond       704.67
    L1/TEX Cache Throughput             %        94.05
    L2 Cache Throughput                 %        42.06
    SM Active Cycles                cycle    893656.72
    Compute (SM) Throughput             %        32.86
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.61
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        11.31
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.96%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.66
    Mem Busy                               %        92.50
    Max Bandwidth                          %        41.68
    L1/TEX Hit Rate                        %        78.20
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        96.28
    Mem Pipes Busy                         %        32.86
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 81.62%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 47.02%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097204 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 56.43%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.01
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.99
    Active Warps Per Scheduler          warp         3.77
    Eligible Warps Per Scheduler        warp         0.17
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.502%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.77 active warps per scheduler, but only an average of 0.17 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.71
    Warp Cycles Per Executed Instruction           cycle        37.72
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.502%                                                                                          
          On average, each warp of this kernel spends 13.0 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.4% of the total average of 37.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.502%                                                                                          
          On average, each warp of this kernel spends 12.0 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 31.9% of the total average of 37.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.65%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     85857.10
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst     85874.63
    Issued Instructions                             inst     19922915
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           33
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.19
    Achieved Active Warps Per SM           warp        14.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.502%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (30.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.502%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185874.67
    Total DRAM Elapsed Cycles        cycle     39702528
    Average L1 Active Cycles         cycle    893656.72
    Total L1 Elapsed Cycles          cycle     52700752
    Average L2 Active Cycles         cycle    762987.67
    Total L2 Elapsed Cycles          cycle     18514488
    Average SM Active Cycles         cycle    893656.72
    Total SM Elapsed Cycles          cycle     52700752
    Average SMSP Active Cycles       cycle    858270.84
    Total SMSP Elapsed Cycles        cycle    210803008
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.1%                                                                                            
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 5.40% above the average, while the minimum instance value is 5.38% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     564.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 87.19%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 57.37%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 17, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       955802
    Memory Throughput                   %        90.91
    DRAM Throughput                     %         2.86
    Duration                      usecond       741.06
    L1/TEX Cache Throughput             %        93.23
    L2 Cache Throughput                 %        18.73
    SM Active Cycles                cycle    931662.59
    Compute (SM) Throughput             %        90.91
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.77
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.30
    Issued Ipc Active     inst/cycle         1.77
    SM Busy                        %        44.30
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.03
    Mem Busy                               %        50.73
    Max Bandwidth                          %        90.91
    L1/TEX Hit Rate                        %        13.60
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.38
    Mem Pipes Busy                         %        90.91
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.29
    Issued Warp Per Scheduler                        0.44
    No Eligible                            %        55.71
    Active Warps Per Scheduler          warp        10.57
    Eligible Warps Per Scheduler        warp         1.81
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.089%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.57 active warps per scheduler, but only an average of 1.81 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.85
    Warp Cycles Per Executed Instruction           cycle        23.86
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.089%                                                                                          
          On average, each warp of this kernel spends 10.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 43.4% of the total average of 23.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    412566.07
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    412689.59
    Issued Instructions                             inst     95743984
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.06
    Achieved Active Warps Per SM           warp        42.27
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.089%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     41751552
    Average L1 Active Cycles         cycle    931662.59
    Total L1 Elapsed Cycles          cycle     55417512
    Average L2 Active Cycles         cycle    787028.08
    Total L2 Elapsed Cycles          cycle     19468968
    Average SM Active Cycles         cycle    931662.59
    Total SM Elapsed Cycles          cycle     55417512
    Average SMSP Active Cycles       cycle    931705.02
    Total SMSP Elapsed Cycles        cycle    221670048
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 18, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       909544
    Memory Throughput                   %        92.41
    DRAM Throughput                     %         3.01
    Duration                      usecond       705.15
    L1/TEX Cache Throughput             %        94.05
    L2 Cache Throughput                 %        41.99
    SM Active Cycles                cycle    893644.43
    Compute (SM) Throughput             %        32.82
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.61
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        11.31
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.96%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.65
    Mem Busy                               %        92.41
    Max Bandwidth                          %        41.70
    L1/TEX Hit Rate                        %        78.17
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        96.02
    Mem Pipes Busy                         %        32.82
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 81.53%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 47.02%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097184 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 56.43%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.01
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.99
    Active Warps Per Scheduler          warp         3.78
    Eligible Warps Per Scheduler        warp         0.17
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.594%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.78 active warps per scheduler, but only an average of 0.17 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.74
    Warp Cycles Per Executed Instruction           cycle        37.75
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.594%                                                                                          
          On average, each warp of this kernel spends 12.9 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.3% of the total average of 37.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.594%                                                                                          
          On average, each warp of this kernel spends 12.0 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 31.9% of the total average of 37.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.64%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     85857.10
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst     85874.68
    Issued Instructions                             inst     19922925
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           33
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.21
    Achieved Active Warps Per SM           warp        14.50
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.594%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (30.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.594%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       185832
    Total DRAM Elapsed Cycles        cycle     39730176
    Average L1 Active Cycles         cycle    893644.43
    Total L1 Elapsed Cycles          cycle     52752984
    Average L2 Active Cycles         cycle    763353.71
    Total L2 Elapsed Cycles          cycle     18528912
    Average SM Active Cycles         cycle    893644.43
    Total SM Elapsed Cycles          cycle     52752984
    Average SMSP Active Cycles       cycle    858141.92
    Total SMSP Elapsed Cycles        cycle    211011936
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.077%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 5.38% above the average, while the minimum instance value is 5.24% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     564.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 87.16%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 57.31%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 19, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       954787
    Memory Throughput                   %        90.98
    DRAM Throughput                     %         2.87
    Duration                      usecond       740.22
    L1/TEX Cache Throughput             %        93.23
    L2 Cache Throughput                 %        18.82
    SM Active Cycles                cycle    931682.90
    Compute (SM) Throughput             %        90.98
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.77
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.29
    Issued Ipc Active     inst/cycle         1.77
    SM Busy                        %        44.29
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.05
    Mem Busy                               %        50.76
    Max Bandwidth                          %        90.98
    L1/TEX Hit Rate                        %        13.30
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.46
    Mem Pipes Busy                         %        90.98
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.30
    Issued Warp Per Scheduler                        0.44
    No Eligible                            %        55.70
    Active Warps Per Scheduler          warp        10.57
    Eligible Warps Per Scheduler        warp         1.81
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.022%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.57 active warps per scheduler, but only an average of 1.81 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.86
    Warp Cycles Per Executed Instruction           cycle        23.87
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.022%                                                                                          
          On average, each warp of this kernel spends 10.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 43.4% of the total average of 23.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    412566.07
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    412688.82
    Issued Instructions                             inst     95743807
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.06
    Achieved Active Warps Per SM           warp        42.27
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.022%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     41705472
    Average L1 Active Cycles         cycle    931682.90
    Total L1 Elapsed Cycles          cycle     55376842
    Average L2 Active Cycles         cycle    785792.83
    Total L2 Elapsed Cycles          cycle     19450320
    Average SM Active Cycles         cycle    931682.90
    Total SM Elapsed Cycles          cycle     55376842
    Average SMSP Active Cycles       cycle    931499.90
    Total SMSP Elapsed Cycles        cycle    221507368
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 20, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       909811
    Memory Throughput                   %        92.38
    DRAM Throughput                     %         3.01
    Duration                      usecond       705.34
    L1/TEX Cache Throughput             %        94.04
    L2 Cache Throughput                 %        41.90
    SM Active Cycles                cycle    893763.26
    Compute (SM) Throughput             %        32.81
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.61
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        11.31
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.96%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.65
    Mem Busy                               %        92.38
    Max Bandwidth                          %        41.75
    L1/TEX Hit Rate                        %        78.26
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        96.62
    Mem Pipes Busy                         %        32.81
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 81.51%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 47.02%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097204 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 56.42%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.00
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.00
    Active Warps Per Scheduler          warp         3.77
    Eligible Warps Per Scheduler        warp         0.17
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.621%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.77 active warps per scheduler, but only an average of 0.17 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.70
    Warp Cycles Per Executed Instruction           cycle        37.71
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.621%                                                                                          
          On average, each warp of this kernel spends 13.0 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.4% of the total average of 37.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.621%                                                                                          
          On average, each warp of this kernel spends 12.0 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 31.8% of the total average of 37.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.64%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     85857.10
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst     85875.51
    Issued Instructions                             inst     19923119
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           33
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.19
    Achieved Active Warps Per SM           warp        14.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.621%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (30.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.621%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       185832
    Total DRAM Elapsed Cycles        cycle     39740416
    Average L1 Active Cycles         cycle    893763.26
    Total L1 Elapsed Cycles          cycle     52768690
    Average L2 Active Cycles         cycle    762679.58
    Total L2 Elapsed Cycles          cycle     18534360
    Average SM Active Cycles         cycle    893763.26
    Total SM Elapsed Cycles          cycle     52768690
    Average SMSP Active Cycles       cycle    858511.71
    Total SMSP Elapsed Cycles        cycle    211074760
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.131%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 5.44% above the average, while the minimum instance value is 5.29% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     564.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 87.06%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 57.3%                                                                                           
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       953713
    Memory Throughput                   %        91.08
    DRAM Throughput                     %         2.87
    Duration                      usecond       739.39
    L1/TEX Cache Throughput             %        93.27
    L2 Cache Throughput                 %        18.85
    SM Active Cycles                cycle    931328.10
    Compute (SM) Throughput             %        91.08
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.77
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.31
    Issued Ipc Active     inst/cycle         1.77
    SM Busy                        %        44.31
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.06
    Mem Busy                               %        50.82
    Max Bandwidth                          %        91.08
    L1/TEX Hit Rate                        %        13.31
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.40
    Mem Pipes Busy                         %        91.08
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.30
    Issued Warp Per Scheduler                        0.44
    No Eligible                            %        55.70
    Active Warps Per Scheduler          warp        10.57
    Eligible Warps Per Scheduler        warp         1.81
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.919%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.57 active warps per scheduler, but only an average of 1.81 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.86
    Warp Cycles Per Executed Instruction           cycle        23.87
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.919%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 43.4% of the total average of 23.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    412566.07
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    412689.03
    Issued Instructions                             inst     95743854
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.12
    Achieved Active Warps Per SM           warp        42.30
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.919%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     41659392
    Average L1 Active Cycles         cycle    931328.10
    Total L1 Elapsed Cycles          cycle     55314454
    Average L2 Active Cycles         cycle    785788.29
    Total L2 Elapsed Cycles          cycle     19428576
    Average SM Active Cycles         cycle    931328.10
    Total SM Elapsed Cycles          cycle     55314454
    Average SMSP Active Cycles       cycle    931518.26
    Total SMSP Elapsed Cycles        cycle    221257816
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       909554
    Memory Throughput                   %        92.43
    DRAM Throughput                     %         3.02
    Duration                      usecond       705.18
    L1/TEX Cache Throughput             %        94.04
    L2 Cache Throughput                 %        42.04
    SM Active Cycles                cycle    893752.29
    Compute (SM) Throughput             %        32.83
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.61
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        11.31
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.96%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.68
    Mem Busy                               %        92.43
    Max Bandwidth                          %        42.00
    L1/TEX Hit Rate                        %        78.20
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        96.37
    Mem Pipes Busy                         %        32.83
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 81.56%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 47.02%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097196 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 56.42%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.01
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.99
    Active Warps Per Scheduler          warp         3.77
    Eligible Warps Per Scheduler        warp         0.17
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.566%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.77 active warps per scheduler, but only an average of 0.17 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.69
    Warp Cycles Per Executed Instruction           cycle        37.70
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.566%                                                                                          
          On average, each warp of this kernel spends 12.9 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.3% of the total average of 37.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.566%                                                                                          
          On average, each warp of this kernel spends 12.0 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 31.9% of the total average of 37.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.64%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     85857.10
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst     85875.41
    Issued Instructions                             inst     19923094
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           33
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.20
    Achieved Active Warps Per SM           warp        14.50
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.566%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (30.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.566%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    186258.67
    Total DRAM Elapsed Cycles        cycle     39730176
    Average L1 Active Cycles         cycle    893752.29
    Total L1 Elapsed Cycles          cycle     52738938
    Average L2 Active Cycles         cycle    763717.38
    Total L2 Elapsed Cycles          cycle     18528312
    Average SM Active Cycles         cycle    893752.29
    Total SM Elapsed Cycles          cycle     52738938
    Average SMSP Active Cycles       cycle    857962.80
    Total SMSP Elapsed Cycles        cycle    210955752
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.165%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 5.47% above the average, while the minimum instance value is 5.29% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     564.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 87.21%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 57.34%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       955882
    Memory Throughput                   %        90.87
    DRAM Throughput                     %         2.86
    Duration                      usecond       741.09
    L1/TEX Cache Throughput             %        93.26
    L2 Cache Throughput                 %        18.74
    SM Active Cycles                cycle    931451.07
    Compute (SM) Throughput             %        90.87
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.77
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.31
    Issued Ipc Active     inst/cycle         1.77
    SM Busy                        %        44.31
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.04
    Mem Busy                               %        50.71
    Max Bandwidth                          %        90.87
    L1/TEX Hit Rate                        %        13.59
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.47
    Mem Pipes Busy                         %        90.87
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.30
    Issued Warp Per Scheduler                        0.44
    No Eligible                            %        55.70
    Active Warps Per Scheduler          warp        10.57
    Eligible Warps Per Scheduler        warp         1.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.126%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.57 active warps per scheduler, but only an average of 1.82 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.86
    Warp Cycles Per Executed Instruction           cycle        23.86
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.126%                                                                                          
          On average, each warp of this kernel spends 10.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 43.4% of the total average of 23.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    412566.07
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    412689.60
    Issued Instructions                             inst     95743988
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.09
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.126%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185906.67
    Total DRAM Elapsed Cycles        cycle     41754624
    Average L1 Active Cycles         cycle    931451.07
    Total L1 Elapsed Cycles          cycle     55440358
    Average L2 Active Cycles         cycle    783913.50
    Total L2 Elapsed Cycles          cycle     19472472
    Average SM Active Cycles         cycle    931451.07
    Total SM Elapsed Cycles          cycle     55440358
    Average SMSP Active Cycles       cycle    931584.47
    Total SMSP Elapsed Cycles        cycle    221761432
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       909608
    Memory Throughput                   %        92.40
    DRAM Throughput                     %         3.01
    Duration                      usecond       705.22
    L1/TEX Cache Throughput             %        94.02
    L2 Cache Throughput                 %        42.02
    SM Active Cycles                cycle    893959.41
    Compute (SM) Throughput             %        32.82
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.61
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        11.30
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.96%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.65
    Mem Busy                               %        92.40
    Max Bandwidth                          %        41.71
    L1/TEX Hit Rate                        %        78.21
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        97.09
    Mem Pipes Busy                         %        32.82
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 81.53%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 47.01%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097194 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 56.41%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.01
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.99
    Active Warps Per Scheduler          warp         3.78
    Eligible Warps Per Scheduler        warp         0.17
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.6%                                                                                      
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.78 active warps per scheduler, but only an average of 0.17 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.73
    Warp Cycles Per Executed Instruction           cycle        37.74
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.6%                                                                                            
          On average, each warp of this kernel spends 13.0 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.3% of the total average of 37.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.6%                                                                                            
          On average, each warp of this kernel spends 12.0 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 31.9% of the total average of 37.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.64%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     85857.10
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst     85874.57
    Issued Instructions                             inst     19922900
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           33
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.20
    Achieved Active Warps Per SM           warp        14.50
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.6%                                                                                            
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (30.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.6%                                                                                            
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       185832
    Total DRAM Elapsed Cycles        cycle     39733248
    Average L1 Active Cycles         cycle    893959.41
    Total L1 Elapsed Cycles          cycle     52756732
    Average L2 Active Cycles         cycle    763884.62
    Total L2 Elapsed Cycles          cycle     18530136
    Average SM Active Cycles         cycle    893959.41
    Total SM Elapsed Cycles          cycle     52756732
    Average SMSP Active Cycles       cycle    858235.65
    Total SMSP Elapsed Cycles        cycle    211026928
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.027%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 5.33% above the average, while the minimum instance value is 5.33% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     564.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 87.22%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 57.33%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 17, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       956438
    Memory Throughput                   %        90.85
    DRAM Throughput                     %         2.86
    Duration                      usecond       741.54
    L1/TEX Cache Throughput             %        93.24
    L2 Cache Throughput                 %        18.79
    SM Active Cycles                cycle    931610.52
    Compute (SM) Throughput             %        90.85
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.77
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.30
    Issued Ipc Active     inst/cycle         1.77
    SM Busy                        %        44.30
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.03
    Mem Busy                               %        50.69
    Max Bandwidth                          %        90.85
    L1/TEX Hit Rate                        %        13.50
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.31
    Mem Pipes Busy                         %        90.85
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.30
    Issued Warp Per Scheduler                        0.44
    No Eligible                            %        55.70
    Active Warps Per Scheduler          warp        10.56
    Eligible Warps Per Scheduler        warp         1.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.147%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.56 active warps per scheduler, but only an average of 1.82 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.85
    Warp Cycles Per Executed Instruction           cycle        23.85
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.147%                                                                                          
          On average, each warp of this kernel spends 10.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 43.4% of the total average of 23.8 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    412566.07
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    412688.72
    Issued Instructions                             inst     95743784
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.09
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.147%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     41779200
    Average L1 Active Cycles         cycle    931610.52
    Total L1 Elapsed Cycles          cycle     55452900
    Average L2 Active Cycles         cycle    786673.67
    Total L2 Elapsed Cycles          cycle     19482144
    Average SM Active Cycles         cycle    931610.52
    Total SM Elapsed Cycles          cycle     55452900
    Average SMSP Active Cycles       cycle    931545.19
    Total SMSP Elapsed Cycles        cycle    221811600
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 18, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       909892
    Memory Throughput                   %        92.37
    DRAM Throughput                     %         3.01
    Duration                      usecond       705.41
    L1/TEX Cache Throughput             %        94.09
    L2 Cache Throughput                 %        42.05
    SM Active Cycles                cycle    893308.64
    Compute (SM) Throughput             %        32.81
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.61
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        11.31
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.95%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.65
    Mem Busy                               %        92.37
    Max Bandwidth                          %        41.66
    L1/TEX Hit Rate                        %        78.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        96.41
    Mem Pipes Busy                         %        32.81
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 81.5%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 47.04%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097198 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 56.45%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.01
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.99
    Active Warps Per Scheduler          warp         3.77
    Eligible Warps Per Scheduler        warp         0.17
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.629%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.77 active warps per scheduler, but only an average of 0.17 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.71
    Warp Cycles Per Executed Instruction           cycle        37.72
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.629%                                                                                          
          On average, each warp of this kernel spends 13.0 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.4% of the total average of 37.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.629%                                                                                          
          On average, each warp of this kernel spends 12.1 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 32.0% of the total average of 37.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.63%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     85857.10
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst     85874.72
    Issued Instructions                             inst     19922934
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           33
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.22
    Achieved Active Warps Per SM           warp        14.50
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.629%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (30.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.629%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       185832
    Total DRAM Elapsed Cycles        cycle     39744512
    Average L1 Active Cycles         cycle    893308.64
    Total L1 Elapsed Cycles          cycle     52773156
    Average L2 Active Cycles         cycle    763028.17
    Total L2 Elapsed Cycles          cycle     18535992
    Average SM Active Cycles         cycle    893308.64
    Total SM Elapsed Cycles          cycle     52773156
    Average SMSP Active Cycles       cycle    858248.21
    Total SMSP Elapsed Cycles        cycle    211092624
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.019%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 5.32% above the average, while the minimum instance value is 5.60% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     564.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 87.09%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 57.27%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 19, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       954195
    Memory Throughput                   %        91.03
    DRAM Throughput                     %         2.87
    Duration                      usecond       739.78
    L1/TEX Cache Throughput             %        93.27
    L2 Cache Throughput                 %        18.79
    SM Active Cycles                cycle    931354.76
    Compute (SM) Throughput             %        91.03
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.77
    Executed Ipc Elapsed  inst/cycle         1.73
    Issue Slots Busy               %        44.31
    Issued Ipc Active     inst/cycle         1.77
    SM Busy                        %        44.31
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.05
    Mem Busy                               %        50.80
    Max Bandwidth                          %        91.03
    L1/TEX Hit Rate                        %        13.67
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.28
    Mem Pipes Busy                         %        91.03
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.30
    Issued Warp Per Scheduler                        0.44
    No Eligible                            %        55.70
    Active Warps Per Scheduler          warp        10.57
    Eligible Warps Per Scheduler        warp         1.81
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.966%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.57 active warps per scheduler, but only an average of 1.81 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.86
    Warp Cycles Per Executed Instruction           cycle        23.86
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.966%                                                                                          
          On average, each warp of this kernel spends 10.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 43.4% of the total average of 23.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    412566.07
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    412688.83
    Issued Instructions                             inst     95743809
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.09
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.966%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     41680896
    Average L1 Active Cycles         cycle    931354.76
    Total L1 Elapsed Cycles          cycle     55342584
    Average L2 Active Cycles         cycle    784248.17
    Total L2 Elapsed Cycles          cycle     19438320
    Average SM Active Cycles         cycle    931354.76
    Total SM Elapsed Cycles          cycle     55342584
    Average SMSP Active Cycles       cycle    931515.47
    Total SMSP Elapsed Cycles        cycle    221370336
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 20, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle       909616
    Memory Throughput                   %        92.40
    DRAM Throughput                     %         3.01
    Duration                      usecond       705.22
    L1/TEX Cache Throughput             %        94.03
    L2 Cache Throughput                 %        42.06
    SM Active Cycles                cycle    893813.21
    Compute (SM) Throughput             %        32.82
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.61
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        11.31
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.96%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        12.65
    Mem Busy                               %        92.40
    Max Bandwidth                          %        41.77
    L1/TEX Hit Rate                        %        78.22
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        96.28
    Mem Pipes Busy                         %        32.82
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 81.53%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 47.01%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097205 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 56.42%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.01
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.99
    Active Warps Per Scheduler          warp         3.77
    Eligible Warps Per Scheduler        warp         0.17
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.601%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.77 active warps per scheduler, but only an average of 0.17 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.73
    Warp Cycles Per Executed Instruction           cycle        37.73
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.601%                                                                                          
          On average, each warp of this kernel spends 12.9 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.3% of the total average of 37.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.601%                                                                                          
          On average, each warp of this kernel spends 12.0 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 31.9% of the total average of 37.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.64%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     85857.10
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst     85875.49
    Issued Instructions                             inst     19923113
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                0.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           33
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.19
    Achieved Active Warps Per SM           warp        14.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.601%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (30.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.601%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       185832
    Total DRAM Elapsed Cycles        cycle     39733248
    Average L1 Active Cycles         cycle    893813.21
    Total L1 Elapsed Cycles          cycle     52757460
    Average L2 Active Cycles         cycle    763573.96
    Total L2 Elapsed Cycles          cycle     18530448
    Average SM Active Cycles         cycle    893813.21
    Total SM Elapsed Cycles          cycle     52757460
    Average SMSP Active Cycles       cycle    858308.85
    Total SMSP Elapsed Cycles        cycle    211029840
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.039%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 5.34% above the average, while the minimum instance value is 5.27% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     564.97
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 87.18%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 57.32%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

