==PROF== Connected to process 2593 (/home/dilaygulhan/cuda_code/CENG443_project/src/hgemm_stream_dilay)
==PROF== Profiling "hgemm_normal" - 0: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 1: 0%....50%....100% - 37 passes
==PROF== Profiling "hgemm_normal" - 2: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 3: 0%....50%....100% - 37 passes
==PROF== Profiling "hgemm_normal" - 4: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 5: 0%....50%....100% - 37 passes
==PROF== Profiling "hgemm_normal" - 6: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 7: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 8: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 9: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 10: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 11: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 12: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 13: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 14: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 15: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 16: 0%....50%....100% - 37 passes
==PROF== Profiling "hgemm_tensor_core" - 17: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 18: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 19: 0%....50%....100% - 37 passes
==PROF== Profiling "hgemm_normal" - 20: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 21: 0%....50%....100% - 37 passes
==PROF== Profiling "hgemm_normal" - 22: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 23: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 24: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 25: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 26: 0%....50%....100% - 37 passes
==PROF== Profiling "hgemm_tensor_core" - 27: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 28: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 29: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 30: 0%....50%....100% - 37 passes
==PROF== Profiling "hgemm_tensor_core" - 31: 0%....50%....100% - 38 passes
Total execution time: 20046.755859 ms
==PROF== Disconnected from process 2593
[2593] hgemm_stream_dilay@127.0.0.1
  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1367681
    Memory Throughput                   %        92.05
    DRAM Throughput                     %        13.53
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        93.89
    L2 Cache Throughput                 %         8.42
    SM Active Cycles                cycle   1341485.45
    Compute (SM) Throughput             %        92.05
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.78
    Executed Ipc Elapsed  inst/cycle         1.75
    Issue Slots Busy               %        44.60
    Issued Ipc Active     inst/cycle         1.78
    SM Busy                        %        44.60
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (25.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        60.58
    Mem Busy                               %        51.38
    Max Bandwidth                          %        92.05
    L1/TEX Hit Rate                        %        16.30
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        41.91
    Mem Pipes Busy                         %        92.05
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.58
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.42
    Active Warps Per Scheduler          warp        11.01
    Eligible Warps Per Scheduler        warp         1.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.949%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.01 active warps per scheduler, but only an average of 1.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        24.70
    Warp Cycles Per Executed Instruction           cycle        24.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.949%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 41.8% of the total average of 24.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    598220.80
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    598344.59
    Issued Instructions                             inst     95735135
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.81
    Achieved Active Warps Per SM           warp        44.07
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1065554
    Total DRAM Elapsed Cycles        cycle     62983168
    Average L1 Active Cycles         cycle   1341485.45
    Total L1 Elapsed Cycles          cycle     54731522
    Average L2 Active Cycles         cycle   1289827.78
    Total L2 Elapsed Cycles          cycle     42144864
    Average SM Active Cycles         cycle   1341485.45
    Total SM Elapsed Cycles          cycle     54731522
    Average SMSP Active Cycles       cycle   1342068.40
    Total SMSP Elapsed Cycles        cycle    218926088
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 14, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1376066
    Memory Throughput                   %        88.56
    DRAM Throughput                     %         2.09
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        96.95
    L2 Cache Throughput                 %         6.36
    SM Active Cycles                cycle   1255163.32
    Compute (SM) Throughput             %        31.50
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.47
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.92
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        11.67
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.79%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.35
    Mem Busy                               %        88.56
    Max Bandwidth                          %        31.50
    L1/TEX Hit Rate                        %        92.51
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        90.16
    Mem Pipes Busy                         %        31.50
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 78.14%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 48.47%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097240 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.17%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.98
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.02
    Active Warps Per Scheduler          warp         3.48
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.44%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.48 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        34.90
    Warp Cycles Per Executed Instruction           cycle        34.91
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.44%                                                                                          
          On average, each warp of this kernel spends 12.5 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 35.7% of the total average of 34.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.05%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    124492.80
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst    124516.38
    Issued Instructions                             inst     19922620
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              40
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        28.88
    Achieved Active Warps Per SM           warp        13.86
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.44%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       165470
    Total DRAM Elapsed Cycles        cycle     63369216
    Average L1 Active Cycles         cycle   1255163.32
    Total L1 Elapsed Cycles          cycle     54962832
    Average L2 Active Cycles         cycle   1098457.06
    Total L2 Elapsed Cycles          cycle     42403200
    Average SM Active Cycles         cycle   1255163.32
    Total SM Elapsed Cycles          cycle     54962832
    Average SMSP Active Cycles       cycle   1248265.65
    Total SMSP Elapsed Cycles        cycle    219851328
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.697%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.43% above the average, while the minimum instance value is 6.77% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.417%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 9.27% above the average, while the minimum instance value is 6.50% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.697%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.43% above the average, while the minimum instance value is 6.77% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     819.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 73.08%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 53.29%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1370737
    Memory Throughput                   %        92.18
    DRAM Throughput                     %        13.51
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        93.86
    L2 Cache Throughput                 %         8.62
    SM Active Cycles                cycle   1341910.15
    Compute (SM) Throughput             %        92.18
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.78
    Executed Ipc Elapsed  inst/cycle         1.75
    Issue Slots Busy               %        44.59
    Issued Ipc Active     inst/cycle         1.78
    SM Busy                        %        44.59
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (25.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        60.48
    Mem Busy                               %        51.45
    Max Bandwidth                          %        92.18
    L1/TEX Hit Rate                        %        16.38
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        41.75
    Mem Pipes Busy                         %        92.18
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.61
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.39
    Active Warps Per Scheduler          warp        11.02
    Eligible Warps Per Scheduler        warp         1.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.824%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.02 active warps per scheduler, but only an average of 1.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        24.70
    Warp Cycles Per Executed Instruction           cycle        24.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.824%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 41.7% of the total average of 24.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    598220.80
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    598343.46
    Issued Instructions                             inst     95734954
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.78
    Achieved Active Warps Per SM           warp        44.06
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1066172
    Total DRAM Elapsed Cycles        cycle     63123456
    Average L1 Active Cycles         cycle   1341910.15
    Total L1 Elapsed Cycles          cycle     54657162
    Average L2 Active Cycles         cycle   1258738.25
    Total L2 Elapsed Cycles          cycle     41156384
    Average SM Active Cycles         cycle   1341910.15
    Total SM Elapsed Cycles          cycle     54657162
    Average SMSP Active Cycles       cycle   1341280.77
    Total SMSP Elapsed Cycles        cycle    218628648
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 16, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1372425
    Memory Throughput                   %        88.70
    DRAM Throughput                     %         2.11
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        96.95
    L2 Cache Throughput                 %         6.61
    SM Active Cycles                cycle   1255181.62
    Compute (SM) Throughput             %        31.55
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.47
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.92
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        11.67
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.79%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.43
    Mem Busy                               %        88.70
    Max Bandwidth                          %        31.55
    L1/TEX Hit Rate                        %        92.63
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        91.88
    Mem Pipes Busy                         %        31.55
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 78.26%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 48.47%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097251 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.17%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.98
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.02
    Active Warps Per Scheduler          warp         3.50
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.3%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.50 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        35.04
    Warp Cycles Per Executed Instruction           cycle        35.05
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.3%                                                                                           
          On average, each warp of this kernel spends 12.2 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.7% of the total average of 35.0 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.07%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    124492.80
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst    124515.37
    Issued Instructions                             inst     19922459
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              40
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        28.95
    Achieved Active Warps Per SM           warp        13.90
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.3%                                                                                           
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       166384
    Total DRAM Elapsed Cycles        cycle     63201280
    Average L1 Active Cycles         cycle   1255181.62
    Total L1 Elapsed Cycles          cycle     54877914
    Average L2 Active Cycles         cycle   1075211.88
    Total L2 Elapsed Cycles          cycle     41206560
    Average SM Active Cycles         cycle   1255181.62
    Total SM Elapsed Cycles          cycle     54877914
    Average SMSP Active Cycles       cycle   1247632.22
    Total SMSP Elapsed Cycles        cycle    219511656
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.067%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 8.82% above the average, while the minimum instance value is 6.63% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.36%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 9.19% above the average, while the minimum instance value is 6.64% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.067%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.82% above the average, while the minimum instance value is 6.63% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     819.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 73.61%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 53.37%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 17, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1367021
    Memory Throughput                   %        92.07
    DRAM Throughput                     %        13.53
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        93.85
    L2 Cache Throughput                 %         8.64
    SM Active Cycles                cycle   1342085.95
    Compute (SM) Throughput             %        92.07
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.78
    Executed Ipc Elapsed  inst/cycle         1.75
    Issue Slots Busy               %        44.58
    Issued Ipc Active     inst/cycle         1.78
    SM Busy                        %        44.58
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.9%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        60.57
    Mem Busy                               %        51.39
    Max Bandwidth                          %        92.07
    L1/TEX Hit Rate                        %        16.24
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        42.02
    Mem Pipes Busy                         %        92.07
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.60
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.40
    Active Warps Per Scheduler          warp        11.02
    Eligible Warps Per Scheduler        warp         1.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.928%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.02 active warps per scheduler, but only an average of 1.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        24.71
    Warp Cycles Per Executed Instruction           cycle        24.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.928%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 41.7% of the total average of 24.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    598220.80
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    598342.50
    Issued Instructions                             inst     95734800
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.78
    Achieved Active Warps Per SM           warp        44.05
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1064840
    Total DRAM Elapsed Cycles        cycle     62952448
    Average L1 Active Cycles         cycle   1342085.95
    Total L1 Elapsed Cycles          cycle     54719052
    Average L2 Active Cycles         cycle   1260234.03
    Total L2 Elapsed Cycles          cycle     41044640
    Average SM Active Cycles         cycle   1342085.95
    Total SM Elapsed Cycles          cycle     54719052
    Average SMSP Active Cycles       cycle   1341519.30
    Total SMSP Elapsed Cycles        cycle    218876208
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 18, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1374617
    Memory Throughput                   %        88.44
    DRAM Throughput                     %         2.12
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        97.07
    L2 Cache Throughput                 %         6.56
    SM Active Cycles                cycle   1253601.93
    Compute (SM) Throughput             %        31.46
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.47
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.93
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        11.69
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.79%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.48
    Mem Busy                               %        88.44
    Max Bandwidth                          %        31.46
    L1/TEX Hit Rate                        %        92.56
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        90.28
    Mem Pipes Busy                         %        31.46
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 78.03%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 48.53%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097260 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.24%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.97
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.03
    Active Warps Per Scheduler          warp         3.48
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.56%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.48 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        34.86
    Warp Cycles Per Executed Instruction           cycle        34.87
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.56%                                                                                          
          On average, each warp of this kernel spends 12.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 35.6% of the total average of 34.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.03%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    124492.80
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst    124515.64
    Issued Instructions                             inst     19922503
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              40
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        28.95
    Achieved Active Warps Per SM           warp        13.90
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.56%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       167504
    Total DRAM Elapsed Cycles        cycle     63301632
    Average L1 Active Cycles         cycle   1253601.93
    Total L1 Elapsed Cycles          cycle     55041558
    Average L2 Active Cycles         cycle   1075784.88
    Total L2 Elapsed Cycles          cycle     41272480
    Average SM Active Cycles         cycle   1253601.93
    Total SM Elapsed Cycles          cycle     55041558
    Average SMSP Active Cycles       cycle   1249104.18
    Total SMSP Elapsed Cycles        cycle    220166232
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.799%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 8.56% above the average, while the minimum instance value is 6.47% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.418%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 9.27% above the average, while the minimum instance value is 6.65% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.799%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.56% above the average, while the minimum instance value is 6.47% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     819.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 73.53%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 53.14%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 19, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1369154
    Memory Throughput                   %        91.97
    DRAM Throughput                     %        13.52
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        93.87
    L2 Cache Throughput                 %         8.63
    SM Active Cycles                cycle   1341786.98
    Compute (SM) Throughput             %        91.97
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.78
    Executed Ipc Elapsed  inst/cycle         1.75
    Issue Slots Busy               %        44.59
    Issued Ipc Active     inst/cycle         1.78
    SM Busy                        %        44.59
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (25.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        60.53
    Mem Busy                               %        51.33
    Max Bandwidth                          %        91.97
    L1/TEX Hit Rate                        %        16.26
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        41.92
    Mem Pipes Busy                         %        91.97
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.60
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.40
    Active Warps Per Scheduler          warp        11.02
    Eligible Warps Per Scheduler        warp         1.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.033%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.02 active warps per scheduler, but only an average of 1.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        24.70
    Warp Cycles Per Executed Instruction           cycle        24.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.033%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 41.7% of the total average of 24.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    598220.80
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    598344.41
    Issued Instructions                             inst     95735106
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.81
    Achieved Active Warps Per SM           warp        44.07
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1065780
    Total DRAM Elapsed Cycles        cycle     63049728
    Average L1 Active Cycles         cycle   1341786.98
    Total L1 Elapsed Cycles          cycle     54781136
    Average L2 Active Cycles         cycle   1260217.09
    Total L2 Elapsed Cycles          cycle     41108320
    Average SM Active Cycles         cycle   1341786.98
    Total SM Elapsed Cycles          cycle     54781136
    Average SMSP Active Cycles       cycle   1341495.77
    Total SMSP Elapsed Cycles        cycle    219124544
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 20, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1372032
    Memory Throughput                   %        88.72
    DRAM Throughput                     %         2.12
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        96.90
    L2 Cache Throughput                 %         6.61
    SM Active Cycles                cycle   1255842.43
    Compute (SM) Throughput             %        31.56
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.91
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        11.67
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.8%                                                                                     
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.47
    Mem Busy                               %        88.72
    Max Bandwidth                          %        31.56
    L1/TEX Hit Rate                        %        92.57
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        89.81
    Mem Pipes Busy                         %        31.56
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 78.28%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 48.45%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097242 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.14%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.98
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.02
    Active Warps Per Scheduler          warp         3.48
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.28%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.48 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        34.90
    Warp Cycles Per Executed Instruction           cycle        34.91
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.28%                                                                                          
          On average, each warp of this kernel spends 12.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 35.4% of the total average of 34.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.08%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    124492.80
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst    124515.79
    Issued Instructions                             inst     19922527
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              40
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        28.86
    Achieved Active Warps Per SM           warp        13.85
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.28%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       167056
    Total DRAM Elapsed Cycles        cycle     63181824
    Average L1 Active Cycles         cycle   1255842.43
    Total L1 Elapsed Cycles          cycle     54866510
    Average L2 Active Cycles         cycle   1076776.28
    Total L2 Elapsed Cycles          cycle     41194976
    Average SM Active Cycles         cycle   1255842.43
    Total SM Elapsed Cycles          cycle     54866510
    Average SMSP Active Cycles       cycle   1247948.68
    Total SMSP Elapsed Cycles        cycle    219466040
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.677%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.39% above the average, while the minimum instance value is 6.83% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.284%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 9.11% above the average, while the minimum instance value is 6.38% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.677%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.39% above the average, while the minimum instance value is 6.83% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     819.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 73.74%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 53.41%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1367284
    Memory Throughput                   %        91.93
    DRAM Throughput                     %        13.53
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        93.85
    L2 Cache Throughput                 %         8.64
    SM Active Cycles                cycle   1342020.10
    Compute (SM) Throughput             %        91.93
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.78
    Executed Ipc Elapsed  inst/cycle         1.75
    Issue Slots Busy               %        44.59
    Issued Ipc Active     inst/cycle         1.78
    SM Busy                        %        44.59
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (25.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        60.55
    Mem Busy                               %        51.31
    Max Bandwidth                          %        91.93
    L1/TEX Hit Rate                        %        16.31
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        41.85
    Mem Pipes Busy                         %        91.93
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.60
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.40
    Active Warps Per Scheduler          warp        11.02
    Eligible Warps Per Scheduler        warp         1.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.066%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.02 active warps per scheduler, but only an average of 1.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        24.71
    Warp Cycles Per Executed Instruction           cycle        24.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.066%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 41.7% of the total average of 24.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    598220.80
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    598343.44
    Issued Instructions                             inst     95734950
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.80
    Achieved Active Warps Per SM           warp        44.07
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1064814
    Total DRAM Elapsed Cycles        cycle     62963712
    Average L1 Active Cycles         cycle   1342020.10
    Total L1 Elapsed Cycles          cycle     54800966
    Average L2 Active Cycles         cycle   1258147.97
    Total L2 Elapsed Cycles          cycle     41052736
    Average SM Active Cycles         cycle   1342020.10
    Total SM Elapsed Cycles          cycle     54800966
    Average SMSP Active Cycles       cycle   1341660.07
    Total SMSP Elapsed Cycles        cycle    219203864
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 14, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1377180
    Memory Throughput                   %        88.52
    DRAM Throughput                     %         2.15
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        96.86
    L2 Cache Throughput                 %         6.55
    SM Active Cycles                cycle   1256311.88
    Compute (SM) Throughput             %        31.49
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.91
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        11.66
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.8%                                                                                     
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.64
    Mem Busy                               %        88.52
    Max Bandwidth                          %        31.49
    L1/TEX Hit Rate                        %        92.59
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        90.98
    Mem Pipes Busy                         %        31.49
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 78.11%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 48.43%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097234 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.12%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.98
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.02
    Active Warps Per Scheduler          warp         3.49
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.48%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.49 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        34.94
    Warp Cycles Per Executed Instruction           cycle        34.95
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.48%                                                                                          
          On average, each warp of this kernel spends 12.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 35.2% of the total average of 34.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.04%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    124492.80
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst    124515.48
    Issued Instructions                             inst     19922477
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              40
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        28.84
    Achieved Active Warps Per SM           warp        13.85
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.48%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       170676
    Total DRAM Elapsed Cycles        cycle     63419392
    Average L1 Active Cycles         cycle   1256311.88
    Total L1 Elapsed Cycles          cycle     54989576
    Average L2 Active Cycles         cycle   1071303.78
    Total L2 Elapsed Cycles          cycle     41349152
    Average SM Active Cycles         cycle   1256311.88
    Total SM Elapsed Cycles          cycle     54989576
    Average SMSP Active Cycles       cycle   1247485.89
    Total SMSP Elapsed Cycles        cycle    219958304
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.691%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.42% above the average, while the minimum instance value is 6.74% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.292%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 9.14% above the average, while the minimum instance value is 6.48% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.691%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.42% above the average, while the minimum instance value is 6.74% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     819.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 73.09%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 53.31%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1367646
    Memory Throughput                   %        92.18
    DRAM Throughput                     %        13.55
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        93.90
    L2 Cache Throughput                 %         8.64
    SM Active Cycles                cycle   1341293.35
    Compute (SM) Throughput             %        92.18
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.78
    Executed Ipc Elapsed  inst/cycle         1.75
    Issue Slots Busy               %        44.61
    Issued Ipc Active     inst/cycle         1.78
    SM Busy                        %        44.61
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (25.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        60.63
    Mem Busy                               %        51.45
    Max Bandwidth                          %        92.18
    L1/TEX Hit Rate                        %        16.30
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        41.85
    Mem Pipes Busy                         %        92.18
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.58
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.42
    Active Warps Per Scheduler          warp        11.01
    Eligible Warps Per Scheduler        warp         1.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.822%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.01 active warps per scheduler, but only an average of 1.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        24.70
    Warp Cycles Per Executed Instruction           cycle        24.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.822%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 41.7% of the total average of 24.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    598220.80
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    598343.07
    Issued Instructions                             inst     95734891
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.83
    Achieved Active Warps Per SM           warp        44.08
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1066436
    Total DRAM Elapsed Cycles        cycle     62980096
    Average L1 Active Cycles         cycle   1341293.35
    Total L1 Elapsed Cycles          cycle     54656166
    Average L2 Active Cycles         cycle   1259648.03
    Total L2 Elapsed Cycles          cycle     41063424
    Average SM Active Cycles         cycle   1341293.35
    Total SM Elapsed Cycles          cycle     54656166
    Average SMSP Active Cycles       cycle   1342169.10
    Total SMSP Elapsed Cycles        cycle    218624664
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 16, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1372800
    Memory Throughput                   %        87.97
    DRAM Throughput                     %         2.16
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        97.04
    L2 Cache Throughput                 %         6.57
    SM Active Cycles                cycle   1254104.88
    Compute (SM) Throughput             %        31.29
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.93
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        11.68
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.79%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.69
    Mem Busy                               %        87.97
    Max Bandwidth                          %        31.29
    L1/TEX Hit Rate                        %        92.47
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        88.55
    Mem Pipes Busy                         %        31.29
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 77.62%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 48.52%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097238 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.22%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.98
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.02
    Active Warps Per Scheduler          warp         3.48
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.03%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.48 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        34.92
    Warp Cycles Per Executed Instruction           cycle        34.92
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.03%                                                                                          
          On average, each warp of this kernel spends 12.5 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 35.7% of the total average of 34.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.96%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    124492.80
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst    124515.54
    Issued Instructions                             inst     19922486
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              40
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        28.86
    Achieved Active Warps Per SM           warp        13.85
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.03%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       171018
    Total DRAM Elapsed Cycles        cycle     63218688
    Average L1 Active Cycles         cycle   1254104.88
    Total L1 Elapsed Cycles          cycle     55333514
    Average L2 Active Cycles         cycle   1073927.50
    Total L2 Elapsed Cycles          cycle     41217696
    Average SM Active Cycles         cycle   1254104.88
    Total SM Elapsed Cycles          cycle     55333514
    Average SMSP Active Cycles       cycle   1248049.22
    Total SMSP Elapsed Cycles        cycle    221334056
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.084%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 8.92% above the average, while the minimum instance value is 6.55% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.205%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 9.09% above the average, while the minimum instance value is 6.90% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.084%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.92% above the average, while the minimum instance value is 6.55% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     819.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 73.5%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.88%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 17, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1368278
    Memory Throughput                   %        91.97
    DRAM Throughput                     %        13.53
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        93.87
    L2 Cache Throughput                 %         8.63
    SM Active Cycles                cycle   1341774.95
    Compute (SM) Throughput             %        91.97
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.78
    Executed Ipc Elapsed  inst/cycle         1.75
    Issue Slots Busy               %        44.59
    Issued Ipc Active     inst/cycle         1.78
    SM Busy                        %        44.59
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (25.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        60.56
    Mem Busy                               %        51.33
    Max Bandwidth                          %        91.97
    L1/TEX Hit Rate                        %        16.28
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        41.84
    Mem Pipes Busy                         %        91.97
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.60
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.40
    Active Warps Per Scheduler          warp        11.02
    Eligible Warps Per Scheduler        warp         1.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.029%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.02 active warps per scheduler, but only an average of 1.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        24.70
    Warp Cycles Per Executed Instruction           cycle        24.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.029%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 41.7% of the total average of 24.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    598220.80
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    598343.36
    Issued Instructions                             inst     95734938
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.81
    Achieved Active Warps Per SM           warp        44.07
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1065644
    Total DRAM Elapsed Cycles        cycle     63011840
    Average L1 Active Cycles         cycle   1341774.95
    Total L1 Elapsed Cycles          cycle     54778808
    Average L2 Active Cycles         cycle   1258468.59
    Total L2 Elapsed Cycles          cycle     41082400
    Average SM Active Cycles         cycle   1341774.95
    Total SM Elapsed Cycles          cycle     54778808
    Average SMSP Active Cycles       cycle   1341633.69
    Total SMSP Elapsed Cycles        cycle    219115232
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 18, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1380033
    Memory Throughput                   %        88.48
    DRAM Throughput                     %         2.11
    Duration                      msecond         1.14
    L1/TEX Cache Throughput             %        96.92
    L2 Cache Throughput                 %         6.51
    SM Active Cycles                cycle   1255542.88
    Compute (SM) Throughput             %        31.48
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.92
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        11.67
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.79%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.44
    Mem Busy                               %        88.48
    Max Bandwidth                          %        31.48
    L1/TEX Hit Rate                        %        92.56
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        91.69
    Mem Pipes Busy                         %        31.48
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 78.07%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 48.46%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097241 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.15%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.96
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.04
    Active Warps Per Scheduler          warp         3.48
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.48 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        34.97
    Warp Cycles Per Executed Instruction           cycle        34.98
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.52%                                                                                          
          On average, each warp of this kernel spends 12.2 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 34.9% of the total average of 35.0 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.04%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    124492.80
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst       124516
    Issued Instructions                             inst     19922560
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              40
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        28.87
    Achieved Active Warps Per SM           warp        13.86
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.52%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       167560
    Total DRAM Elapsed Cycles        cycle     63551488
    Average L1 Active Cycles         cycle   1255542.88
    Total L1 Elapsed Cycles          cycle     55012886
    Average L2 Active Cycles         cycle   1075099.47
    Total L2 Elapsed Cycles          cycle     41435072
    Average SM Active Cycles         cycle   1255542.88
    Total SM Elapsed Cycles          cycle     55012886
    Average SMSP Active Cycles       cycle   1250249.10
    Total SMSP Elapsed Cycles        cycle    220051544
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.88%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.63% above the average, while the minimum instance value is 6.81% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.359%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 9.20% above the average, while the minimum instance value is 6.72% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.88%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.63% above the average, while the minimum instance value is 6.81% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     819.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 73.19%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 53.25%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 19, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1369201
    Memory Throughput                   %        91.86
    DRAM Throughput                     %        13.51
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        93.87
    L2 Cache Throughput                 %         8.63
    SM Active Cycles                cycle   1341718.38
    Compute (SM) Throughput             %        91.86
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.78
    Executed Ipc Elapsed  inst/cycle         1.75
    Issue Slots Busy               %        44.60
    Issued Ipc Active     inst/cycle         1.78
    SM Busy                        %        44.60
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (25.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        60.47
    Mem Busy                               %        51.27
    Max Bandwidth                          %        91.86
    L1/TEX Hit Rate                        %        16.12
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        41.88
    Mem Pipes Busy                         %        91.86
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.59
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.41
    Active Warps Per Scheduler          warp        11.02
    Eligible Warps Per Scheduler        warp         1.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.144%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.02 active warps per scheduler, but only an average of 1.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        24.70
    Warp Cycles Per Executed Instruction           cycle        24.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.144%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 41.7% of the total average of 24.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    598220.80
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    598344.31
    Issued Instructions                             inst     95735089
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.83
    Achieved Active Warps Per SM           warp        44.08
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1064774
    Total DRAM Elapsed Cycles        cycle     63052800
    Average L1 Active Cycles         cycle   1341718.38
    Total L1 Elapsed Cycles          cycle     54847660
    Average L2 Active Cycles         cycle   1258773.75
    Total L2 Elapsed Cycles          cycle     41110080
    Average SM Active Cycles         cycle   1341718.38
    Total SM Elapsed Cycles          cycle     54847660
    Average SMSP Active Cycles       cycle   1341770.30
    Total SMSP Elapsed Cycles        cycle    219390640
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 20, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1374371
    Memory Throughput                   %        88.41
    DRAM Throughput                     %         2.13
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        96.88
    L2 Cache Throughput                 %         6.61
    SM Active Cycles                cycle   1256109.30
    Compute (SM) Throughput             %        31.45
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.41
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.91
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        11.67
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.8%                                                                                     
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.51
    Mem Busy                               %        88.41
    Max Bandwidth                          %        31.45
    L1/TEX Hit Rate                        %        92.64
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        89.02
    Mem Pipes Busy                         %        31.45
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 78.01%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 48.44%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097237 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.13%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.97
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.03
    Active Warps Per Scheduler          warp         3.48
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.59%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.48 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        34.94
    Warp Cycles Per Executed Instruction           cycle        34.95
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.59%                                                                                          
          On average, each warp of this kernel spends 12.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 35.5% of the total average of 34.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.03%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    124492.80
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst    124515.79
    Issued Instructions                             inst     19922527
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              40
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        28.81
    Achieved Active Warps Per SM           warp        13.83
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.59%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       168164
    Total DRAM Elapsed Cycles        cycle     63291392
    Average L1 Active Cycles         cycle   1256109.30
    Total L1 Elapsed Cycles          cycle     55059688
    Average L2 Active Cycles         cycle   1079231.72
    Total L2 Elapsed Cycles          cycle     41265056
    Average SM Active Cycles         cycle   1256109.30
    Total SM Elapsed Cycles          cycle     55059688
    Average SMSP Active Cycles       cycle   1248675.76
    Total SMSP Elapsed Cycles        cycle    220238752
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.79%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.54% above the average, while the minimum instance value is 6.65% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.116%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 8.95% above the average, while the minimum instance value is 6.77% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.79%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.54% above the average, while the minimum instance value is 6.65% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     819.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 73.78%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 53.23%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1368394
    Memory Throughput                   %        91.99
    DRAM Throughput                     %        13.55
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        93.86
    L2 Cache Throughput                 %         8.63
    SM Active Cycles                cycle   1341965.82
    Compute (SM) Throughput             %        91.99
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.78
    Executed Ipc Elapsed  inst/cycle         1.75
    Issue Slots Busy               %        44.59
    Issued Ipc Active     inst/cycle         1.78
    SM Busy                        %        44.59
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (25.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        60.65
    Mem Busy                               %        51.35
    Max Bandwidth                          %        91.99
    L1/TEX Hit Rate                        %        16.23
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        41.83
    Mem Pipes Busy                         %        91.99
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.58
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.42
    Active Warps Per Scheduler          warp        11.01
    Eligible Warps Per Scheduler        warp         1.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.006%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.01 active warps per scheduler, but only an average of 1.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        24.71
    Warp Cycles Per Executed Instruction           cycle        24.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.006%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 41.7% of the total average of 24.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    598220.80
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    598343.22
    Issued Instructions                             inst     95734916
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.81
    Achieved Active Warps Per SM           warp        44.07
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1067272
    Total DRAM Elapsed Cycles        cycle     63015936
    Average L1 Active Cycles         cycle   1341965.82
    Total L1 Elapsed Cycles          cycle     54765398
    Average L2 Active Cycles         cycle      1259163
    Total L2 Elapsed Cycles          cycle     41086048
    Average SM Active Cycles         cycle   1341965.82
    Total SM Elapsed Cycles          cycle     54765398
    Average SMSP Active Cycles       cycle   1342056.98
    Total SMSP Elapsed Cycles        cycle    219061592
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 14, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1375177
    Memory Throughput                   %        88.52
    DRAM Throughput                     %         2.09
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        96.95
    L2 Cache Throughput                 %         6.61
    SM Active Cycles                cycle   1255249.95
    Compute (SM) Throughput             %        31.49
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.92
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        11.67
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.79%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.35
    Mem Busy                               %        88.52
    Max Bandwidth                          %        31.49
    L1/TEX Hit Rate                        %        92.53
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        89.26
    Mem Pipes Busy                         %        31.49
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 78.1%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 48.47%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097216 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.17%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.98
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.02
    Active Warps Per Scheduler          warp         3.48
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.48%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.48 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        34.92
    Warp Cycles Per Executed Instruction           cycle        34.92
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.48%                                                                                          
          On average, each warp of this kernel spends 12.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 35.2% of the total average of 34.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.04%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    124492.80
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst    124514.87
    Issued Instructions                             inst     19922379
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              40
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        28.89
    Achieved Active Warps Per SM           warp        13.87
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.48%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       165396
    Total DRAM Elapsed Cycles        cycle     63328256
    Average L1 Active Cycles         cycle   1255249.95
    Total L1 Elapsed Cycles          cycle     54990714
    Average L2 Active Cycles         cycle   1074068.03
    Total L2 Elapsed Cycles          cycle     41289312
    Average SM Active Cycles         cycle   1255249.95
    Total SM Elapsed Cycles          cycle     54990714
    Average SMSP Active Cycles       cycle   1247969.59
    Total SMSP Elapsed Cycles        cycle    219962856
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.91%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 8.66% above the average, while the minimum instance value is 6.62% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.198%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 9.03% above the average, while the minimum instance value is 6.43% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.91%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.66% above the average, while the minimum instance value is 6.62% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     819.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 73.38%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 53.26%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1368276
    Memory Throughput                   %        91.97
    DRAM Throughput                     %        13.52
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        93.85
    L2 Cache Throughput                 %         8.64
    SM Active Cycles                cycle   1342085.05
    Compute (SM) Throughput             %        91.97
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.78
    Executed Ipc Elapsed  inst/cycle         1.75
    Issue Slots Busy               %        44.58
    Issued Ipc Active     inst/cycle         1.78
    SM Busy                        %        44.58
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (24.9%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        60.53
    Mem Busy                               %        51.33
    Max Bandwidth                          %        91.97
    L1/TEX Hit Rate                        %        16.27
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        41.89
    Mem Pipes Busy                         %        91.97
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.60
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.40
    Active Warps Per Scheduler          warp        11.02
    Eligible Warps Per Scheduler        warp         1.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.029%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.02 active warps per scheduler, but only an average of 1.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        24.71
    Warp Cycles Per Executed Instruction           cycle        24.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.029%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 41.7% of the total average of 24.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    598220.80
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    598343.03
    Issued Instructions                             inst     95734885
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.77
    Achieved Active Warps Per SM           warp        44.05
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1065120
    Total DRAM Elapsed Cycles        cycle     63008768
    Average L1 Active Cycles         cycle   1342085.05
    Total L1 Elapsed Cycles          cycle     54779086
    Average L2 Active Cycles         cycle   1258574.09
    Total L2 Elapsed Cycles          cycle     41082304
    Average SM Active Cycles         cycle   1342085.05
    Total SM Elapsed Cycles          cycle     54779086
    Average SMSP Active Cycles       cycle   1341431.35
    Total SMSP Elapsed Cycles        cycle    219116344
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 16, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1376338
    Memory Throughput                   %        88.28
    DRAM Throughput                     %         2.09
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        96.92
    L2 Cache Throughput                 %         6.59
    SM Active Cycles                cycle   1255574.95
    Compute (SM) Throughput             %        31.40
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.47
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.92
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        11.67
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.79%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.35
    Mem Busy                               %        88.28
    Max Bandwidth                          %        31.40
    L1/TEX Hit Rate                        %        92.60
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        89.44
    Mem Pipes Busy                         %        31.40
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 77.9%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 48.46%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097220 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.15%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.98
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.02
    Active Warps Per Scheduler          warp         3.49
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.72%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.49 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        34.92
    Warp Cycles Per Executed Instruction           cycle        34.93
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.72%                                                                                          
          On average, each warp of this kernel spends 12.5 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 35.7% of the total average of 34.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.01%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    124492.80
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst    124515.23
    Issued Instructions                             inst     19922436
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              40
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        28.82
    Achieved Active Warps Per SM           warp        13.83
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.72%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       165528
    Total DRAM Elapsed Cycles        cycle     63381504
    Average L1 Active Cycles         cycle   1255574.95
    Total L1 Elapsed Cycles          cycle     55137682
    Average L2 Active Cycles         cycle   1075362.22
    Total L2 Elapsed Cycles          cycle     41324192
    Average SM Active Cycles         cycle   1255574.95
    Total SM Elapsed Cycles          cycle     55137682
    Average SMSP Active Cycles       cycle   1247630.18
    Total SMSP Elapsed Cycles        cycle    220550728
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.912%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.69% above the average, while the minimum instance value is 6.77% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.296%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 9.17% above the average, while the minimum instance value is 6.34% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.912%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.69% above the average, while the minimum instance value is 6.77% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     819.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 73.41%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 53.13%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 17, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1369100
    Memory Throughput                   %        92.07
    DRAM Throughput                     %        13.54
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        93.91
    L2 Cache Throughput                 %         8.64
    SM Active Cycles                cycle   1341177.93
    Compute (SM) Throughput             %        92.07
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.78
    Executed Ipc Elapsed  inst/cycle         1.75
    Issue Slots Busy               %        44.61
    Issued Ipc Active     inst/cycle         1.78
    SM Busy                        %        44.61
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (25.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        60.60
    Mem Busy                               %        51.39
    Max Bandwidth                          %        92.07
    L1/TEX Hit Rate                        %        16.31
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        41.80
    Mem Pipes Busy                         %        92.07
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.60
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.40
    Active Warps Per Scheduler          warp        11.01
    Eligible Warps Per Scheduler        warp         1.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.926%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.01 active warps per scheduler, but only an average of 1.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        24.70
    Warp Cycles Per Executed Instruction           cycle        24.70
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.926%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 41.7% of the total average of 24.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    598220.80
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    598341.81
    Issued Instructions                             inst     95734690
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.84
    Achieved Active Warps Per SM           warp        44.08
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1067102
    Total DRAM Elapsed Cycles        cycle     63049728
    Average L1 Active Cycles         cycle   1341177.93
    Total L1 Elapsed Cycles          cycle     54717998
    Average L2 Active Cycles         cycle   1260369.06
    Total L2 Elapsed Cycles          cycle     41107104
    Average SM Active Cycles         cycle   1341177.93
    Total SM Elapsed Cycles          cycle     54717998
    Average SMSP Active Cycles       cycle   1341697.88
    Total SMSP Elapsed Cycles        cycle    218871992
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 18, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1375128
    Memory Throughput                   %        88.40
    DRAM Throughput                     %         2.09
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        96.98
    L2 Cache Throughput                 %         6.60
    SM Active Cycles                cycle   1254852.30
    Compute (SM) Throughput             %        31.45
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.41
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.92
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        11.68
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.79%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.36
    Mem Busy                               %        88.40
    Max Bandwidth                          %        31.45
    L1/TEX Hit Rate                        %        92.63
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        88.54
    Mem Pipes Busy                         %        31.45
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 78%                                                                                             
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 48.49%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097237 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.19%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.95
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.05
    Active Warps Per Scheduler          warp         3.47
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.6%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.47 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        34.90
    Warp Cycles Per Executed Instruction           cycle        34.91
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.6%                                                                                           
          On average, each warp of this kernel spends 12.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 35.5% of the total average of 34.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.03%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    124492.80
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst    124515.95
    Issued Instructions                             inst     19922552
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              40
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        28.92
    Achieved Active Warps Per SM           warp        13.88
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.6%                                                                                           
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       165532
    Total DRAM Elapsed Cycles        cycle     63325184
    Average L1 Active Cycles         cycle   1254852.30
    Total L1 Elapsed Cycles          cycle     55063592
    Average L2 Active Cycles         cycle      1082751
    Total L2 Elapsed Cycles          cycle     41287840
    Average SM Active Cycles         cycle   1254852.30
    Total SM Elapsed Cycles          cycle     55063592
    Average SMSP Active Cycles       cycle   1250987.38
    Total SMSP Elapsed Cycles        cycle    220254368
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.968%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 8.74% above the average, while the minimum instance value is 6.73% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.14%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 8.96% above the average, while the minimum instance value is 6.60% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.968%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.74% above the average, while the minimum instance value is 6.73% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     819.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 73.98%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 53.17%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 19, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1367745
    Memory Throughput                   %        92.09
    DRAM Throughput                     %        13.55
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        93.90
    L2 Cache Throughput                 %         8.65
    SM Active Cycles                cycle   1341331.57
    Compute (SM) Throughput             %        92.09
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.78
    Executed Ipc Elapsed  inst/cycle         1.75
    Issue Slots Busy               %        44.61
    Issued Ipc Active     inst/cycle         1.78
    SM Busy                        %        44.61
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (25.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        60.65
    Mem Busy                               %        51.40
    Max Bandwidth                          %        92.09
    L1/TEX Hit Rate                        %        16.29
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        41.81
    Mem Pipes Busy                         %        92.09
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.60
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.40
    Active Warps Per Scheduler          warp        11.02
    Eligible Warps Per Scheduler        warp         1.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.905%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.02 active warps per scheduler, but only an average of 1.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        24.70
    Warp Cycles Per Executed Instruction           cycle        24.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.905%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 41.7% of the total average of 24.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    598220.80
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    598344.19
    Issued Instructions                             inst     95735070
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.84
    Achieved Active Warps Per SM           warp        44.08
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1066832
    Total DRAM Elapsed Cycles        cycle     62985216
    Average L1 Active Cycles         cycle   1341331.57
    Total L1 Elapsed Cycles          cycle     54705360
    Average L2 Active Cycles         cycle   1257670.88
    Total L2 Elapsed Cycles          cycle     41066432
    Average SM Active Cycles         cycle   1341331.57
    Total SM Elapsed Cycles          cycle     54705360
    Average SMSP Active Cycles       cycle   1341560.70
    Total SMSP Elapsed Cycles        cycle    218821440
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 20, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1375896
    Memory Throughput                   %        88.25
    DRAM Throughput                     %         2.09
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        97.02
    L2 Cache Throughput                 %         6.64
    SM Active Cycles                cycle   1254329.18
    Compute (SM) Throughput             %        31.39
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.47
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.93
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        11.68
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.79%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.34
    Mem Busy                               %        88.25
    Max Bandwidth                          %        31.39
    L1/TEX Hit Rate                        %        92.56
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        90.60
    Mem Pipes Busy                         %        31.39
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 77.86%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 48.51%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097227 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.21%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.98
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.02
    Active Warps Per Scheduler          warp         3.49
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.75%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.49 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        34.96
    Warp Cycles Per Executed Instruction           cycle        34.96
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.75%                                                                                          
          On average, each warp of this kernel spends 12.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 35.2% of the total average of 35.0 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14%                                                                                             
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    124492.80
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst    124515.64
    Issued Instructions                             inst     19922503
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              40
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        28.92
    Achieved Active Warps Per SM           warp        13.88
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.75%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       165288
    Total DRAM Elapsed Cycles        cycle     63361024
    Average L1 Active Cycles         cycle   1254329.18
    Total L1 Elapsed Cycles          cycle     55160628
    Average L2 Active Cycles         cycle   1073468.28
    Total L2 Elapsed Cycles          cycle     41310624
    Average SM Active Cycles         cycle   1254329.18
    Total SM Elapsed Cycles          cycle     55160628
    Average SMSP Active Cycles       cycle   1247085.69
    Total SMSP Elapsed Cycles        cycle    220642512
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.939%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 8.73% above the average, while the minimum instance value is 6.68% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.285%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 9.16% above the average, while the minimum instance value is 6.33% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.939%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.73% above the average, while the minimum instance value is 6.68% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     819.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 73.3%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 53.06%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1365570
    Memory Throughput                   %        91.90
    DRAM Throughput                     %        13.56
    Duration                      msecond         1.12
    L1/TEX Cache Throughput             %        93.89
    L2 Cache Throughput                 %         8.64
    SM Active Cycles                cycle   1341483.52
    Compute (SM) Throughput             %        91.90
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.78
    Executed Ipc Elapsed  inst/cycle         1.75
    Issue Slots Busy               %        44.60
    Issued Ipc Active     inst/cycle         1.78
    SM Busy                        %        44.60
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (25.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        60.71
    Mem Busy                               %        51.29
    Max Bandwidth                          %        91.90
    L1/TEX Hit Rate                        %        16.21
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        41.95
    Mem Pipes Busy                         %        91.90
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.60
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.40
    Active Warps Per Scheduler          warp        11.02
    Eligible Warps Per Scheduler        warp         1.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.105%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.02 active warps per scheduler, but only an average of 1.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        24.70
    Warp Cycles Per Executed Instruction           cycle        24.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.105%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 41.7% of the total average of 24.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    598220.80
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    598342.91
    Issued Instructions                             inst     95734865
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.82
    Achieved Active Warps Per SM           warp        44.07
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1066124
    Total DRAM Elapsed Cycles        cycle     62885888
    Average L1 Active Cycles         cycle   1341483.52
    Total L1 Elapsed Cycles          cycle     54824192
    Average L2 Active Cycles         cycle   1258937.41
    Total L2 Elapsed Cycles          cycle     41001120
    Average SM Active Cycles         cycle   1341483.52
    Total SM Elapsed Cycles          cycle     54824192
    Average SMSP Active Cycles       cycle   1341686.82
    Total SMSP Elapsed Cycles        cycle    219296768
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 14, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.20
    Elapsed Cycles                  cycle      1377956
    Memory Throughput                   %        88.47
    DRAM Throughput                     %         2.11
    Duration                      msecond         1.14
    L1/TEX Cache Throughput             %        96.90
    L2 Cache Throughput                 %         6.60
    SM Active Cycles                cycle   1255900.80
    Compute (SM) Throughput             %        31.47
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.91
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        11.67
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.8%                                                                                     
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.42
    Mem Busy                               %        88.47
    Max Bandwidth                          %        31.47
    L1/TEX Hit Rate                        %        92.66
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        88.87
    Mem Pipes Busy                         %        31.47
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 78.06%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 48.45%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097235 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.14%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.97
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.03
    Active Warps Per Scheduler          warp         3.47
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.53%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.47 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        34.83
    Warp Cycles Per Executed Instruction           cycle        34.84
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.53%                                                                                          
          On average, each warp of this kernel spends 12.5 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 36.0% of the total average of 34.8 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.04%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    124492.80
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst    124515.47
    Issued Instructions                             inst     19922475
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              40
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        28.86
    Achieved Active Warps Per SM           warp        13.85
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.53%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       167822
    Total DRAM Elapsed Cycles        cycle     63766528
    Average L1 Active Cycles         cycle   1255900.80
    Total L1 Elapsed Cycles          cycle     55021832
    Average L2 Active Cycles         cycle   1073861.69
    Total L2 Elapsed Cycles          cycle     41574176
    Average SM Active Cycles         cycle   1255900.80
    Total SM Elapsed Cycles          cycle     55021832
    Average SMSP Active Cycles       cycle   1248895.46
    Total SMSP Elapsed Cycles        cycle    220087328
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.895%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.65% above the average, while the minimum instance value is 6.70% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.054%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 8.87% above the average, while the minimum instance value is 6.66% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.895%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.65% above the average, while the minimum instance value is 6.70% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     819.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 72.86%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 53.26%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1368699
    Memory Throughput                   %        92.11
    DRAM Throughput                     %        13.52
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        93.87
    L2 Cache Throughput                 %         8.64
    SM Active Cycles                cycle   1341813.80
    Compute (SM) Throughput             %        92.11
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.78
    Executed Ipc Elapsed  inst/cycle         1.75
    Issue Slots Busy               %        44.59
    Issued Ipc Active     inst/cycle         1.78
    SM Busy                        %        44.59
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (25.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        60.53
    Mem Busy                               %        51.41
    Max Bandwidth                          %        92.11
    L1/TEX Hit Rate                        %        16.32
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        41.80
    Mem Pipes Busy                         %        92.11
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.61
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.39
    Active Warps Per Scheduler          warp        11.02
    Eligible Warps Per Scheduler        warp         1.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.889%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.02 active warps per scheduler, but only an average of 1.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        24.70
    Warp Cycles Per Executed Instruction           cycle        24.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.889%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 41.7% of the total average of 24.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    598220.80
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    598343.74
    Issued Instructions                             inst     95734999
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.83
    Achieved Active Warps Per SM           warp        44.08
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1065430
    Total DRAM Elapsed Cycles        cycle     63030272
    Average L1 Active Cycles         cycle   1341813.80
    Total L1 Elapsed Cycles          cycle     54695542
    Average L2 Active Cycles         cycle   1260391.94
    Total L2 Elapsed Cycles          cycle     41095040
    Average SM Active Cycles         cycle   1341813.80
    Total SM Elapsed Cycles          cycle     54695542
    Average SMSP Active Cycles       cycle   1341304.85
    Total SMSP Elapsed Cycles        cycle    218782168
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 16, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1374408
    Memory Throughput                   %        88.73
    DRAM Throughput                     %         2.11
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        96.83
    L2 Cache Throughput                 %         6.58
    SM Active Cycles                cycle   1256767.62
    Compute (SM) Throughput             %        31.56
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.91
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        11.66
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.8%                                                                                     
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.45
    Mem Busy                               %        88.73
    Max Bandwidth                          %        31.56
    L1/TEX Hit Rate                        %        92.54
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        89.91
    Mem Pipes Busy                         %        31.56
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 78.29%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 48.41%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097240 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.1%                                                                                           
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.96
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.04
    Active Warps Per Scheduler          warp         3.47
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.27%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.47 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        34.85
    Warp Cycles Per Executed Instruction           cycle        34.86
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.27%                                                                                          
          On average, each warp of this kernel spends 12.4 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 35.4% of the total average of 34.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.08%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    124492.80
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst    124515.51
    Issued Instructions                             inst     19922482
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              40
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        28.86
    Achieved Active Warps Per SM           warp        13.85
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.27%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       167018
    Total DRAM Elapsed Cycles        cycle     63291392
    Average L1 Active Cycles         cycle   1256767.62
    Total L1 Elapsed Cycles          cycle     54860842
    Average L2 Active Cycles         cycle   1071766.41
    Total L2 Elapsed Cycles          cycle     41266176
    Average SM Active Cycles         cycle   1256767.62
    Total SM Elapsed Cycles          cycle     54860842
    Average SMSP Active Cycles       cycle   1250091.38
    Total SMSP Elapsed Cycles        cycle    219443368
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.004%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.74% above the average, while the minimum instance value is 6.76% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.264%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 9.07% above the average, while the minimum instance value is 6.76% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.004%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.74% above the average, while the minimum instance value is 6.76% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     819.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 73.27%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 53.45%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 17, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1367322
    Memory Throughput                   %        92.15
    DRAM Throughput                     %        13.55
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        93.85
    L2 Cache Throughput                 %         8.65
    SM Active Cycles                cycle      1342063
    Compute (SM) Throughput             %        92.15
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.78
    Executed Ipc Elapsed  inst/cycle         1.75
    Issue Slots Busy               %        44.58
    Issued Ipc Active     inst/cycle         1.78
    SM Busy                        %        44.58
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (25.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        60.67
    Mem Busy                               %        51.43
    Max Bandwidth                          %        92.15
    L1/TEX Hit Rate                        %        16.27
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        41.71
    Mem Pipes Busy                         %        92.15
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.60
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.40
    Active Warps Per Scheduler          warp        11.02
    Eligible Warps Per Scheduler        warp         1.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.853%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.02 active warps per scheduler, but only an average of 1.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        24.70
    Warp Cycles Per Executed Instruction           cycle        24.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.853%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 41.7% of the total average of 24.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    598220.80
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    598341.69
    Issued Instructions                             inst     95734670
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.81
    Achieved Active Warps Per SM           warp        44.07
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1066810
    Total DRAM Elapsed Cycles        cycle     62967808
    Average L1 Active Cycles         cycle      1342063
    Total L1 Elapsed Cycles          cycle     54674392
    Average L2 Active Cycles         cycle   1259486.44
    Total L2 Elapsed Cycles          cycle     41053760
    Average SM Active Cycles         cycle      1342063
    Total SM Elapsed Cycles          cycle     54674392
    Average SMSP Active Cycles       cycle   1341595.95
    Total SMSP Elapsed Cycles        cycle    218697568
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 18, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.19
    Elapsed Cycles                  cycle      1382451
    Memory Throughput                   %        88.45
    DRAM Throughput                     %         2.10
    Duration                      msecond         1.15
    L1/TEX Cache Throughput             %        96.95
    L2 Cache Throughput                 %         6.50
    SM Active Cycles                cycle   1255144.62
    Compute (SM) Throughput             %        31.46
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.92
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        11.67
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.79%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.41
    Mem Busy                               %        88.45
    Max Bandwidth                          %        31.46
    L1/TEX Hit Rate                        %        92.58
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        86.57
    Mem Pipes Busy                         %        31.46
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 78.04%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 48.48%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097236 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.17%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.98
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.02
    Active Warps Per Scheduler          warp         3.48
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.55%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.48 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        34.91
    Warp Cycles Per Executed Instruction           cycle        34.92
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.55%                                                                                          
          On average, each warp of this kernel spends 12.6 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 36.1% of the total average of 34.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.03%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    124492.80
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst    124515.68
    Issued Instructions                             inst     19922508
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              40
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        28.81
    Achieved Active Warps Per SM           warp        13.83
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.55%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       169638
    Total DRAM Elapsed Cycles        cycle     64546816
    Average L1 Active Cycles         cycle   1255144.62
    Total L1 Elapsed Cycles          cycle     55035348
    Average L2 Active Cycles         cycle   1075790.50
    Total L2 Elapsed Cycles          cycle     42077952
    Average SM Active Cycles         cycle   1255144.62
    Total SM Elapsed Cycles          cycle     55035348
    Average SMSP Active Cycles       cycle   1247894.42
    Total SMSP Elapsed Cycles        cycle    220141392
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.046%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 8.82% above the average, while the minimum instance value is 6.78% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.408%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 9.27% above the average, while the minimum instance value is 6.57% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.046%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.82% above the average, while the minimum instance value is 6.78% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     819.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 72.12%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 53.21%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  hgemm_normal(const __half *, const __half *, float *, int, int, unsigned long, unsigned long, unsigned long) (128, 8, 1)x(16, 16, 1), Context 1, Stream 19, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1368374
    Memory Throughput                   %        92.00
    DRAM Throughput                     %        13.53
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        93.91
    L2 Cache Throughput                 %         8.63
    SM Active Cycles                cycle   1341198.52
    Compute (SM) Throughput             %        92.00
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 8% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.78
    Executed Ipc Elapsed  inst/cycle         1.75
    Issue Slots Busy               %        44.61
    Issued Ipc Active     inst/cycle         1.78
    SM Busy                        %        44.61
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (25.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        60.58
    Mem Busy                               %        51.35
    Max Bandwidth                          %        92.00
    L1/TEX Hit Rate                        %        16.24
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        41.91
    Mem Pipes Busy                         %        92.00
    --------------------------- ------------ ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        44.60
    Issued Warp Per Scheduler                        0.45
    No Eligible                            %        55.40
    Active Warps Per Scheduler          warp        11.02
    Eligible Warps Per Scheduler        warp         1.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.003%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.02 active warps per scheduler, but only an average of 1.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        24.70
    Warp Cycles Per Executed Instruction           cycle        24.70
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.003%                                                                                          
          On average, each warp of this kernel spends 10.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 41.7% of the total average of 24.7 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    598220.80
    Executed Instructions                           inst     95715328
    Avg. Issued Instructions Per Scheduler          inst    598344.62
    Issued Instructions                             inst     95735139
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              35
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              40
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                4.27
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.88
    Achieved Active Warps Per SM           warp        44.10
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1066042
    Total DRAM Elapsed Cycles        cycle     63013888
    Average L1 Active Cycles         cycle   1341198.52
    Total L1 Elapsed Cycles          cycle     54763304
    Average L2 Active Cycles         cycle   1258572.44
    Total L2 Elapsed Cycles          cycle     41085248
    Average SM Active Cycles         cycle   1341198.52
    Total SM Elapsed Cycles          cycle     54763304
    Average SMSP Active Cycles       cycle   1341619.90
    Total SMSP Elapsed Cycles        cycle    219053216
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3162112
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (8, 128, 1)x(32, 1, 1), Context 1, Stream 20, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.99
    SM Frequency            cycle/nsecond         1.21
    Elapsed Cycles                  cycle      1370965
    Memory Throughput                   %        88.25
    DRAM Throughput                     %         2.14
    Duration                      msecond         1.13
    L1/TEX Cache Throughput             %        96.99
    L2 Cache Throughput                 %         6.58
    SM Active Cycles                cycle   1254674.73
    Compute (SM) Throughput             %        31.39
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.28
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.92
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        11.68
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.79%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.59
    Mem Busy                               %        88.25
    Max Bandwidth                          %        31.39
    L1/TEX Hit Rate                        %        92.53
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        90.52
    Mem Pipes Busy                         %        31.39
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 77.87%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 3.8 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 48.49%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 8.0 - way bank        
          conflict across all 262144 shared load requests.This results in 1048576 bank conflicts,  which represent      
          50.00% of the overall 2097228 wavefronts for shared loads. Check the Source Counters section for uncoalesced  
          shared loads.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 58.19%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank       
          conflict across all 4194304 shared store requests.This results in 6291456 bank conflicts,  which represent    
          60.00% of the overall 10485760 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.96
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.04
    Active Warps Per Scheduler          warp         3.48
    Eligible Warps Per Scheduler        warp         0.16
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.75%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          3.48 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        34.92
    Warp Cycles Per Executed Instruction           cycle        34.93
    Avg. Active Threads Per Warp                                17.84
    Avg. Not Predicated Off Threads Per Warp                    17.73
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.75%                                                                                          
          On average, each warp of this kernel spends 12.3 cycles being stalled waiting for the MIO (memory             
          input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of  
          the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory        
          instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline  
          pressure. This stall type represents about 35.2% of the total average of 34.9 cycles between issuing two      
          instructions.                                                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14%                                                                                             
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 17.8 threads being active per cycle. This is further reduced    
          to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    124492.80
    Executed Instructions                           inst     19918848
    Avg. Issued Instructions Per Scheduler          inst    124515.88
    Issued Instructions                             inst     19922541
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              40
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.60
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        28.87
    Achieved Active Warps Per SM           warp        13.86
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.75%                                                                                          
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       169082
    Total DRAM Elapsed Cycles        cycle     63134720
    Average L1 Active Cycles         cycle   1254674.73
    Total L1 Elapsed Cycles          cycle     55156780
    Average L2 Active Cycles         cycle   1074963.31
    Total L2 Elapsed Cycles          cycle     41162688
    Average SM Active Cycles         cycle   1254674.73
    Total SM Elapsed Cycles          cycle     55156780
    Average SMSP Active Cycles       cycle   1249755.21
    Total SMSP Elapsed Cycles        cycle    220627120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.911%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 8.69% above the average, while the minimum instance value is 6.69% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.024%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 8.85% above the average, while the minimum instance value is 6.50% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.911%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.69% above the average, while the minimum instance value is 6.69% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       528384
    Branch Efficiency                   %        50.19
    Avg. Divergent Branches                     819.20
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 73.67%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 31457280 excessive sectors (88% of the    
          total 35684352 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 53.08%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 7340032 excessive wavefronts (58% of the  
          total 12582912 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.  
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

