==PROF== Connected to process 8632 (/home/deniz/Documents/CENG443_project/src/hgemm_stream)
==PROF== Profiling "hgemm_normal" - 0: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_tensor_core" - 1: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 2: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_tensor_core" - 3: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 4: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_tensor_core" - 5: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 6: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 7: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 8: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 9: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 10: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_tensor_core" - 11: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 12: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_tensor_core" - 13: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 14: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 15: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 16: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 17: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 18: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_tensor_core" - 19: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_normal" - 20: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 21: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 22: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 23: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 24: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_tensor_core" - 25: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 26: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 27: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 28: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_tensor_core" - 29: 0%....50%....100% - 39 passes
==PROF== Profiling "hgemm_normal" - 30: 0%....50%....100% - 38 passes
==PROF== Profiling "hgemm_tensor_core" - 31: 0%....50%....100% - 39 passes

=== Concurrent HGEMM Performance (Mixed Tensor + Normal) ===

=== Performance Results ===
Matrix size (Normal): 2048x2048
Matrix size (Tensor): 8192x8192
Number of streams: 4
Mixed version (concurrent tensor + normal) time: 37075.824 ms

=== Result Verification ===
C[0][0] = 8192
C[4096][0] = 8192
Expected value = 8192
==PROF== Disconnected from process 8632
[8632] hgemm_stream@127.0.0.1
  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle      1241521
    Memory Throughput                   %        93.35
    DRAM Throughput                     %         2.27
    Duration                      usecond       962.62
    L1/TEX Cache Throughput             %        96.42
    L2 Cache Throughput                 %        16.41
    SM Active Cycles                cycle   1200836.91
    Compute (SM) Throughput             %        93.35
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.87
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.36
    Issue Slots Busy               %        35.09
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.09
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.26
    Mem Busy                               %        70.02
    Max Bandwidth                          %        93.35
    L1/TEX Hit Rate                        %        91.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        93.06
    Mem Pipes Busy                         %        93.35
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 58.34%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.09
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.91
    Active Warps Per Scheduler          warp        10.43
    Eligible Warps Per Scheduler        warp         1.85
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.65%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.43 active warps per scheduler, but only an average of 1.85 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.73
    Warp Cycles Per Executed Instruction           cycle        29.74
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.65%                                                                                           
          On average, each warp of this kernel spends 17.5 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 58.8% of the total average of 29.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    421323.03
    Executed Instructions                           inst     97746944
    Avg. Issued Instructions Per Scheduler          inst    421411.87
    Issued Instructions                             inst     97767554
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.95
    Achieved Active Warps Per SM           warp        41.74
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.65%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     54237184
    Average L1 Active Cycles         cycle   1200836.91
    Total L1 Elapsed Cycles          cycle     71942334
    Average L2 Active Cycles         cycle   1002353.17
    Total L2 Elapsed Cycles          cycle     24598416
    Average SM Active Cycles         cycle   1200836.91
    Total SM Elapsed Cycles          cycle     71942334
    Average SMSP Active Cycles       cycle   1200921.20
    Total SMSP Elapsed Cycles        cycle    287769336
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle     11720482
    Memory Throughput                   %        96.89
    DRAM Throughput                     %         4.21
    Duration                      msecond         9.09
    L1/TEX Cache Throughput             %        85.47
    L2 Cache Throughput                 %        96.89
    SM Active Cycles                cycle  11545467.67
    Compute (SM) Throughput             %        29.84
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.36
    Executed Ipc Elapsed  inst/cycle         0.35
    Issue Slots Busy               %         8.99
    Issued Ipc Active     inst/cycle         0.36
    SM Busy                        %        10.09
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.98%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        17.23
    Mem Busy                               %        96.89
    Max Bandwidth                          %        79.41
    L1/TEX Hit Rate                        %        57.16
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        96.07
    Mem Pipes Busy                         %        29.84
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 42.15%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.90
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.10
    Active Warps Per Scheduler          warp         5.88
    Eligible Warps Per Scheduler        warp         0.15
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 3.11%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 11.2 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          5.88 active warps per scheduler, but only an average of 0.15 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        66.10
    Warp Cycles Per Executed Instruction           cycle        66.11
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 3.11%                                                                                           
          On average, each warp of this kernel spends 35.8 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 54.2% of the total average of 66.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1037559.17
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst   1037647.35
    Issued Instructions                             inst    240734186
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.66
    Achieved Active Warps Per SM           warp        23.84
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 3.11%                                                                                           
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   3260605.33
    Total DRAM Elapsed Cycles        cycle    511979520
    Average L1 Active Cycles         cycle  11545467.67
    Total L1 Elapsed Cycles          cycle    678834286
    Average L2 Active Cycles         cycle   9727556.46
    Total L2 Elapsed Cycles          cycle    232225968
    Average SM Active Cycles         cycle  11545467.67
    Total SM Elapsed Cycles          cycle    678834286
    Average SMSP Active Cycles       cycle  11656060.78
    Total SMSP Elapsed Cycles        cycle   2715337144
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 50.22%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle      1232988
    Memory Throughput                   %        93.98
    DRAM Throughput                     %         2.29
    Duration                      usecond          956
    L1/TEX Cache Throughput             %        96.45
    L2 Cache Throughput                 %        16.54
    SM Active Cycles                cycle   1200525.19
    Compute (SM) Throughput             %        93.98
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.37
    Issue Slots Busy               %        35.10
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.10
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.35
    Mem Busy                               %        70.50
    Max Bandwidth                          %        93.98
    L1/TEX Hit Rate                        %        91.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        93.05
    Mem Pipes Busy                         %        93.98
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 58.74%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.09
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.91
    Active Warps Per Scheduler          warp        10.44
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.018%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.44 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.74
    Warp Cycles Per Executed Instruction           cycle        29.74
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.018%                                                                                          
          On average, each warp of this kernel spends 17.5 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 58.8% of the total average of 29.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    421323.03
    Executed Instructions                           inst     97746944
    Avg. Issued Instructions Per Scheduler          inst    421411.92
    Issued Instructions                             inst     97767565
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.96
    Achieved Active Warps Per SM           warp        41.74
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.018%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    186162.67
    Total DRAM Elapsed Cycles        cycle     53864448
    Average L1 Active Cycles         cycle   1200525.19
    Total L1 Elapsed Cycles          cycle     71458538
    Average L2 Active Cycles         cycle   1001399.79
    Total L2 Elapsed Cycles          cycle     24428952
    Average SM Active Cycles         cycle   1200525.19
    Total SM Elapsed Cycles          cycle     71458538
    Average SMSP Active Cycles       cycle   1200945.15
    Total SMSP Elapsed Cycles        cycle    285834152
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle     12309524
    Memory Throughput                   %        97.63
    DRAM Throughput                     %         3.99
    Duration                      msecond         9.54
    L1/TEX Cache Throughput             %        81.24
    L2 Cache Throughput                 %        97.63
    SM Active Cycles                cycle  12146729.16
    Compute (SM) Throughput             %        28.40
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.34
    Executed Ipc Elapsed  inst/cycle         0.34
    Issue Slots Busy               %         8.54
    Issued Ipc Active     inst/cycle         0.34
    SM Busy                        %         9.59
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.47%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        16.32
    Mem Busy                               %        97.63
    Max Bandwidth                          %        79.53
    L1/TEX Hit Rate                        %        56.95
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        95.10
    Mem Pipes Busy                         %        28.40
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 40.13%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.05
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        90.95
    Active Warps Per Scheduler          warp         6.08
    Eligible Warps Per Scheduler        warp         0.15
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 2.371%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 11.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          6.08 active warps per scheduler, but only an average of 0.15 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        67.20
    Warp Cycles Per Executed Instruction           cycle        67.21
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.371%                                                                                          
          On average, each warp of this kernel spends 35.7 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 53.1% of the total average of 67.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1037559.17
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst   1037646.53
    Issued Instructions                             inst    240733994
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.08
    Achieved Active Warps Per SM           warp        23.56
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.371%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      3244008
    Total DRAM Elapsed Cycles        cycle    537708544
    Average L1 Active Cycles         cycle  12146729.16
    Total L1 Elapsed Cycles          cycle    713042078
    Average L2 Active Cycles         cycle   9788849.17
    Total L2 Elapsed Cycles          cycle    243896952
    Average SM Active Cycles         cycle  12146729.16
    Total SM Elapsed Cycles          cycle    713042078
    Average SMSP Active Cycles       cycle  11463060.65
    Total SMSP Elapsed Cycles        cycle   2852168312
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 48.12%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle      1236066
    Memory Throughput                   %        93.78
    DRAM Throughput                     %         2.29
    Duration                      usecond       958.40
    L1/TEX Cache Throughput             %        96.46
    L2 Cache Throughput                 %        16.49
    SM Active Cycles                cycle   1200406.02
    Compute (SM) Throughput             %        93.78
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.36
    Issue Slots Busy               %        35.11
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.11
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.37
    Mem Busy                               %        70.35
    Max Bandwidth                          %        93.78
    L1/TEX Hit Rate                        %        91.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        93.02
    Mem Pipes Busy                         %        93.78
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 58.61%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.11
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.89
    Active Warps Per Scheduler          warp        10.44
    Eligible Warps Per Scheduler        warp         1.85
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.219%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.44 active warps per scheduler, but only an average of 1.85 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.73
    Warp Cycles Per Executed Instruction           cycle        29.74
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.219%                                                                                          
          On average, each warp of this kernel spends 17.5 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 58.8% of the total average of 29.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    421323.03
    Executed Instructions                           inst     97746944
    Avg. Issued Instructions Per Scheduler          inst    421413.59
    Issued Instructions                             inst     97767953
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.99
    Achieved Active Warps Per SM           warp        41.76
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.219%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    187093.33
    Total DRAM Elapsed Cycles        cycle     53999616
    Average L1 Active Cycles         cycle   1200406.02
    Total L1 Elapsed Cycles          cycle     71611452
    Average L2 Active Cycles         cycle   1003152.67
    Total L2 Elapsed Cycles          cycle     24490728
    Average SM Active Cycles         cycle   1200406.02
    Total SM Elapsed Cycles          cycle     71611452
    Average SMSP Active Cycles       cycle   1200270.73
    Total SMSP Elapsed Cycles        cycle    286445808
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle     10960425
    Memory Throughput                   %       101.82
    DRAM Throughput                     %         4.47
    Duration                      msecond         8.50
    L1/TEX Cache Throughput             %        92.19
    L2 Cache Throughput                 %       101.82
    SM Active Cycles                cycle  10703557.84
    Compute (SM) Throughput             %        31.90
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.54
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.39
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.69
    Issued Ipc Active     inst/cycle         0.39
    SM Busy                        %        10.88
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.19%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        18.30
    Mem Busy                               %       101.82
    Max Bandwidth                          %        82.71
    L1/TEX Hit Rate                        %        59.98
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %       106.73
    Mem Pipes Busy                         %        31.90
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.08%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.72
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.28
    Active Warps Per Scheduler          warp         5.66
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: -1.816%                                                                                   
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 11.5 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          5.66 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        64.86
    Warp Cycles Per Executed Instruction           cycle        64.87
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: -1.816%                                                                                         
          On average, each warp of this kernel spends 39.6 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.0% of the total average of 64.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1037559.17
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst   1037647.40
    Issued Instructions                             inst    240734196
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        51.31
    Achieved Active Warps Per SM           warp        24.63
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: -1.816%                                                                                         
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   3238914.67
    Total DRAM Elapsed Cycles        cycle    478776320
    Average L1 Active Cycles         cycle  10703557.84
    Total L1 Elapsed Cycles          cycle    634826604
    Average L2 Active Cycles         cycle   9528288.12
    Total L2 Elapsed Cycles          cycle    217167000
    Average SM Active Cycles         cycle  10703557.84
    Total SM Elapsed Cycles          cycle    634826604
    Average SMSP Active Cycles       cycle  11899960.91
    Total SMSP Elapsed Cycles        cycle   2539306416
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 52.6%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle      1232166
    Memory Throughput                   %        94.04
    DRAM Throughput                     %         2.28
    Duration                      usecond       955.36
    L1/TEX Cache Throughput             %        96.45
    L2 Cache Throughput                 %        16.55
    SM Active Cycles                cycle   1200509.64
    Compute (SM) Throughput             %        94.04
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.06
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.37
    Issue Slots Busy               %        35.10
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.10
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.33
    Mem Busy                               %        70.54
    Max Bandwidth                          %        94.04
    L1/TEX Hit Rate                        %        91.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        93.08
    Mem Pipes Busy                         %        94.04
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 58.77%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.10
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.90
    Active Warps Per Scheduler          warp        10.44
    Eligible Warps Per Scheduler        warp         1.85
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.962%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.44 active warps per scheduler, but only an average of 1.85 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.73
    Warp Cycles Per Executed Instruction           cycle        29.74
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.962%                                                                                          
          On average, each warp of this kernel spends 17.5 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 58.8% of the total average of 29.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    421323.03
    Executed Instructions                           inst     97746944
    Avg. Issued Instructions Per Scheduler          inst    421412.98
    Issued Instructions                             inst     97767811
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.99
    Achieved Active Warps Per SM           warp        41.76
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.962%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     53827584
    Average L1 Active Cycles         cycle   1200509.64
    Total L1 Elapsed Cycles          cycle     71415586
    Average L2 Active Cycles         cycle   1001960.88
    Total L2 Elapsed Cycles          cycle     24412824
    Average SM Active Cycles         cycle   1200509.64
    Total SM Elapsed Cycles          cycle     71415586
    Average SMSP Active Cycles       cycle   1200489.82
    Total SMSP Elapsed Cycles        cycle    285662344
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle     12069097
    Memory Throughput                   %        97.69
    DRAM Throughput                     %         4.06
    Duration                      msecond         9.36
    L1/TEX Cache Throughput             %        82.74
    L2 Cache Throughput                 %        97.69
    SM Active Cycles                cycle  11925580.91
    Compute (SM) Throughput             %        28.99
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.35
    Executed Ipc Elapsed  inst/cycle         0.34
    Issue Slots Busy               %         8.70
    Issued Ipc Active     inst/cycle         0.35
    SM Busy                        %         9.76
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.3%                                                                                     
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        16.61
    Mem Busy                               %        97.69
    Max Bandwidth                          %        82.13
    L1/TEX Hit Rate                        %        56.80
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %       100.26
    Mem Pipes Busy                         %        28.99
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 40.95%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.55
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.45
    Active Warps Per Scheduler          warp         5.95
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 2.307%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 11.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          5.95 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        69.59
    Warp Cycles Per Executed Instruction           cycle        69.60
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.307%                                                                                          
          On average, each warp of this kernel spends 35.4 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 50.9% of the total average of 69.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1037559.17
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst   1037646.51
    Issued Instructions                             inst    240733990
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        48.20
    Achieved Active Warps Per SM           warp        23.14
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.307%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   3236773.33
    Total DRAM Elapsed Cycles        cycle    527206400
    Average L1 Active Cycles         cycle  11925580.91
    Total L1 Elapsed Cycles          cycle    698763594
    Average L2 Active Cycles         cycle  10200370.04
    Total L2 Elapsed Cycles          cycle    239131944
    Average SM Active Cycles         cycle  11925580.91
    Total SM Elapsed Cycles          cycle    698763594
    Average SMSP Active Cycles       cycle  12131459.36
    Total SMSP Elapsed Cycles        cycle   2795054376
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 51.14%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle      1232148
    Memory Throughput                   %        94.07
    DRAM Throughput                     %         2.30
    Duration                      usecond       955.33
    L1/TEX Cache Throughput             %        96.47
    L2 Cache Throughput                 %        16.55
    SM Active Cycles                cycle   1200277.67
    Compute (SM) Throughput             %        94.07
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.06
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.37
    Issue Slots Busy               %        35.11
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.11
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.41
    Mem Busy                               %        70.56
    Max Bandwidth                          %        94.07
    L1/TEX Hit Rate                        %        91.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        93.00
    Mem Pipes Busy                         %        94.07
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 58.79%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.09
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.91
    Active Warps Per Scheduler          warp        10.43
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.932%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.43 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.74
    Warp Cycles Per Executed Instruction           cycle        29.75
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.932%                                                                                          
          On average, each warp of this kernel spends 17.5 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 58.8% of the total average of 29.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    421323.03
    Executed Instructions                           inst     97746944
    Avg. Issued Instructions Per Scheduler          inst    421411.81
    Issued Instructions                             inst     97767540
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.01
    Achieved Active Warps Per SM           warp        41.76
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.932%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    187357.33
    Total DRAM Elapsed Cycles        cycle     53828608
    Average L1 Active Cycles         cycle   1200277.67
    Total L1 Elapsed Cycles          cycle     71393360
    Average L2 Active Cycles         cycle   1002395.83
    Total L2 Elapsed Cycles          cycle     24412608
    Average SM Active Cycles         cycle   1200277.67
    Total SM Elapsed Cycles          cycle     71393360
    Average SMSP Active Cycles       cycle   1201069.30
    Total SMSP Elapsed Cycles        cycle    285573440
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle     11326821
    Memory Throughput                   %        97.15
    DRAM Throughput                     %         4.32
    Duration                      msecond         8.78
    L1/TEX Cache Throughput             %        88.72
    L2 Cache Throughput                 %        97.15
    SM Active Cycles                cycle  11122753.47
    Compute (SM) Throughput             %        30.88
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.00
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.37
    Executed Ipc Elapsed  inst/cycle         0.37
    Issue Slots Busy               %         9.33
    Issued Ipc Active     inst/cycle         0.37
    SM Busy                        %        10.47
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.6%                                                                                     
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        17.68
    Mem Busy                               %        97.15
    Max Bandwidth                          %        82.17
    L1/TEX Hit Rate                        %        58.18
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        99.87
    Mem Pipes Busy                         %        30.88
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 43.62%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.18
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        90.82
    Active Warps Per Scheduler          warp         6.19
    Eligible Warps Per Scheduler        warp         0.15
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 2.848%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.9 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          6.19 active warps per scheduler, but only an average of 0.15 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        67.39
    Warp Cycles Per Executed Instruction           cycle        67.40
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.848%                                                                                          
          On average, each warp of this kernel spends 36.6 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 54.3% of the total average of 67.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1037559.17
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst   1037647.50
    Issued Instructions                             inst    240734221
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.40
    Achieved Active Warps Per SM           warp        23.71
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.848%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   3234685.33
    Total DRAM Elapsed Cycles        cycle    494783488
    Average L1 Active Cycles         cycle  11122753.47
    Total L1 Elapsed Cycles          cycle    655982092
    Average L2 Active Cycles         cycle   9823976.08
    Total L2 Elapsed Cycles          cycle    224424696
    Average SM Active Cycles         cycle  11122753.47
    Total SM Elapsed Cycles          cycle    655982092
    Average SMSP Active Cycles       cycle  11300417.02
    Total SMSP Elapsed Cycles        cycle   2623928368
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 52.48%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle      1236313
    Memory Throughput                   %        93.74
    DRAM Throughput                     %         2.28
    Duration                      usecond       958.56
    L1/TEX Cache Throughput             %        96.41
    L2 Cache Throughput                 %        16.48
    SM Active Cycles                cycle   1200955.10
    Compute (SM) Throughput             %        93.74
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.36
    Issue Slots Busy               %        35.09
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.09
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.31
    Mem Busy                               %        70.31
    Max Bandwidth                          %        93.74
    L1/TEX Hit Rate                        %        91.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        93.21
    Mem Pipes Busy                         %        93.74
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 58.59%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.10
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.90
    Active Warps Per Scheduler          warp        10.44
    Eligible Warps Per Scheduler        warp         1.85
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.263%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.44 active warps per scheduler, but only an average of 1.85 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.73
    Warp Cycles Per Executed Instruction           cycle        29.74
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.263%                                                                                          
          On average, each warp of this kernel spends 17.5 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 58.8% of the total average of 29.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    421323.03
    Executed Instructions                           inst     97746944
    Avg. Issued Instructions Per Scheduler          inst    421412.50
    Issued Instructions                             inst     97767700
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.97
    Achieved Active Warps Per SM           warp        41.74
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.263%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       185864
    Total DRAM Elapsed Cycles        cycle     54008832
    Average L1 Active Cycles         cycle   1200955.10
    Total L1 Elapsed Cycles          cycle     71645156
    Average L2 Active Cycles         cycle   1001508.58
    Total L2 Elapsed Cycles          cycle     24495048
    Average SM Active Cycles         cycle   1200955.10
    Total SM Elapsed Cycles          cycle     71645156
    Average SMSP Active Cycles       cycle   1200756.42
    Total SMSP Elapsed Cycles        cycle    286580624
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle     11522030
    Memory Throughput                   %        97.49
    DRAM Throughput                     %         4.25
    Duration                      msecond         8.93
    L1/TEX Cache Throughput             %        87.01
    L2 Cache Throughput                 %        97.49
    SM Active Cycles                cycle  11340460.69
    Compute (SM) Throughput             %        30.35
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.37
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.15
    Issued Ipc Active     inst/cycle         0.37
    SM Busy                        %        10.27
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.8%                                                                                     
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        17.39
    Mem Busy                               %        97.49
    Max Bandwidth                          %        79.48
    L1/TEX Hit Rate                        %        59.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        95.93
    Mem Pipes Busy                         %        30.35
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 42.88%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.20
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        90.80
    Active Warps Per Scheduler          warp         5.87
    Eligible Warps Per Scheduler        warp         0.13
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 2.51%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.9 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          5.87 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        63.77
    Warp Cycles Per Executed Instruction           cycle        63.77
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.51%                                                                                           
          On average, each warp of this kernel spends 37.8 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.2% of the total average of 63.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1037559.17
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst   1037646.52
    Issued Instructions                             inst    240733992
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        47.76
    Achieved Active Warps Per SM           warp        22.93
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.51%                                                                                           
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   3235309.33
    Total DRAM Elapsed Cycles        cycle    503309312
    Average L1 Active Cycles         cycle  11340460.69
    Total L1 Elapsed Cycles          cycle    667382920
    Average L2 Active Cycles         cycle   9347383.08
    Total L2 Elapsed Cycles          cycle    228291480
    Average SM Active Cycles         cycle  11340460.69
    Total SM Elapsed Cycles          cycle    667382920
    Average SMSP Active Cycles       cycle  11281568.81
    Total SMSP Elapsed Cycles        cycle   2669531680
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.09%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle      1234869
    Memory Throughput                   %        93.88
    DRAM Throughput                     %         2.28
    Duration                      usecond       957.44
    L1/TEX Cache Throughput             %        96.45
    L2 Cache Throughput                 %        16.51
    SM Active Cycles                cycle   1200475.26
    Compute (SM) Throughput             %        93.88
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.37
    Issue Slots Busy               %        35.10
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.10
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.31
    Mem Busy                               %        70.42
    Max Bandwidth                          %        93.88
    L1/TEX Hit Rate                        %        91.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        93.17
    Mem Pipes Busy                         %        93.88
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 58.67%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.12
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.88
    Active Warps Per Scheduler          warp        10.44
    Eligible Warps Per Scheduler        warp         1.85
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.125%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.44 active warps per scheduler, but only an average of 1.85 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.73
    Warp Cycles Per Executed Instruction           cycle        29.74
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.125%                                                                                          
          On average, each warp of this kernel spends 17.5 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 58.8% of the total average of 29.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    421323.03
    Executed Instructions                           inst     97746944
    Avg. Issued Instructions Per Scheduler          inst    421412.83
    Issued Instructions                             inst     97767776
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.99
    Achieved Active Warps Per SM           warp        41.76
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.125%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     53946368
    Average L1 Active Cycles         cycle   1200475.26
    Total L1 Elapsed Cycles          cycle     71539682
    Average L2 Active Cycles         cycle   1002984.92
    Total L2 Elapsed Cycles          cycle     24465936
    Average SM Active Cycles         cycle   1200475.26
    Total SM Elapsed Cycles          cycle     71539682
    Average SMSP Active Cycles       cycle   1200090.60
    Total SMSP Elapsed Cycles        cycle    286158728
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle     11032479
    Memory Throughput                   %       105.15
    DRAM Throughput                     %         4.44
    Duration                      msecond         8.55
    L1/TEX Cache Throughput             %        90.70
    L2 Cache Throughput                 %       105.15
    SM Active Cycles                cycle  10879090.62
    Compute (SM) Throughput             %        31.70
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.00
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.54
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        10.70
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.36%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        18.18
    Mem Busy                               %       105.15
    Max Bandwidth                          %        84.78
    L1/TEX Hit Rate                        %        55.39
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %       110.94
    Mem Pipes Busy                         %        31.70
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 44.79%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.52
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.48
    Active Warps Per Scheduler          warp         5.61
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: -5.15%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 11.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          5.61 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        65.85
    Warp Cycles Per Executed Instruction           cycle        65.86
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: -5.15%                                                                                          
          On average, each warp of this kernel spends 33.6 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 51.1% of the total average of 65.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1037559.17
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst   1037647.35
    Issued Instructions                             inst    240734186
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        54.25
    Achieved Active Warps Per SM           warp        26.04
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: -5.15%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   3238373.33
    Total DRAM Elapsed Cycles        cycle    481925120
    Average L1 Active Cycles         cycle  10879090.62
    Total L1 Elapsed Cycles          cycle    638894364
    Average L2 Active Cycles         cycle   9715327.75
    Total L2 Elapsed Cycles          cycle    218590944
    Average SM Active Cycles         cycle  10879090.62
    Total SM Elapsed Cycles          cycle    638894364
    Average SMSP Active Cycles       cycle  12177033.49
    Total SMSP Elapsed Cycles        cycle   2555577456
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 53.28%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle      1232122
    Memory Throughput                   %        94.06
    DRAM Throughput                     %         2.30
    Duration                      usecond       955.33
    L1/TEX Cache Throughput             %        96.47
    L2 Cache Throughput                 %        16.55
    SM Active Cycles                cycle   1200289.86
    Compute (SM) Throughput             %        94.06
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.06
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.37
    Issue Slots Busy               %        35.11
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.11
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.39
    Mem Busy                               %        70.55
    Max Bandwidth                          %        94.06
    L1/TEX Hit Rate                        %        91.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        93.09
    Mem Pipes Busy                         %        94.06
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 58.79%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.09
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.91
    Active Warps Per Scheduler          warp        10.43
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.94%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.43 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.74
    Warp Cycles Per Executed Instruction           cycle        29.74
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.94%                                                                                           
          On average, each warp of this kernel spends 17.5 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 58.8% of the total average of 29.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    421323.03
    Executed Instructions                           inst     97746944
    Avg. Issued Instructions Per Scheduler          inst    421412.40
    Issued Instructions                             inst     97767677
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.00
    Achieved Active Warps Per SM           warp        41.76
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.94%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    186802.67
    Total DRAM Elapsed Cycles        cycle     53827584
    Average L1 Active Cycles         cycle   1200289.86
    Total L1 Elapsed Cycles          cycle     71399494
    Average L2 Active Cycles         cycle   1002922.50
    Total L2 Elapsed Cycles          cycle     24412104
    Average SM Active Cycles         cycle   1200289.86
    Total SM Elapsed Cycles          cycle     71399494
    Average SMSP Active Cycles       cycle   1200951.98
    Total SMSP Elapsed Cycles        cycle    285597976
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle     11962075
    Memory Throughput                   %        99.78
    DRAM Throughput                     %         4.10
    Duration                      msecond         9.27
    L1/TEX Cache Throughput             %        83.24
    L2 Cache Throughput                 %        99.78
    SM Active Cycles                cycle  11854475.43
    Compute (SM) Throughput             %        29.24
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.00
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.35
    Executed Ipc Elapsed  inst/cycle         0.35
    Issue Slots Busy               %         8.75
    Issued Ipc Active     inst/cycle         0.35
    SM Busy                        %         9.82
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.24%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        16.77
    Mem Busy                               %        99.78
    Max Bandwidth                          %        78.39
    L1/TEX Hit Rate                        %        55.48
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        95.44
    Mem Pipes Busy                         %        29.24
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.31%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.70
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.30
    Active Warps Per Scheduler          warp         5.77
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 0.221%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 11.5 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          5.77 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        66.34
    Warp Cycles Per Executed Instruction           cycle        66.35
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 0.221%                                                                                          
          On average, each warp of this kernel spends 33.9 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 51.0% of the total average of 66.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1037559.17
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst   1037646.56
    Issued Instructions                             inst    240734001
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        47.78
    Achieved Active Warps Per SM           warp        22.93
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 0.221%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      3238832
    Total DRAM Elapsed Cycles        cycle    522530816
    Average L1 Active Cycles         cycle  11854475.43
    Total L1 Elapsed Cycles          cycle    692623516
    Average L2 Active Cycles         cycle      9733509
    Total L2 Elapsed Cycles          cycle    237012696
    Average SM Active Cycles         cycle  11854475.43
    Total SM Elapsed Cycles          cycle    692623516
    Average SMSP Active Cycles       cycle  11933565.09
    Total SMSP Elapsed Cycles        cycle   2770494064
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.23%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle      1234196
    Memory Throughput                   %        93.90
    DRAM Throughput                     %         2.28
    Duration                      usecond       956.93
    L1/TEX Cache Throughput             %        96.45
    L2 Cache Throughput                 %        16.52
    SM Active Cycles                cycle   1200468.97
    Compute (SM) Throughput             %        93.90
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.06
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.37
    Issue Slots Busy               %        35.10
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.10
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.32
    Mem Busy                               %        70.44
    Max Bandwidth                          %        93.90
    L1/TEX Hit Rate                        %        91.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        93.06
    Mem Pipes Busy                         %        93.90
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 58.69%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.11
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.89
    Active Warps Per Scheduler          warp        10.44
    Eligible Warps Per Scheduler        warp         1.85
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.095%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.44 active warps per scheduler, but only an average of 1.85 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.74
    Warp Cycles Per Executed Instruction           cycle        29.74
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.095%                                                                                          
          On average, each warp of this kernel spends 17.5 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 58.8% of the total average of 29.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    421323.03
    Executed Instructions                           inst     97746944
    Avg. Issued Instructions Per Scheduler          inst    421411.80
    Issued Instructions                             inst     97767537
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.00
    Achieved Active Warps Per SM           warp        41.76
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.095%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     53917696
    Average L1 Active Cycles         cycle   1200468.97
    Total L1 Elapsed Cycles          cycle     71517332
    Average L2 Active Cycles         cycle   1002660.92
    Total L2 Elapsed Cycles          cycle     24452712
    Average SM Active Cycles         cycle   1200468.97
    Total SM Elapsed Cycles          cycle     71517332
    Average SMSP Active Cycles       cycle   1200362.11
    Total SMSP Elapsed Cycles        cycle    286069328
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle     12080661
    Memory Throughput                   %        98.65
    DRAM Throughput                     %         4.05
    Duration                      msecond         9.37
    L1/TEX Cache Throughput             %        82.52
    L2 Cache Throughput                 %        98.65
    SM Active Cycles                cycle  11958331.29
    Compute (SM) Throughput             %        28.95
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.35
    Executed Ipc Elapsed  inst/cycle         0.34
    Issue Slots Busy               %         8.68
    Issued Ipc Active     inst/cycle         0.35
    SM Busy                        %         9.74
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.32%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        16.57
    Mem Busy                               %        98.65
    Max Bandwidth                          %        79.95
    L1/TEX Hit Rate                        %        55.53
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %       100.52
    Mem Pipes Busy                         %        28.95
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 40.91%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.54
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.46
    Active Warps Per Scheduler          warp         5.94
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 1.354%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 11.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          5.94 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        69.49
    Warp Cycles Per Executed Instruction           cycle        69.50
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 1.354%                                                                                          
          On average, each warp of this kernel spends 33.5 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.2% of the total average of 69.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1037559.17
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst   1037647.38
    Issued Instructions                             inst    240734192
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        50.31
    Achieved Active Warps Per SM           warp        24.15
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 1.354%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      3232936
    Total DRAM Elapsed Cycles        cycle    527712256
    Average L1 Active Cycles         cycle  11958331.29
    Total L1 Elapsed Cycles          cycle    699577882
    Average L2 Active Cycles         cycle  10180506.67
    Total L2 Elapsed Cycles          cycle    239362608
    Average SM Active Cycles         cycle  11958331.29
    Total SM Elapsed Cycles          cycle    699577882
    Average SMSP Active Cycles       cycle  12149040.83
    Total SMSP Elapsed Cycles        cycle   2798311528
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 50.99%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle      1235541
    Memory Throughput                   %        93.81
    DRAM Throughput                     %         2.28
    Duration                      usecond       957.95
    L1/TEX Cache Throughput             %        96.48
    L2 Cache Throughput                 %        16.50
    SM Active Cycles                cycle   1200115.33
    Compute (SM) Throughput             %        93.81
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.06
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.37
    Issue Slots Busy               %        35.11
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.11
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.31
    Mem Busy                               %        70.37
    Max Bandwidth                          %        93.81
    L1/TEX Hit Rate                        %        91.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        93.18
    Mem Pipes Busy                         %        93.81
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 58.63%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.09
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.91
    Active Warps Per Scheduler          warp        10.44
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.186%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.44 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.74
    Warp Cycles Per Executed Instruction           cycle        29.74
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.186%                                                                                          
          On average, each warp of this kernel spends 17.5 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 58.8% of the total average of 29.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    421323.03
    Executed Instructions                           inst     97746944
    Avg. Issued Instructions Per Scheduler          inst    421412.15
    Issued Instructions                             inst     97767618
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.01
    Achieved Active Warps Per SM           warp        41.77
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.186%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     53976064
    Average L1 Active Cycles         cycle   1200115.33
    Total L1 Elapsed Cycles          cycle     71586320
    Average L2 Active Cycles         cycle   1002425.71
    Total L2 Elapsed Cycles          cycle     24479904
    Average SM Active Cycles         cycle   1200115.33
    Total SM Elapsed Cycles          cycle     71586320
    Average SMSP Active Cycles       cycle   1200783.51
    Total SMSP Elapsed Cycles        cycle    286345280
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle     12309543
    Memory Throughput                   %        98.61
    DRAM Throughput                     %         3.99
    Duration                      msecond         9.54
    L1/TEX Cache Throughput             %        81.35
    L2 Cache Throughput                 %        98.61
    SM Active Cycles                cycle  12130304.34
    Compute (SM) Throughput             %        28.41
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.34
    Executed Ipc Elapsed  inst/cycle         0.34
    Issue Slots Busy               %         8.55
    Issued Ipc Active     inst/cycle         0.34
    SM Busy                        %         9.60
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.46%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        16.32
    Mem Busy                               %        98.61
    Max Bandwidth                          %        81.38
    L1/TEX Hit Rate                        %        55.22
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        98.35
    Mem Pipes Busy                         %        28.41
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 40.14%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.49
    Issued Warp Per Scheduler                        0.08
    No Eligible                            %        91.51
    Active Warps Per Scheduler          warp         5.93
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 1.389%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 11.8 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          5.93 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        69.80
    Warp Cycles Per Executed Instruction           cycle        69.81
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 1.389%                                                                                          
          On average, each warp of this kernel spends 33.6 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.1% of the total average of 69.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1037559.17
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst   1037646.58
    Issued Instructions                             inst    240734006
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.67
    Achieved Active Warps Per SM           warp        23.84
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 1.389%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   3244709.33
    Total DRAM Elapsed Cycles        cycle    537710592
    Average L1 Active Cycles         cycle  12130304.34
    Total L1 Elapsed Cycles          cycle    712896506
    Average L2 Active Cycles         cycle  10232819.62
    Total L2 Elapsed Cycles          cycle    243897840
    Average SM Active Cycles         cycle  12130304.34
    Total SM Elapsed Cycles          cycle    712896506
    Average SMSP Active Cycles       cycle  12216600.57
    Total SMSP Elapsed Cycles        cycle   2851586024
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 50.3%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle      1235260
    Memory Throughput                   %        93.83
    DRAM Throughput                     %         2.28
    Duration                      usecond       957.73
    L1/TEX Cache Throughput             %        96.46
    L2 Cache Throughput                 %        16.50
    SM Active Cycles                cycle   1200411.71
    Compute (SM) Throughput             %        93.83
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.06
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.37
    Issue Slots Busy               %        35.11
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.11
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.33
    Mem Busy                               %        70.38
    Max Bandwidth                          %        93.83
    L1/TEX Hit Rate                        %        91.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.97
    Mem Pipes Busy                         %        93.83
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 58.64%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.09
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.91
    Active Warps Per Scheduler          warp        10.43
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.171%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.43 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.73
    Warp Cycles Per Executed Instruction           cycle        29.74
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.171%                                                                                          
          On average, each warp of this kernel spends 17.5 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 58.8% of the total average of 29.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    421323.03
    Executed Instructions                           inst     97746944
    Avg. Issued Instructions Per Scheduler          inst    421412.74
    Issued Instructions                             inst     97767756
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.98
    Achieved Active Warps Per SM           warp        41.75
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.171%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       186120
    Total DRAM Elapsed Cycles        cycle     53963776
    Average L1 Active Cycles         cycle   1200411.71
    Total L1 Elapsed Cycles          cycle     71575106
    Average L2 Active Cycles         cycle   1001964.50
    Total L2 Elapsed Cycles          cycle     24474216
    Average SM Active Cycles         cycle   1200411.71
    Total SM Elapsed Cycles          cycle     71575106
    Average SMSP Active Cycles       cycle   1200885.34
    Total SMSP Elapsed Cycles        cycle    286300424
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle     11042949
    Memory Throughput                   %       103.05
    DRAM Throughput                     %         4.44
    Duration                      msecond         8.56
    L1/TEX Cache Throughput             %        90.91
    L2 Cache Throughput                 %       103.05
    SM Active Cycles                cycle  10854918.45
    Compute (SM) Throughput             %        31.68
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.00
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.56
    Issued Ipc Active     inst/cycle         0.38
    SM Busy                        %        10.73
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.34%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        18.15
    Mem Busy                               %       103.05
    Max Bandwidth                          %        89.99
    L1/TEX Hit Rate                        %        57.41
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %       107.08
    Mem Pipes Busy                         %        31.68
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 44.77%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.11
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        90.89
    Active Warps Per Scheduler          warp         6.25
    Eligible Warps Per Scheduler        warp         0.15
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: -3.052%                                                                                   
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 11.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          6.25 active warps per scheduler, but only an average of 0.15 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        68.59
    Warp Cycles Per Executed Instruction           cycle        68.60
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: -3.052%                                                                                         
          On average, each warp of this kernel spends 36.1 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 52.6% of the total average of 68.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1037559.17
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst   1037647.41
    Issued Instructions                             inst    240734198
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        51.64
    Achieved Active Warps Per SM           warp        24.79
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: -3.052%                                                                                         
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   3236474.67
    Total DRAM Elapsed Cycles        cycle    482382848
    Average L1 Active Cycles         cycle  10854918.45
    Total L1 Elapsed Cycles          cycle    639257442
    Average L2 Active Cycles         cycle  10035819.67
    Total L2 Elapsed Cycles          cycle    218798016
    Average SM Active Cycles         cycle  10854918.45
    Total SM Elapsed Cycles          cycle    639257442
    Average SMSP Active Cycles       cycle  11385262.79
    Total SMSP Elapsed Cycles        cycle   2557029768
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 54.99%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle      1237350
    Memory Throughput                   %        93.70
    DRAM Throughput                     %         2.27
    Duration                      usecond       959.36
    L1/TEX Cache Throughput             %        96.44
    L2 Cache Throughput                 %        16.47
    SM Active Cycles                cycle   1200594.64
    Compute (SM) Throughput             %        93.70
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.06
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.36
    Issue Slots Busy               %        35.10
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.10
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.30
    Mem Busy                               %        70.29
    Max Bandwidth                          %        93.70
    L1/TEX Hit Rate                        %        91.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        93.02
    Mem Pipes Busy                         %        93.70
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 58.56%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.08
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.92
    Active Warps Per Scheduler          warp        10.43
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.299%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.43 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.73
    Warp Cycles Per Executed Instruction           cycle        29.74
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.299%                                                                                          
          On average, each warp of this kernel spends 17.5 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 58.8% of the total average of 29.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    421323.03
    Executed Instructions                           inst     97746944
    Avg. Issued Instructions Per Scheduler          inst    421413.19
    Issued Instructions                             inst     97767860
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.00
    Achieved Active Warps Per SM           warp        41.76
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.299%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     54054912
    Average L1 Active Cycles         cycle   1200594.64
    Total L1 Elapsed Cycles          cycle     71672382
    Average L2 Active Cycles         cycle   1002494.71
    Total L2 Elapsed Cycles          cycle     24515112
    Average SM Active Cycles         cycle   1200594.64
    Total SM Elapsed Cycles          cycle     71672382
    Average SMSP Active Cycles       cycle   1201180.78
    Total SMSP Elapsed Cycles        cycle    286689528
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle     11426558
    Memory Throughput                   %        97.09
    DRAM Throughput                     %         4.30
    Duration                      msecond         8.86
    L1/TEX Cache Throughput             %        88.02
    L2 Cache Throughput                 %        97.09
    SM Active Cycles                cycle  11210087.57
    Compute (SM) Throughput             %        30.61
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.37
    Executed Ipc Elapsed  inst/cycle         0.36
    Issue Slots Busy               %         9.26
    Issued Ipc Active     inst/cycle         0.37
    SM Busy                        %        10.39
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.68%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        17.58
    Mem Busy                               %        97.09
    Max Bandwidth                          %        76.86
    L1/TEX Hit Rate                        %        61.04
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        92.30
    Mem Pipes Busy                         %        30.61
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 43.24%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.68
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.32
    Active Warps Per Scheduler          warp         6.23
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 2.907%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.3 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          6.23 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        64.36
    Warp Cycles Per Executed Instruction           cycle        64.37
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.907%                                                                                          
          On average, each warp of this kernel spends 41.1 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 63.9% of the total average of 64.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1037559.17
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst   1037646.46
    Issued Instructions                             inst    240733979
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        46.64
    Achieved Active Warps Per SM           warp        22.39
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.907%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      3244608
    Total DRAM Elapsed Cycles        cycle    499138560
    Average L1 Active Cycles         cycle  11210087.57
    Total L1 Elapsed Cycles          cycle    661731544
    Average L2 Active Cycles         cycle   9471199.71
    Total L2 Elapsed Cycles          cycle    226398144
    Average SM Active Cycles         cycle  11210087.57
    Total SM Elapsed Cycles          cycle    661731544
    Average SMSP Active Cycles       cycle  10716998.28
    Total SMSP Elapsed Cycles        cycle   2646926176
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 50.15%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle      1236408
    Memory Throughput                   %        93.78
    DRAM Throughput                     %         2.28
    Duration                      usecond       958.62
    L1/TEX Cache Throughput             %        96.43
    L2 Cache Throughput                 %        16.49
    SM Active Cycles                cycle   1200773.33
    Compute (SM) Throughput             %        93.78
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.36
    Issue Slots Busy               %        35.10
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.10
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.30
    Mem Busy                               %        70.34
    Max Bandwidth                          %        93.78
    L1/TEX Hit Rate                        %        91.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        93.11
    Mem Pipes Busy                         %        93.78
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 58.61%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.09
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.91
    Active Warps Per Scheduler          warp        10.43
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.223%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.43 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.74
    Warp Cycles Per Executed Instruction           cycle        29.74
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.223%                                                                                          
          On average, each warp of this kernel spends 17.5 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 58.8% of the total average of 29.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    421323.03
    Executed Instructions                           inst     97746944
    Avg. Issued Instructions Per Scheduler          inst    421411.62
    Issued Instructions                             inst     97767496
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.97
    Achieved Active Warps Per SM           warp        41.74
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.223%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     54013952
    Average L1 Active Cycles         cycle   1200773.33
    Total L1 Elapsed Cycles          cycle     71614846
    Average L2 Active Cycles         cycle   1001639.54
    Total L2 Elapsed Cycles          cycle     24496296
    Average SM Active Cycles         cycle   1200773.33
    Total SM Elapsed Cycles          cycle     71614846
    Average SMSP Active Cycles       cycle   1200914.48
    Total SMSP Elapsed Cycles        cycle    286459384
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle     10948379
    Memory Throughput                   %       102.49
    DRAM Throughput                     %         4.47
    Duration                      msecond         8.49
    L1/TEX Cache Throughput             %        92.60
    L2 Cache Throughput                 %       102.49
    SM Active Cycles                cycle  10656517.69
    Compute (SM) Throughput             %        31.95
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.54
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.39
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %         9.74
    Issued Ipc Active     inst/cycle         0.39
    SM Busy                        %        10.93
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 89.14%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        18.29
    Mem Busy                               %       102.49
    Max Bandwidth                          %        85.26
    L1/TEX Hit Rate                        %        60.39
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %       106.29
    Mem Pipes Busy                         %        31.95
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.15%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.62
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        90.38
    Active Warps Per Scheduler          warp         5.91
    Eligible Warps Per Scheduler        warp         0.13
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: -2.486%                                                                                   
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 10.4 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          5.91 active warps per scheduler, but only an average of 0.13 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        61.47
    Warp Cycles Per Executed Instruction           cycle        61.48
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: -2.486%                                                                                         
          On average, each warp of this kernel spends 40.0 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 65.1% of the total average of 61.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1037559.17
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst   1037647.34
    Issued Instructions                             inst    240734182
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        50.37
    Achieved Active Warps Per SM           warp        24.18
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: -2.486%                                                                                         
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   3234442.67
    Total DRAM Elapsed Cycles        cycle    478252032
    Average L1 Active Cycles         cycle  10656517.69
    Total L1 Elapsed Cycles          cycle    633856176
    Average L2 Active Cycles         cycle   9045937.58
    Total L2 Elapsed Cycles          cycle    216919824
    Average SM Active Cycles         cycle  10656517.69
    Total SM Elapsed Cycles          cycle    633856176
    Average SMSP Active Cycles       cycle  10788015.42
    Total SMSP Elapsed Cycles        cycle   2535424704
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.99%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle      1233696
    Memory Throughput                   %        93.96
    DRAM Throughput                     %         2.28
    Duration                      usecond       956.51
    L1/TEX Cache Throughput             %        96.44
    L2 Cache Throughput                 %        16.53
    SM Active Cycles                cycle   1200632.21
    Compute (SM) Throughput             %        93.96
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.06
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.37
    Issue Slots Busy               %        35.10
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.10
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.32
    Mem Busy                               %        70.48
    Max Bandwidth                          %        93.96
    L1/TEX Hit Rate                        %        91.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        93.04
    Mem Pipes Busy                         %        93.96
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 58.73%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.13
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.87
    Active Warps Per Scheduler          warp        10.45
    Eligible Warps Per Scheduler        warp         1.85
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.036%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.45 active warps per scheduler, but only an average of 1.85 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.74
    Warp Cycles Per Executed Instruction           cycle        29.75
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.036%                                                                                          
          On average, each warp of this kernel spends 17.5 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 58.8% of the total average of 29.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    421323.03
    Executed Instructions                           inst     97746944
    Avg. Issued Instructions Per Scheduler          inst    421412.53
    Issued Instructions                             inst     97767707
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.00
    Achieved Active Warps Per SM           warp        41.76
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.036%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    185778.67
    Total DRAM Elapsed Cycles        cycle     53894144
    Average L1 Active Cycles         cycle   1200632.21
    Total L1 Elapsed Cycles          cycle     71472334
    Average L2 Active Cycles         cycle   1002543.38
    Total L2 Elapsed Cycles          cycle     24443184
    Average SM Active Cycles         cycle   1200632.21
    Total SM Elapsed Cycles          cycle     71472334
    Average SMSP Active Cycles       cycle   1199721.09
    Total SMSP Elapsed Cycles        cycle    285889336
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle     12013816
    Memory Throughput                   %       100.86
    DRAM Throughput                     %         4.08
    Duration                      msecond         9.31
    L1/TEX Cache Throughput             %        82.94
    L2 Cache Throughput                 %       100.86
    SM Active Cycles                cycle  11897743.64
    Compute (SM) Throughput             %        29.13
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.35
    Executed Ipc Elapsed  inst/cycle         0.35
    Issue Slots Busy               %         8.72
    Issued Ipc Active     inst/cycle         0.35
    SM Busy                        %         9.79
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.28%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        16.70
    Mem Busy                               %       100.86
    Max Bandwidth                          %        78.91
    L1/TEX Hit Rate                        %        55.35
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        99.84
    Mem Pipes Busy                         %        29.13
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.15%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.74
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.26
    Active Warps Per Scheduler          warp         5.94
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: -0.8552%                                                                                  
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 11.4 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          5.94 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        67.95
    Warp Cycles Per Executed Instruction           cycle        67.95
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: -0.8552%                                                                                        
          On average, each warp of this kernel spends 33.5 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 49.3% of the total average of 67.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1037559.17
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst   1037646.53
    Issued Instructions                             inst    240733996
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        50.50
    Achieved Active Warps Per SM           warp        24.24
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: -0.8552%                                                                                        
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   3239341.33
    Total DRAM Elapsed Cycles        cycle    524790784
    Average L1 Active Cycles         cycle  11897743.64
    Total L1 Elapsed Cycles          cycle    695385654
    Average L2 Active Cycles         cycle   9923053.46
    Total L2 Elapsed Cycles          cycle    238037232
    Average SM Active Cycles         cycle  11897743.64
    Total SM Elapsed Cycles          cycle    695385654
    Average SMSP Active Cycles       cycle  11866402.23
    Total SMSP Elapsed Cycles        cycle   2781542616
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.98%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle      1239323
    Memory Throughput                   %        93.54
    DRAM Throughput                     %         2.28
    Duration                      usecond       960.93
    L1/TEX Cache Throughput             %        96.41
    L2 Cache Throughput                 %        16.44
    SM Active Cycles                cycle   1201006.74
    Compute (SM) Throughput             %        93.54
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.36
    Issue Slots Busy               %        35.09
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.09
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.32
    Mem Busy                               %        70.17
    Max Bandwidth                          %        93.54
    L1/TEX Hit Rate                        %        91.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        93.04
    Mem Pipes Busy                         %        93.54
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 58.46%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.11
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.89
    Active Warps Per Scheduler          warp        10.44
    Eligible Warps Per Scheduler        warp         1.85
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.457%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.44 active warps per scheduler, but only an average of 1.85 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.74
    Warp Cycles Per Executed Instruction           cycle        29.75
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.457%                                                                                          
          On average, each warp of this kernel spends 17.5 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 58.7% of the total average of 29.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    421323.03
    Executed Instructions                           inst     97746944
    Avg. Issued Instructions Per Scheduler          inst    421413.16
    Issued Instructions                             inst     97767854
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.96
    Achieved Active Warps Per SM           warp        41.74
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.457%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    186546.67
    Total DRAM Elapsed Cycles        cycle     54141952
    Average L1 Active Cycles         cycle   1201006.74
    Total L1 Elapsed Cycles          cycle     71793888
    Average L2 Active Cycles         cycle   1003204.83
    Total L2 Elapsed Cycles          cycle     24554808
    Average SM Active Cycles         cycle   1201006.74
    Total SM Elapsed Cycles          cycle     71793888
    Average SMSP Active Cycles       cycle   1200344.44
    Total SMSP Elapsed Cycles        cycle    287175552
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle     12398240
    Memory Throughput                   %        97.63
    DRAM Throughput                     %         3.97
    Duration                      msecond         9.61
    L1/TEX Cache Throughput             %        80.81
    L2 Cache Throughput                 %        97.63
    SM Active Cycles                cycle  12211540.31
    Compute (SM) Throughput             %        28.21
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         3.80
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.34
    Executed Ipc Elapsed  inst/cycle         0.34
    Issue Slots Busy               %         8.50
    Issued Ipc Active     inst/cycle         0.34
    SM Busy                        %         9.54
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.52%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        16.22
    Mem Busy                               %        97.63
    Max Bandwidth                          %        80.47
    L1/TEX Hit Rate                        %        55.41
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        98.28
    Mem Pipes Busy                         %        28.21
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.86%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.51
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.49
    Active Warps Per Scheduler          warp         5.93
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 2.374%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 11.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          5.93 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        69.66
    Warp Cycles Per Executed Instruction           cycle        69.67
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.374%                                                                                          
          On average, each warp of this kernel spends 33.7 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.4% of the total average of 69.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1037559.17
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst   1037647.50
    Issued Instructions                             inst    240734220
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.59
    Achieved Active Warps Per SM           warp        23.80
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.374%                                                                                          
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   3247205.33
    Total DRAM Elapsed Cycles        cycle    541584384
    Average L1 Active Cycles         cycle  12211540.31
    Total L1 Elapsed Cycles          cycle    717870020
    Average L2 Active Cycles         cycle  10210018.21
    Total L2 Elapsed Cycles          cycle    245653824
    Average SM Active Cycles         cycle  12211540.31
    Total SM Elapsed Cycles          cycle    717870020
    Average SMSP Active Cycles       cycle  12187042.72
    Total SMSP Elapsed Cycles        cycle   2871480080
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.83%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle      1234136
    Memory Throughput                   %        93.90
    DRAM Throughput                     %         2.28
    Duration                      usecond       956.86
    L1/TEX Cache Throughput             %        96.42
    L2 Cache Throughput                 %        16.52
    SM Active Cycles                cycle   1200880.88
    Compute (SM) Throughput             %        93.90
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.06
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.37
    Issue Slots Busy               %        35.09
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.09
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second         9.32
    Mem Busy                               %        70.44
    Max Bandwidth                          %        93.90
    L1/TEX Hit Rate                        %        91.98
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        93.14
    Mem Pipes Busy                         %        93.90
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 58.69%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.12
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.88
    Active Warps Per Scheduler          warp        10.44
    Eligible Warps Per Scheduler        warp         1.85
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.097%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          10.44 active warps per scheduler, but only an average of 1.85 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.74
    Warp Cycles Per Executed Instruction           cycle        29.74
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.097%                                                                                          
          On average, each warp of this kernel spends 17.5 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 58.8% of the total average of 29.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    421323.03
    Executed Instructions                           inst     97746944
    Avg. Issued Instructions Per Scheduler          inst    421412.04
    Issued Instructions                             inst     97767593
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.97
    Achieved Active Warps Per SM           warp        41.74
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.097%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       185864
    Total DRAM Elapsed Cycles        cycle     53915648
    Average L1 Active Cycles         cycle   1200880.88
    Total L1 Elapsed Cycles          cycle     71518772
    Average L2 Active Cycles         cycle   1002357.42
    Total L2 Elapsed Cycles          cycle     24452400
    Average SM Active Cycles         cycle   1200880.88
    Total SM Elapsed Cycles          cycle     71518772
    Average SMSP Active Cycles       cycle   1200069.16
    Total SMSP Elapsed Cycles        cycle    286075088
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         9.39
    SM Frequency            cycle/nsecond         1.29
    Elapsed Cycles                  cycle     12398457
    Memory Throughput                   %        97.76
    DRAM Throughput                     %         3.95
    Duration                      msecond         9.61
    L1/TEX Cache Throughput             %        80.56
    L2 Cache Throughput                 %        97.76
    SM Active Cycles                cycle  12248778.22
    Compute (SM) Throughput             %        28.20
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L2 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.36
    Dropped Samples                sample            0
    Maximum Sampling Interval     usecond           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.34
    Executed Ipc Elapsed  inst/cycle         0.34
    Issue Slots Busy               %         8.47
    Issued Ipc Active     inst/cycle         0.34
    SM Busy                        %         9.51
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.55%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        16.17
    Mem Busy                               %        97.76
    Max Bandwidth                          %        80.65
    L1/TEX Hit Rate                        %        55.36
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        98.18
    Mem Pipes Busy                         %        28.20
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.84%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.53
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.47
    Active Warps Per Scheduler          warp         5.93
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 2.24%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 11.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          5.93 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        69.59
    Warp Cycles Per Executed Instruction           cycle        69.59
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.24%                                                                                           
          On average, each warp of this kernel spends 33.6 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 48.3% of the total average of 69.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1037559.17
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst   1037646.53
    Issued Instructions                             inst    240733996
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           48
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.19
    Achieved Active Warps Per SM           warp        23.61
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.24%                                                                                           
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   3237602.67
    Total DRAM Elapsed Cycles        cycle    541593600
    Average L1 Active Cycles         cycle  12248778.22
    Total L1 Elapsed Cycles          cycle    718205256
    Average L2 Active Cycles         cycle  10200796.17
    Total L2 Elapsed Cycles          cycle    245657424
    Average SM Active Cycles         cycle  12248778.22
    Total SM Elapsed Cycles          cycle    718205256
    Average SMSP Active Cycles       cycle  12170105.54
    Total SMSP Elapsed Cycles        cycle   2872821024
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.78%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

