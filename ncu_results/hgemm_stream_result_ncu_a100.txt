==PROF== Connected to process 157 (/data/CENG443_project/src/hgemm_stream_sm80)
==PROF== Profiling "hgemm_normal" - 0: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_tensor_core" - 1: 0%....50%....100% - 49 passes
==PROF== Profiling "hgemm_normal" - 2: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_tensor_core" - 3: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_normal" - 4: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_tensor_core" - 5: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_normal" - 6: 0%....50%....100% - 48 passes
==PROF== Profiling "hgemm_tensor_core" - 7: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_normal" - 8: 0%....50%....100% - 48 passes
==PROF== Profiling "hgemm_tensor_core" - 9: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_normal" - 10: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_tensor_core" - 11: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_normal" - 12: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_tensor_core" - 13: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_normal" - 14: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_tensor_core" - 15: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_normal" - 16: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_tensor_core" - 17: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_normal" - 18: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_tensor_core" - 19: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_normal" - 20: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_tensor_core" - 21: 0%....50%....100% - 49 passes
==PROF== Profiling "hgemm_normal" - 22: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_tensor_core" - 23: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_normal" - 24: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_tensor_core" - 25: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_normal" - 26: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_tensor_core" - 27: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_normal" - 28: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_tensor_core" - 29: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_normal" - 30: 0%....50%....100% - 47 passes
==PROF== Profiling "hgemm_tensor_core" - 31: 0%....50%....100% - 47 passes

=== Concurrent HGEMM Performance (Mixed Tensor + Normal) ===

=== Performance Results ===
Matrix size (Normal): 2048x2048
Matrix size (Tensor): 8192x8192
Number of streams: 4
Mixed version (concurrent tensor + normal) time: 115242.500 ms

=== Result Verification ===
C[0][0] = 8192
C[4096][0] = 8192
Expected value = 8192
==PROF== Disconnected from process 157
[157] hgemm_stream_sm80@127.0.0.1
  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       637104
    Memory Throughput                 %        73.22
    DRAM Throughput                   %         0.77
    Duration                         us       598.30
    L1/TEX Cache Throughput           %        78.33
    L2 Cache Throughput               %         8.03
    SM Active Cycles              cycle    595385.72
    Compute (SM) Throughput           %        48.81
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 12% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.62
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       160000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.50
    Executed Ipc Elapsed  inst/cycle         1.41
    Issue Slots Busy               %        37.61
    Issued Ipc Active     inst/cycle         1.50
    SM Busy                        %        37.61
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.48%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        14.90
    Mem Busy                              %        73.22
    Max Bandwidth                         %        49.06
    L1/TEX Hit Rate                       %        92.54
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        91.82
    Mem Pipes Busy                        %        48.81
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.76%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.65
    Issued Warp Per Scheduler                        0.38
    No Eligible                            %        62.35
    Active Warps Per Scheduler          warp        11.06
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.78%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          11.06 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.37
    Warp Cycles Per Executed Instruction           cycle        29.38
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.78%                                                                                          
          On average, each warp of this kernel spends 15.2 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 51.7% of the total average of 29.4 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    223838.81
    Executed Instructions                           inst     96698368
    Avg. Issued Instructions Per Scheduler          inst    223919.82
    Issued Instructions                             inst     96733363
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                1.19
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 31.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.05
    Achieved Active Warps Per SM           warp        44.19
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.78%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6966.80
    Total DRAM Elapsed Cycles        cycle     36181760
    Average L1 Active Cycles         cycle    595385.72
    Total L1 Elapsed Cycles          cycle     68793068
    Average L2 Active Cycles         cycle    568180.47
    Total L2 Elapsed Cycles          cycle     48805920
    Average SM Active Cycles         cycle    595385.72
    Total SM Elapsed Cycles          cycle     68793068
    Average SMSP Active Cycles       cycle    594775.84
    Total SMSP Elapsed Cycles        cycle    275172272
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.764%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 6.17% above the average, while the minimum instance value is 16.62% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.307%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.68% above the average, while the minimum instance value is 15.70% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.764%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 6.17% above the average, while the minimum instance value is 16.62% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 15, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle      5807279
    Memory Throughput                 %        91.22
    DRAM Throughput                   %         1.84
    Duration                         ms         5.45
    L1/TEX Cache Throughput           %        96.36
    L2 Cache Throughput               %        42.66
    SM Active Cycles              cycle   5497438.45
    Compute (SM) Throughput           %        16.15
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.88
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       320000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.41
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.14
    Issued Ipc Active     inst/cycle         0.41
    SM Busy                        %        10.14
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.67%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        35.53
    Mem Busy                              %        91.22
    Max Bandwidth                         %        40.54
    L1/TEX Hit Rate                       %        65.61
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        95.54
    Mem Pipes Busy                        %        16.15
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.61%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.13
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.87
    Active Warps Per Scheduler          warp         7.87
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.783%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          7.87 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        77.72
    Warp Cycles Per Executed Instruction           cycle        77.73
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.783%                                                                                          
          On average, each warp of this kernel spends 73.3 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.2% of the total average of 77.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    557207.70
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst    557255.62
    Issued Instructions                             inst    240734429
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                4.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.24
    Achieved Active Warps Per SM           warp        31.52
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.783%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    151348.50
    Total DRAM Elapsed Cycles        cycle    329788416
    Average L1 Active Cycles         cycle   5497438.45
    Total L1 Elapsed Cycles          cycle    627180592
    Average L2 Active Cycles         cycle   5548732.26
    Total L2 Elapsed Cycles          cycle    444949840
    Average SM Active Cycles         cycle   5497438.45
    Total SM Elapsed Cycles          cycle    627180592
    Average SMSP Active Cycles       cycle   5500861.31
    Total SMSP Elapsed Cycles        cycle   2508722368
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.011%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.29% above the average, while the minimum instance value is 15.33% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.011%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.29% above the average, while the minimum instance value is 15.33% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.83%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       633395
    Memory Throughput                 %        73.65
    DRAM Throughput                   %         0.77
    Duration                         us       594.78
    L1/TEX Cache Throughput           %        78.41
    L2 Cache Throughput               %         8.07
    SM Active Cycles              cycle    594795.55
    Compute (SM) Throughput           %        49.10
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 12% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.82
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       160000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.51
    Executed Ipc Elapsed  inst/cycle         1.41
    Issue Slots Busy               %        37.65
    Issued Ipc Active     inst/cycle         1.51
    SM Busy                        %        37.65
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.47%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        14.99
    Mem Busy                              %        73.65
    Max Bandwidth                         %        49.34
    L1/TEX Hit Rate                       %        92.55
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        91.90
    Mem Pipes Busy                        %        49.10
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.03%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.64
    Issued Warp Per Scheduler                        0.38
    No Eligible                            %        62.36
    Active Warps Per Scheduler          warp        11.05
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.35%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          11.05 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.34
    Warp Cycles Per Executed Instruction           cycle        29.35
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.35%                                                                                          
          On average, each warp of this kernel spends 15.2 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 51.8% of the total average of 29.3 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    223838.81
    Executed Instructions                           inst     96698368
    Avg. Issued Instructions Per Scheduler          inst    223920.63
    Issued Instructions                             inst     96733712
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                1.19
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 31.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.05
    Achieved Active Warps Per SM           warp        44.19
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.35%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6967.50
    Total DRAM Elapsed Cycles        cycle     35969536
    Average L1 Active Cycles         cycle    594795.55
    Total L1 Elapsed Cycles          cycle     68390264
    Average L2 Active Cycles         cycle    566814.61
    Total L2 Elapsed Cycles          cycle     48522480
    Average SM Active Cycles         cycle    594795.55
    Total SM Elapsed Cycles          cycle     68390264
    Average SMSP Active Cycles       cycle    594879.03
    Total SMSP Elapsed Cycles        cycle    273561056
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.375%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.72% above the average, while the minimum instance value is 16.48% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.657%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 6.02% above the average, while the minimum instance value is 16.12% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.375%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.72% above the average, while the minimum instance value is 16.48% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 17, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle      5807752
    Memory Throughput                 %        91.21
    DRAM Throughput                   %         1.83
    Duration                         ms         5.45
    L1/TEX Cache Throughput           %        96.35
    L2 Cache Throughput               %        42.12
    SM Active Cycles              cycle   5497582.03
    Compute (SM) Throughput           %        16.15
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.88
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       320000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.41
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.14
    Issued Ipc Active     inst/cycle         0.41
    SM Busy                        %        10.14
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.67%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        35.43
    Mem Busy                              %        91.21
    Max Bandwidth                         %        40.40
    L1/TEX Hit Rate                       %        66.00
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.18
    Mem Pipes Busy                        %        16.15
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.6%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.13
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.87
    Active Warps Per Scheduler          warp         7.87
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.79%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          7.87 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        77.68
    Warp Cycles Per Executed Instruction           cycle        77.69
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.79%                                                                                           
          On average, each warp of this kernel spends 73.3 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.3% of the total average of 77.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    557207.70
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst    557255.63
    Issued Instructions                             inst    240734431
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                4.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.21
    Achieved Active Warps Per SM           warp        31.50
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.79%                                                                                           
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    150947.90
    Total DRAM Elapsed Cycles        cycle    329816064
    Average L1 Active Cycles         cycle   5497582.03
    Total L1 Elapsed Cycles          cycle    627232618
    Average L2 Active Cycles         cycle   5557356.89
    Total L2 Elapsed Cycles          cycle    444985920
    Average SM Active Cycles         cycle   5497582.03
    Total SM Elapsed Cycles          cycle    627232618
    Average SMSP Active Cycles       cycle   5503002.88
    Total SMSP Elapsed Cycles        cycle   2508930472
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.016%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.30% above the average, while the minimum instance value is 15.44% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.038%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.32% above the average, while the minimum instance value is 15.43% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.016%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.30% above the average, while the minimum instance value is 15.44% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.91%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       634898
    Memory Throughput                 %        73.47
    DRAM Throughput                   %         0.77
    Duration                         us       596.22
    L1/TEX Cache Throughput           %        78.41
    L2 Cache Throughput               %         8.06
    SM Active Cycles              cycle    594845.69
    Compute (SM) Throughput           %        48.98
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 12% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.82
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       160000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.51
    Executed Ipc Elapsed  inst/cycle         1.41
    Issue Slots Busy               %        37.64
    Issued Ipc Active     inst/cycle         1.51
    SM Busy                        %        37.64
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.47%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        14.96
    Mem Busy                              %        73.47
    Max Bandwidth                         %        49.23
    L1/TEX Hit Rate                       %        92.54
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        91.74
    Mem Pipes Busy                        %        48.98
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.92%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.62
    Issued Warp Per Scheduler                        0.38
    No Eligible                            %        62.38
    Active Warps Per Scheduler          warp        11.06
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.53%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          11.06 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.39
    Warp Cycles Per Executed Instruction           cycle        29.40
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.53%                                                                                          
          On average, each warp of this kernel spends 15.2 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 51.7% of the total average of 29.4 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    223838.81
    Executed Instructions                           inst     96698368
    Avg. Issued Instructions Per Scheduler          inst    223920.22
    Issued Instructions                             inst     96733537
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                1.19
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 30.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.15
    Achieved Active Warps Per SM           warp        44.25
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.53%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6966.70
    Total DRAM Elapsed Cycles        cycle     36056832
    Average L1 Active Cycles         cycle    594845.69
    Total L1 Elapsed Cycles          cycle     68554904
    Average L2 Active Cycles         cycle    567883.26
    Total L2 Elapsed Cycles          cycle     48639040
    Average SM Active Cycles         cycle    594845.69
    Total SM Elapsed Cycles          cycle     68554904
    Average SMSP Active Cycles       cycle    595143.23
    Total SMSP Elapsed Cycles        cycle    274219616
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.527%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.90% above the average, while the minimum instance value is 15.68% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.676%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 6.05% above the average, while the minimum instance value is 15.80% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.527%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.90% above the average, while the minimum instance value is 15.68% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 15, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle      5800476
    Memory Throughput                 %        91.32
    DRAM Throughput                   %         1.82
    Duration                         ms         5.45
    L1/TEX Cache Throughput           %        96.39
    L2 Cache Throughput               %        43.43
    SM Active Cycles              cycle   5495514.41
    Compute (SM) Throughput           %        16.17
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.88
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       320000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.41
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.14
    Issued Ipc Active     inst/cycle         0.41
    SM Busy                        %        10.14
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.66%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        35.19
    Mem Busy                              %        91.32
    Max Bandwidth                         %        39.72
    L1/TEX Hit Rate                       %        65.90
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        94.58
    Mem Pipes Busy                        %        16.17
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.66%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.13
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.87
    Active Warps Per Scheduler          warp         7.84
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.676%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          7.84 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        77.33
    Warp Cycles Per Executed Instruction           cycle        77.34
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.676%                                                                                          
          On average, each warp of this kernel spends 73.4 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.9% of the total average of 77.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    557207.70
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst    557255.70
    Issued Instructions                             inst    240734461
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                4.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.01
    Achieved Active Warps Per SM           warp        31.37
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.676%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    149717.20
    Total DRAM Elapsed Cycles        cycle    329402880
    Average L1 Active Cycles         cycle   5495514.41
    Total L1 Elapsed Cycles          cycle    626446568
    Average L2 Active Cycles         cycle   5555000.30
    Total L2 Elapsed Cycles          cycle    444428160
    Average SM Active Cycles         cycle   5495514.41
    Total SM Elapsed Cycles          cycle    626446568
    Average SMSP Active Cycles       cycle   5499592.61
    Total SMSP Elapsed Cycles        cycle   2505786272
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.14%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.42% above the average, while the minimum instance value is 15.43% below the average.      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.95%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       635781
    Memory Throughput                 %        73.37
    DRAM Throughput                   %         0.77
    Duration                         us       597.06
    L1/TEX Cache Throughput           %        78.36
    L2 Cache Throughput               %         8.04
    SM Active Cycles              cycle    595194.07
    Compute (SM) Throughput           %        48.91
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 12% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       524.29
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle        20000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.50
    Executed Ipc Elapsed  inst/cycle         1.41
    Issue Slots Busy               %        37.62
    Issued Ipc Active     inst/cycle         1.50
    SM Busy                        %        37.62
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.48%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        14.94
    Mem Busy                              %        73.37
    Max Bandwidth                         %        49.16
    L1/TEX Hit Rate                       %        92.54
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        91.83
    Mem Pipes Busy                        %        48.91
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.86%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.64
    Issued Warp Per Scheduler                        0.38
    No Eligible                            %        62.36
    Active Warps Per Scheduler          warp        11.06
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.63%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          11.06 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.37
    Warp Cycles Per Executed Instruction           cycle        29.38
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.63%                                                                                          
          On average, each warp of this kernel spends 15.2 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 51.7% of the total average of 29.4 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    223838.81
    Executed Instructions                           inst     96698368
    Avg. Issued Instructions Per Scheduler          inst    223921.41
    Issued Instructions                             inst     96734048
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                1.19
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 30.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.06
    Achieved Active Warps Per SM           warp        44.20
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.63%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6966.80
    Total DRAM Elapsed Cycles        cycle     36106240
    Average L1 Active Cycles         cycle    595194.07
    Total L1 Elapsed Cycles          cycle     68649182
    Average L2 Active Cycles         cycle    568258.28
    Total L2 Elapsed Cycles          cycle     48705840
    Average SM Active Cycles         cycle    595194.07
    Total SM Elapsed Cycles          cycle     68649182
    Average SMSP Active Cycles       cycle    594855.16
    Total SMSP Elapsed Cycles        cycle    274596728
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.631%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 6.01% above the average, while the minimum instance value is 16.22% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.751%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 6.15% above the average, while the minimum instance value is 16.59% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.631%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 6.01% above the average, while the minimum instance value is 16.22% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 17, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle      5800145
    Memory Throughput                 %        91.33
    DRAM Throughput                   %         1.82
    Duration                         ms         5.45
    L1/TEX Cache Throughput           %        96.29
    L2 Cache Throughput               %        42.72
    SM Active Cycles              cycle   5501266.01
    Compute (SM) Throughput           %        16.17
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.88
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       320000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.41
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.13
    Issued Ipc Active     inst/cycle         0.41
    SM Busy                        %        10.13
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.67%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        35.20
    Mem Busy                              %        91.33
    Max Bandwidth                         %        41.15
    L1/TEX Hit Rate                       %        65.70
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        94.94
    Mem Pipes Busy                        %        16.17
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.66%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.14
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.86
    Active Warps Per Scheduler          warp         7.86
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.67%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          7.86 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        77.53
    Warp Cycles Per Executed Instruction           cycle        77.54
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.67%                                                                                           
          On average, each warp of this kernel spends 73.3 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.6% of the total average of 77.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    557207.70
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst    557255.81
    Issued Instructions                             inst    240734508
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                4.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.09
    Achieved Active Warps Per SM           warp        31.42
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.67%                                                                                           
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    149755.70
    Total DRAM Elapsed Cycles        cycle    329384704
    Average L1 Active Cycles         cycle   5501266.01
    Total L1 Elapsed Cycles          cycle    626410264
    Average L2 Active Cycles         cycle   5554897.78
    Total L2 Elapsed Cycles          cycle    444403920
    Average SM Active Cycles         cycle   5501266.01
    Total SM Elapsed Cycles          cycle    626410264
    Average SMSP Active Cycles       cycle   5497298.31
    Total SMSP Elapsed Cycles        cycle   2505641056
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.069%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.35% above the average, while the minimum instance value is 15.36% below the average.      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.95%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       634998
    Memory Throughput                 %        73.46
    DRAM Throughput                   %         0.77
    Duration                         us       596.32
    L1/TEX Cache Throughput           %        78.42
    L2 Cache Throughput               %         8.05
    SM Active Cycles              cycle    594718.27
    Compute (SM) Throughput           %        48.97
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 12% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Kbyte       524.29
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle        20000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.51
    Executed Ipc Elapsed  inst/cycle         1.41
    Issue Slots Busy               %        37.65
    Issued Ipc Active     inst/cycle         1.51
    SM Busy                        %        37.65
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.47%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        14.95
    Mem Busy                              %        73.46
    Max Bandwidth                         %        49.23
    L1/TEX Hit Rate                       %        92.55
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        91.77
    Mem Pipes Busy                        %        48.97
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.91%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.65
    Issued Warp Per Scheduler                        0.38
    No Eligible                            %        62.35
    Active Warps Per Scheduler          warp        11.05
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.54%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          11.05 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.36
    Warp Cycles Per Executed Instruction           cycle        29.37
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.54%                                                                                          
          On average, each warp of this kernel spends 15.2 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 51.8% of the total average of 29.4 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    223838.81
    Executed Instructions                           inst     96698368
    Avg. Issued Instructions Per Scheduler          inst    223919.66
    Issued Instructions                             inst     96733292
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                1.19
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 30.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.09
    Achieved Active Warps Per SM           warp        44.22
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.54%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6966.70
    Total DRAM Elapsed Cycles        cycle     36062720
    Average L1 Active Cycles         cycle    594718.27
    Total L1 Elapsed Cycles          cycle     68565640
    Average L2 Active Cycles         cycle    566635.18
    Total L2 Elapsed Cycles          cycle     48647680
    Average SM Active Cycles         cycle    594718.27
    Total SM Elapsed Cycles          cycle     68565640
    Average SMSP Active Cycles       cycle    594752.19
    Total SMSP Elapsed Cycles        cycle    274262560
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.584%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.96% above the average, while the minimum instance value is 16.43% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.57%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.95% above the average, while the minimum instance value is 15.92% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.584%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.96% above the average, while the minimum instance value is 16.43% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 15, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle      5808537
    Memory Throughput                 %        91.20
    DRAM Throughput                   %         1.82
    Duration                         ms         5.45
    L1/TEX Cache Throughput           %        96.36
    L2 Cache Throughput               %        42.50
    SM Active Cycles              cycle   5497233.69
    Compute (SM) Throughput           %        16.14
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.88
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       320000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.41
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.14
    Issued Ipc Active     inst/cycle         0.41
    SM Busy                        %        10.14
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.67%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        35.31
    Mem Busy                              %        91.20
    Max Bandwidth                         %        40.58
    L1/TEX Hit Rate                       %        66.04
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        95.42
    Mem Pipes Busy                        %        16.14
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.6%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.13
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.87
    Active Warps Per Scheduler          warp         7.85
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.803%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          7.85 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        77.42
    Warp Cycles Per Executed Instruction           cycle        77.42
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.803%                                                                                          
          On average, each warp of this kernel spends 73.2 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.6% of the total average of 77.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    557207.70
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst    557256.69
    Issued Instructions                             inst    240734892
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                4.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.05
    Achieved Active Warps Per SM           warp        31.39
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.803%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    150444.10
    Total DRAM Elapsed Cycles        cycle    329861120
    Average L1 Active Cycles         cycle   5497233.69
    Total L1 Elapsed Cycles          cycle    627317194
    Average L2 Active Cycles         cycle   5555467.78
    Total L2 Elapsed Cycles          cycle    445046560
    Average SM Active Cycles         cycle   5497233.69
    Total SM Elapsed Cycles          cycle    627317194
    Average SMSP Active Cycles       cycle   5498441.03
    Total SMSP Elapsed Cycles        cycle   2509268776
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.034%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.32% above the average, while the minimum instance value is 15.38% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.066%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.35% above the average, while the minimum instance value is 15.41% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.034%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.32% above the average, while the minimum instance value is 15.38% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.88%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       634282
    Memory Throughput                 %        73.54
    DRAM Throughput                   %         0.77
    Duration                         us       595.68
    L1/TEX Cache Throughput           %        78.33
    L2 Cache Throughput               %         8.07
    SM Active Cycles              cycle    595441.11
    Compute (SM) Throughput           %        49.03
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 12% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.75
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       160000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.50
    Executed Ipc Elapsed  inst/cycle         1.41
    Issue Slots Busy               %        37.61
    Issued Ipc Active     inst/cycle         1.50
    SM Busy                        %        37.61
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.48%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        14.97
    Mem Busy                              %        73.54
    Max Bandwidth                         %        49.27
    L1/TEX Hit Rate                       %        92.55
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        91.86
    Mem Pipes Busy                        %        49.03
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.97%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.65
    Issued Warp Per Scheduler                        0.38
    No Eligible                            %        62.35
    Active Warps Per Scheduler          warp        11.06
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.46%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          11.06 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.37
    Warp Cycles Per Executed Instruction           cycle        29.38
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.46%                                                                                          
          On average, each warp of this kernel spends 15.2 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 51.7% of the total average of 29.4 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    223838.81
    Executed Instructions                           inst     96698368
    Avg. Issued Instructions Per Scheduler          inst    223925.46
    Issued Instructions                             inst     96735797
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                1.19
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 31.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.04
    Achieved Active Warps Per SM           warp        44.19
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.46%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6966.80
    Total DRAM Elapsed Cycles        cycle     36021248
    Average L1 Active Cycles         cycle    595441.11
    Total L1 Elapsed Cycles          cycle     68490050
    Average L2 Active Cycles         cycle    568033.74
    Total L2 Elapsed Cycles          cycle     48593280
    Average SM Active Cycles         cycle    595441.11
    Total SM Elapsed Cycles          cycle     68490050
    Average SMSP Active Cycles       cycle    594706.84
    Total SMSP Elapsed Cycles        cycle    273960200
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.383%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.73% above the average, while the minimum instance value is 16.18% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.656%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 6.03% above the average, while the minimum instance value is 15.83% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.383%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.73% above the average, while the minimum instance value is 16.18% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 17, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle      5809189
    Memory Throughput                 %        91.19
    DRAM Throughput                   %         1.82
    Duration                         ms         5.45
    L1/TEX Cache Throughput           %        96.21
    L2 Cache Throughput               %        42.33
    SM Active Cycles              cycle   5505674.65
    Compute (SM) Throughput           %        16.14
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.88
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       320000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.12
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        10.12
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.68%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        35.26
    Mem Busy                              %        91.19
    Max Bandwidth                         %        40.19
    L1/TEX Hit Rate                       %        65.71
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        95.37
    Mem Pipes Busy                        %        16.14
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.59%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.13
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.87
    Active Warps Per Scheduler          warp         7.84
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.813%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          7.84 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        77.43
    Warp Cycles Per Executed Instruction           cycle        77.43
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.813%                                                                                          
          On average, each warp of this kernel spends 73.3 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.6% of the total average of 77.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    557207.70
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst    557255.70
    Issued Instructions                             inst    240734464
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                4.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        48.98
    Achieved Active Warps Per SM           warp        31.35
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.813%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    150250.80
    Total DRAM Elapsed Cycles        cycle    329896960
    Average L1 Active Cycles         cycle   5505674.65
    Total L1 Elapsed Cycles          cycle    627387696
    Average L2 Active Cycles         cycle   5551138.21
    Total L2 Elapsed Cycles          cycle    445097120
    Average SM Active Cycles         cycle   5505674.65
    Total SM Elapsed Cycles          cycle    627387696
    Average SMSP Active Cycles       cycle   5501272.43
    Total SMSP Elapsed Cycles        cycle   2509550784
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.84%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       635637
    Memory Throughput                 %        73.39
    DRAM Throughput                   %         0.77
    Duration                         us       596.93
    L1/TEX Cache Throughput           %        78.38
    L2 Cache Throughput               %         8.04
    SM Active Cycles              cycle    595016.94
    Compute (SM) Throughput           %        48.92
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 12% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.75
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       160000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.50
    Executed Ipc Elapsed  inst/cycle         1.41
    Issue Slots Busy               %        37.63
    Issued Ipc Active     inst/cycle         1.51
    SM Busy                        %        37.63
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.47%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        14.94
    Mem Busy                              %        73.39
    Max Bandwidth                         %        49.18
    L1/TEX Hit Rate                       %        92.55
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        91.73
    Mem Pipes Busy                        %        48.92
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.87%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.66
    Issued Warp Per Scheduler                        0.38
    No Eligible                            %        62.34
    Active Warps Per Scheduler          warp        11.06
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.61%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          11.06 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.37
    Warp Cycles Per Executed Instruction           cycle        29.38
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.61%                                                                                          
          On average, each warp of this kernel spends 15.2 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 51.7% of the total average of 29.4 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    223838.81
    Executed Instructions                           inst     96698368
    Avg. Issued Instructions Per Scheduler          inst    223920.15
    Issued Instructions                             inst     96733504
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                1.19
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 30.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.08
    Achieved Active Warps Per SM           warp        44.21
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.61%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6966.70
    Total DRAM Elapsed Cycles        cycle     36098560
    Average L1 Active Cycles         cycle    595016.94
    Total L1 Elapsed Cycles          cycle     68634164
    Average L2 Active Cycles         cycle    567002.66
    Total L2 Elapsed Cycles          cycle     48695840
    Average SM Active Cycles         cycle    595016.94
    Total SM Elapsed Cycles          cycle     68634164
    Average SMSP Active Cycles       cycle    594626.10
    Total SMSP Elapsed Cycles        cycle    274536656
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.625%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 6.01% above the average, while the minimum instance value is 16.37% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.427%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.80% above the average, while the minimum instance value is 15.95% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.625%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 6.01% above the average, while the minimum instance value is 16.37% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 15, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle      5803159
    Memory Throughput                 %        91.28
    DRAM Throughput                   %         1.84
    Duration                         ms         5.45
    L1/TEX Cache Throughput           %        96.37
    L2 Cache Throughput               %        41.92
    SM Active Cycles              cycle   5496815.56
    Compute (SM) Throughput           %        16.16
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.88
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       320000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.41
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.14
    Issued Ipc Active     inst/cycle         0.41
    SM Busy                        %        10.14
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.66%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        35.64
    Mem Busy                              %        91.28
    Max Bandwidth                         %        40.67
    L1/TEX Hit Rate                       %        65.92
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.99
    Mem Pipes Busy                        %        16.16
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.64%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.14
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.86
    Active Warps Per Scheduler          warp         7.86
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.718%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          7.86 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        77.50
    Warp Cycles Per Executed Instruction           cycle        77.50
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.718%                                                                                          
          On average, each warp of this kernel spends 73.2 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.4% of the total average of 77.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    557207.70
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst    557255.94
    Issued Instructions                             inst    240734565
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                4.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.10
    Achieved Active Warps Per SM           warp        31.43
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.718%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    151711.50
    Total DRAM Elapsed Cycles        cycle    329554432
    Average L1 Active Cycles         cycle   5496815.56
    Total L1 Elapsed Cycles          cycle    626735458
    Average L2 Active Cycles         cycle   5554733.78
    Total L2 Elapsed Cycles          cycle    444634240
    Average SM Active Cycles         cycle   5496815.56
    Total SM Elapsed Cycles          cycle    626735458
    Average SMSP Active Cycles       cycle   5493258.66
    Total SMSP Elapsed Cycles        cycle   2506941832
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.92%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       635105
    Memory Throughput                 %        73.45
    DRAM Throughput                   %         0.77
    Duration                         us       596.42
    L1/TEX Cache Throughput           %        78.39
    L2 Cache Throughput               %         8.05
    SM Active Cycles              cycle    594942.26
    Compute (SM) Throughput           %        48.96
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 12% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.82
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       160000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.50
    Executed Ipc Elapsed  inst/cycle         1.41
    Issue Slots Busy               %        37.64
    Issued Ipc Active     inst/cycle         1.51
    SM Busy                        %        37.64
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.47%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        14.95
    Mem Busy                              %        73.45
    Max Bandwidth                         %        49.21
    L1/TEX Hit Rate                       %        92.54
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        91.71
    Mem Pipes Busy                        %        48.96
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.9%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.65
    Issued Warp Per Scheduler                        0.38
    No Eligible                            %        62.35
    Active Warps Per Scheduler          warp        11.07
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.55%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          11.07 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.40
    Warp Cycles Per Executed Instruction           cycle        29.41
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.55%                                                                                          
          On average, each warp of this kernel spends 15.2 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 51.7% of the total average of 29.4 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    223838.81
    Executed Instructions                           inst     96698368
    Avg. Issued Instructions Per Scheduler          inst    223921.14
    Issued Instructions                             inst     96733931
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                1.19
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 30.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.15
    Achieved Active Warps Per SM           warp        44.26
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.55%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6966.70
    Total DRAM Elapsed Cycles        cycle     36067840
    Average L1 Active Cycles         cycle    594942.26
    Total L1 Elapsed Cycles          cycle     68579990
    Average L2 Active Cycles         cycle    567489.51
    Total L2 Elapsed Cycles          cycle     48657040
    Average SM Active Cycles         cycle    594942.26
    Total SM Elapsed Cycles          cycle     68579990
    Average SMSP Active Cycles       cycle    594749.93
    Total SMSP Elapsed Cycles        cycle    274319960
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.583%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.96% above the average, while the minimum instance value is 16.10% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.464%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.83% above the average, while the minimum instance value is 16.36% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.583%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.96% above the average, while the minimum instance value is 16.10% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 17, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle      5810911
    Memory Throughput                 %        91.16
    DRAM Throughput                   %         1.85
    Duration                         ms         5.46
    L1/TEX Cache Throughput           %        96.18
    L2 Cache Throughput               %        43.23
    SM Active Cycles              cycle   5507410.30
    Compute (SM) Throughput           %        16.14
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.88
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       320000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.12
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        10.12
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.68%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        35.86
    Mem Busy                              %        91.16
    Max Bandwidth                         %        40.45
    L1/TEX Hit Rate                       %        65.26
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        93.86
    Mem Pipes Busy                        %        16.14
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.58%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.15
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.85
    Active Warps Per Scheduler          warp         7.87
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.84%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          7.87 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        77.53
    Warp Cycles Per Executed Instruction           cycle        77.54
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.84%                                                                                           
          On average, each warp of this kernel spends 73.6 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 95.0% of the total average of 77.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    557207.70
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst    557255.72
    Issued Instructions                             inst    240734469
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                4.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.03
    Achieved Active Warps Per SM           warp        31.38
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.84%                                                                                           
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    152876.10
    Total DRAM Elapsed Cycles        cycle    329995264
    Average L1 Active Cycles         cycle   5507410.30
    Total L1 Elapsed Cycles          cycle    627573774
    Average L2 Active Cycles         cycle   5553095.41
    Total L2 Elapsed Cycles          cycle    445227840
    Average SM Active Cycles         cycle   5507410.30
    Total SM Elapsed Cycles          cycle    627573774
    Average SMSP Active Cycles       cycle   5491326.70
    Total SMSP Elapsed Cycles        cycle   2510295096
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.142%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.44% above the average, while the minimum instance value is 15.32% below the average.      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.84%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       634029
    Memory Throughput                 %        73.57
    DRAM Throughput                   %         0.77
    Duration                         us       595.42
    L1/TEX Cache Throughput           %        78.44
    L2 Cache Throughput               %         8.07
    SM Active Cycles              cycle    594604.08
    Compute (SM) Throughput           %        49.05
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 12% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.75
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       160000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.51
    Executed Ipc Elapsed  inst/cycle         1.41
    Issue Slots Busy               %        37.66
    Issued Ipc Active     inst/cycle         1.51
    SM Busy                        %        37.66
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.46%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        14.98
    Mem Busy                              %        73.57
    Max Bandwidth                         %        49.30
    L1/TEX Hit Rate                       %        92.54
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        91.78
    Mem Pipes Busy                        %        49.05
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.98%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.64
    Issued Warp Per Scheduler                        0.38
    No Eligible                            %        62.36
    Active Warps Per Scheduler          warp        11.05
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.43%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          11.05 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.35
    Warp Cycles Per Executed Instruction           cycle        29.36
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.43%                                                                                          
          On average, each warp of this kernel spends 15.2 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 51.8% of the total average of 29.3 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    223838.81
    Executed Instructions                           inst     96698368
    Avg. Issued Instructions Per Scheduler          inst    223920.17
    Issued Instructions                             inst     96733512
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                1.19
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 30.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.08
    Achieved Active Warps Per SM           warp        44.21
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.43%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6966.70
    Total DRAM Elapsed Cycles        cycle     36008320
    Average L1 Active Cycles         cycle    594604.08
    Total L1 Elapsed Cycles          cycle     68462738
    Average L2 Active Cycles         cycle    567744.91
    Total L2 Elapsed Cycles          cycle     48572240
    Average SM Active Cycles         cycle    594604.08
    Total SM Elapsed Cycles          cycle     68462738
    Average SMSP Active Cycles       cycle    594888.25
    Total SMSP Elapsed Cycles        cycle    273850952
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.49%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.85% above the average, while the minimum instance value is 16.03% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.363%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.72% above the average, while the minimum instance value is 16.43% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.49%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.85% above the average, while the minimum instance value is 16.03% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 15, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle      5808396
    Memory Throughput                 %        91.20
    DRAM Throughput                   %         1.84
    Duration                         ms         5.45
    L1/TEX Cache Throughput           %        96.37
    L2 Cache Throughput               %        42.83
    SM Active Cycles              cycle   5496615.25
    Compute (SM) Throughput           %        16.14
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.88
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       320000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.41
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.14
    Issued Ipc Active     inst/cycle         0.41
    SM Busy                        %        10.14
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.66%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        35.55
    Mem Busy                              %        91.20
    Max Bandwidth                         %        40.81
    L1/TEX Hit Rate                       %        65.92
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        92.91
    Mem Pipes Busy                        %        16.14
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.6%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.14
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.86
    Active Warps Per Scheduler          warp         7.86
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.8%                                                                                      
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          7.86 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        77.50
    Warp Cycles Per Executed Instruction           cycle        77.50
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.8%                                                                                            
          On average, each warp of this kernel spends 73.4 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.7% of the total average of 77.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    557207.70
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst    557255.63
    Issued Instructions                             inst    240734431
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                4.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.11
    Achieved Active Warps Per SM           warp        31.43
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.8%                                                                                            
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    151477.60
    Total DRAM Elapsed Cycles        cycle    329852928
    Average L1 Active Cycles         cycle   5496615.25
    Total L1 Elapsed Cycles          cycle    627301428
    Average L2 Active Cycles         cycle   5553689.21
    Total L2 Elapsed Cycles          cycle    445034560
    Average SM Active Cycles         cycle   5496615.25
    Total SM Elapsed Cycles          cycle    627301428
    Average SMSP Active Cycles       cycle   5497753.13
    Total SMSP Elapsed Cycles        cycle   2509205712
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.04%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.33% above the average, while the minimum instance value is 15.42% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.098%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.39% above the average, while the minimum instance value is 15.37% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.04%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.33% above the average, while the minimum instance value is 15.42% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.87%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       634297
    Memory Throughput                 %        73.54
    DRAM Throughput                   %         0.77
    Duration                         us       595.68
    L1/TEX Cache Throughput           %        78.41
    L2 Cache Throughput               %         8.07
    SM Active Cycles              cycle    594801.56
    Compute (SM) Throughput           %        49.03
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 12% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.75
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       160000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.51
    Executed Ipc Elapsed  inst/cycle         1.41
    Issue Slots Busy               %        37.65
    Issued Ipc Active     inst/cycle         1.51
    SM Busy                        %        37.65
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.47%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        14.97
    Mem Busy                              %        73.54
    Max Bandwidth                         %        49.28
    L1/TEX Hit Rate                       %        92.54
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        91.85
    Mem Pipes Busy                        %        49.03
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.96%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.64
    Issued Warp Per Scheduler                        0.38
    No Eligible                            %        62.36
    Active Warps Per Scheduler          warp        11.05
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.46%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          11.05 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.37
    Warp Cycles Per Executed Instruction           cycle        29.38
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.46%                                                                                          
          On average, each warp of this kernel spends 15.2 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 51.7% of the total average of 29.4 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    223838.81
    Executed Instructions                           inst     96698368
    Avg. Issued Instructions Per Scheduler          inst    223921.14
    Issued Instructions                             inst     96733931
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                1.19
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 30.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.10
    Achieved Active Warps Per SM           warp        44.23
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.46%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6966.70
    Total DRAM Elapsed Cycles        cycle     36022272
    Average L1 Active Cycles         cycle    594801.56
    Total L1 Elapsed Cycles          cycle     68491812
    Average L2 Active Cycles         cycle    567664.03
    Total L2 Elapsed Cycles          cycle     48592640
    Average SM Active Cycles         cycle    594801.56
    Total SM Elapsed Cycles          cycle     68491812
    Average SMSP Active Cycles       cycle    594964.31
    Total SMSP Elapsed Cycles        cycle    273967248
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.516%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.88% above the average, while the minimum instance value is 16.07% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.716%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 6.09% above the average, while the minimum instance value is 16.05% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.516%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.88% above the average, while the minimum instance value is 16.07% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 17, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle      5805803
    Memory Throughput                 %        91.24
    DRAM Throughput                   %         1.82
    Duration                         ms         5.45
    L1/TEX Cache Throughput           %        96.45
    L2 Cache Throughput               %        42.44
    SM Active Cycles              cycle   5492248.27
    Compute (SM) Throughput           %        16.15
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.88
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       320000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.41
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.15
    Issued Ipc Active     inst/cycle         0.41
    SM Busy                        %        10.15
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.66%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        35.27
    Mem Busy                              %        91.24
    Max Bandwidth                         %        40.28
    L1/TEX Hit Rate                       %        65.56
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        95.11
    Mem Pipes Busy                        %        16.15
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.62%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.14
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.86
    Active Warps Per Scheduler          warp         7.88
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.76%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          7.88 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        77.66
    Warp Cycles Per Executed Instruction           cycle        77.67
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.76%                                                                                           
          On average, each warp of this kernel spends 73.6 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.8% of the total average of 77.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    557207.70
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst    557255.71
    Issued Instructions                             inst    240734466
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                4.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.25
    Achieved Active Warps Per SM           warp        31.52
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.76%                                                                                           
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    150229.80
    Total DRAM Elapsed Cycles        cycle    329704960
    Average L1 Active Cycles         cycle   5492248.27
    Total L1 Elapsed Cycles          cycle    627022312
    Average L2 Active Cycles         cycle   5550439.69
    Total L2 Elapsed Cycles          cycle    444838080
    Average SM Active Cycles         cycle   5492248.27
    Total SM Elapsed Cycles          cycle    627022312
    Average SMSP Active Cycles       cycle   5495369.67
    Total SMSP Elapsed Cycles        cycle   2508089248
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.07%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.36% above the average, while the minimum instance value is 15.36% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.07%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.36% above the average, while the minimum instance value is 15.36% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.86%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       634059
    Memory Throughput                 %        73.57
    DRAM Throughput                   %         0.77
    Duration                         us       595.42
    L1/TEX Cache Throughput           %        78.38
    L2 Cache Throughput               %         8.06
    SM Active Cycles              cycle    595028.47
    Compute (SM) Throughput           %        49.05
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 12% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.75
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       160000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.50
    Executed Ipc Elapsed  inst/cycle         1.41
    Issue Slots Busy               %        37.63
    Issued Ipc Active     inst/cycle         1.51
    SM Busy                        %        37.63
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.47%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        14.98
    Mem Busy                              %        73.57
    Max Bandwidth                         %        49.30
    L1/TEX Hit Rate                       %        92.54
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        91.79
    Mem Pipes Busy                        %        49.05
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.98%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.63
    Issued Warp Per Scheduler                        0.38
    No Eligible                            %        62.37
    Active Warps Per Scheduler          warp        11.06
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.43%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          11.06 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.40
    Warp Cycles Per Executed Instruction           cycle        29.41
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.43%                                                                                          
          On average, each warp of this kernel spends 15.2 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 51.7% of the total average of 29.4 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    223838.81
    Executed Instructions                           inst     96698368
    Avg. Issued Instructions Per Scheduler          inst    223920.20
    Issued Instructions                             inst     96733527
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                1.19
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 30.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.15
    Achieved Active Warps Per SM           warp        44.25
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.43%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6970.20
    Total DRAM Elapsed Cycles        cycle     36008960
    Average L1 Active Cycles         cycle    595028.47
    Total L1 Elapsed Cycles          cycle     68463906
    Average L2 Active Cycles         cycle    566592.05
    Total L2 Elapsed Cycles          cycle     48574880
    Average SM Active Cycles         cycle    595028.47
    Total SM Elapsed Cycles          cycle     68463906
    Average SMSP Active Cycles       cycle    595058.21
    Total SMSP Elapsed Cycles        cycle    273855624
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.399%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.75% above the average, while the minimum instance value is 16.33% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.426%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.78% above the average, while the minimum instance value is 15.90% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.399%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.75% above the average, while the minimum instance value is 16.33% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 15, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle      5803173
    Memory Throughput                 %        91.28
    DRAM Throughput                   %         1.83
    Duration                         ms         5.45
    L1/TEX Cache Throughput           %        96.34
    L2 Cache Throughput               %        42.32
    SM Active Cycles              cycle   5498159.43
    Compute (SM) Throughput           %        16.16
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.19
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle        20000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.41
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.14
    Issued Ipc Active     inst/cycle         0.41
    SM Busy                        %        10.14
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.67%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        35.40
    Mem Busy                              %        91.28
    Max Bandwidth                         %        40.06
    L1/TEX Hit Rate                       %        65.32
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.73
    Mem Pipes Busy                        %        16.16
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.64%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.14
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.86
    Active Warps Per Scheduler          warp         7.84
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.718%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          7.84 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        77.33
    Warp Cycles Per Executed Instruction           cycle        77.34
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.718%                                                                                          
          On average, each warp of this kernel spends 73.5 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 95.0% of the total average of 77.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    557207.70
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst    557255.69
    Issued Instructions                             inst    240734460
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                4.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        48.99
    Achieved Active Warps Per SM           warp        31.35
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.718%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    150709.30
    Total DRAM Elapsed Cycles        cycle    329555200
    Average L1 Active Cycles         cycle   5498159.43
    Total L1 Elapsed Cycles          cycle    626737434
    Average L2 Active Cycles         cycle   5546311.66
    Total L2 Elapsed Cycles          cycle    444635840
    Average SM Active Cycles         cycle   5498159.43
    Total SM Elapsed Cycles          cycle    626737434
    Average SMSP Active Cycles       cycle   5496964.38
    Total SMSP Elapsed Cycles        cycle   2506949736
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.013%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.29% above the average, while the minimum instance value is 15.43% below the average.      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.85%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       633728
    Memory Throughput                 %        73.61
    DRAM Throughput                   %         0.77
    Duration                         us       595.14
    L1/TEX Cache Throughput           %        78.35
    L2 Cache Throughput               %         8.07
    SM Active Cycles              cycle    595239.62
    Compute (SM) Throughput           %        49.07
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 12% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.75
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       160000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.50
    Executed Ipc Elapsed  inst/cycle         1.41
    Issue Slots Busy               %        37.62
    Issued Ipc Active     inst/cycle         1.50
    SM Busy                        %        37.62
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.48%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        14.98
    Mem Busy                              %        73.61
    Max Bandwidth                         %        49.33
    L1/TEX Hit Rate                       %        92.55
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        92.04
    Mem Pipes Busy                        %        49.07
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.01%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.61
    Issued Warp Per Scheduler                        0.38
    No Eligible                            %        62.39
    Active Warps Per Scheduler          warp        11.06
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.39%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          11.06 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.40
    Warp Cycles Per Executed Instruction           cycle        29.41
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.39%                                                                                          
          On average, each warp of this kernel spends 15.2 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 51.7% of the total average of 29.4 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    223838.81
    Executed Instructions                           inst     96698368
    Avg. Issued Instructions Per Scheduler          inst    223921.06
    Issued Instructions                             inst     96733897
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                1.19
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 30.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.14
    Achieved Active Warps Per SM           warp        44.25
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.39%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6966.80
    Total DRAM Elapsed Cycles        cycle     35988992
    Average L1 Active Cycles         cycle    595239.62
    Total L1 Elapsed Cycles          cycle     68429194
    Average L2 Active Cycles         cycle    567302.35
    Total L2 Elapsed Cycles          cycle     48550320
    Average SM Active Cycles         cycle    595239.62
    Total SM Elapsed Cycles          cycle     68429194
    Average SMSP Active Cycles       cycle    595302.81
    Total SMSP Elapsed Cycles        cycle    273716776
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.363%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.71% above the average, while the minimum instance value is 16.12% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.355%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.70% above the average, while the minimum instance value is 15.95% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.363%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.71% above the average, while the minimum instance value is 16.12% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 17, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle      5813100
    Memory Throughput                 %        91.13
    DRAM Throughput                   %         1.82
    Duration                         ms         5.46
    L1/TEX Cache Throughput           %        96.31
    L2 Cache Throughput               %        43.20
    SM Active Cycles              cycle   5500226.98
    Compute (SM) Throughput           %        16.13
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.88
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       320000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.41
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.13
    Issued Ipc Active     inst/cycle         0.41
    SM Busy                        %        10.13
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.67%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        35.14
    Mem Busy                              %        91.13
    Max Bandwidth                         %        40.47
    L1/TEX Hit Rate                       %        65.89
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        95.39
    Mem Pipes Busy                        %        16.13
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.56%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.13
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.87
    Active Warps Per Scheduler          warp         7.83
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.874%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          7.83 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        77.28
    Warp Cycles Per Executed Instruction           cycle        77.28
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.874%                                                                                          
          On average, each warp of this kernel spends 73.0 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.5% of the total average of 77.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    557207.70
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst    557255.78
    Issued Instructions                             inst    240734495
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                4.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        48.93
    Achieved Active Warps Per SM           warp        31.32
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.874%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    149848.50
    Total DRAM Elapsed Cycles        cycle    330119808
    Average L1 Active Cycles         cycle   5500226.98
    Total L1 Elapsed Cycles          cycle    627809720
    Average L2 Active Cycles         cycle   5555663.66
    Total L2 Elapsed Cycles          cycle    445394880
    Average SM Active Cycles         cycle   5500226.98
    Total SM Elapsed Cycles          cycle    627809720
    Average SMSP Active Cycles       cycle   5500121.57
    Total SMSP Elapsed Cycles        cycle   2511238880
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.054%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.34% above the average, while the minimum instance value is 15.39% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.037%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.32% above the average, while the minimum instance value is 15.43% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.054%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.34% above the average, while the minimum instance value is 15.39% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.85%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       636421
    Memory Throughput                 %        73.30
    DRAM Throughput                   %         0.77
    Duration                         us       597.66
    L1/TEX Cache Throughput           %        78.37
    L2 Cache Throughput               %         8.04
    SM Active Cycles              cycle    595137.24
    Compute (SM) Throughput           %        48.86
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 12% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.75
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       160000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.50
    Executed Ipc Elapsed  inst/cycle         1.41
    Issue Slots Busy               %        37.63
    Issued Ipc Active     inst/cycle         1.51
    SM Busy                        %        37.63
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.48%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        14.92
    Mem Busy                              %        73.30
    Max Bandwidth                         %        49.11
    L1/TEX Hit Rate                       %        92.54
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        91.91
    Mem Pipes Busy                        %        48.86
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.81%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.65
    Issued Warp Per Scheduler                        0.38
    No Eligible                            %        62.35
    Active Warps Per Scheduler          warp        11.05
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.7%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          11.05 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.36
    Warp Cycles Per Executed Instruction           cycle        29.37
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.7%                                                                                           
          On average, each warp of this kernel spends 15.2 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 51.7% of the total average of 29.4 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    223838.81
    Executed Instructions                           inst     96698368
    Avg. Issued Instructions Per Scheduler          inst    223925.17
    Issued Instructions                             inst     96735672
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                1.19
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 31.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.04
    Achieved Active Warps Per SM           warp        44.19
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.7%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6966.80
    Total DRAM Elapsed Cycles        cycle     36142080
    Average L1 Active Cycles         cycle    595137.24
    Total L1 Elapsed Cycles          cycle     68718056
    Average L2 Active Cycles         cycle    566609.86
    Total L2 Elapsed Cycles          cycle     48752720
    Average SM Active Cycles         cycle    595137.24
    Total SM Elapsed Cycles          cycle     68718056
    Average SMSP Active Cycles       cycle    594714.41
    Total SMSP Elapsed Cycles        cycle    274872224
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.715%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 6.11% above the average, while the minimum instance value is 16.02% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.416%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.79% above the average, while the minimum instance value is 15.68% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.715%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 6.11% above the average, while the minimum instance value is 16.02% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 15, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle      5810450
    Memory Throughput                 %        91.17
    DRAM Throughput                   %         1.82
    Duration                         ms         5.46
    L1/TEX Cache Throughput           %        96.16
    L2 Cache Throughput               %        43.03
    SM Active Cycles              cycle   5508699.08
    Compute (SM) Throughput           %        16.14
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.88
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       320000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.12
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        10.12
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.68%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        35.31
    Mem Busy                              %        91.17
    Max Bandwidth                         %        39.96
    L1/TEX Hit Rate                       %        65.97
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        95.54
    Mem Pipes Busy                        %        16.14
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.58%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.14
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.86
    Active Warps Per Scheduler          warp         7.84
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.833%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          7.84 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        77.31
    Warp Cycles Per Executed Instruction           cycle        77.32
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.833%                                                                                          
          On average, each warp of this kernel spends 73.3 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.8% of the total average of 77.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    557207.70
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst    557255.71
    Issued Instructions                             inst    240734467
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                4.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        48.88
    Achieved Active Warps Per SM           warp        31.28
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.833%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    150522.10
    Total DRAM Elapsed Cycles        cycle    329968896
    Average L1 Active Cycles         cycle   5508699.08
    Total L1 Elapsed Cycles          cycle    627524484
    Average L2 Active Cycles         cycle      5550153
    Total L2 Elapsed Cycles          cycle    445193440
    Average SM Active Cycles         cycle   5508699.08
    Total SM Elapsed Cycles          cycle    627524484
    Average SMSP Active Cycles       cycle   5495612.50
    Total SMSP Elapsed Cycles        cycle   2510097936
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.027%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.31% above the average, while the minimum instance value is 15.38% below the average.      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.82%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       634443
    Memory Throughput                 %        73.53
    DRAM Throughput                   %         0.77
    Duration                         us       595.81
    L1/TEX Cache Throughput           %        78.38
    L2 Cache Throughput               %         8.06
    SM Active Cycles              cycle    595048.80
    Compute (SM) Throughput           %        49.02
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 12% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.75
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       160000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.50
    Executed Ipc Elapsed  inst/cycle         1.41
    Issue Slots Busy               %        37.63
    Issued Ipc Active     inst/cycle         1.51
    SM Busy                        %        37.63
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.47%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        14.97
    Mem Busy                              %        73.53
    Max Bandwidth                         %        49.27
    L1/TEX Hit Rate                       %        92.54
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        91.79
    Mem Pipes Busy                        %        49.02
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.95%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.65
    Issued Warp Per Scheduler                        0.38
    No Eligible                            %        62.35
    Active Warps Per Scheduler          warp        11.07
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.47%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          11.07 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.39
    Warp Cycles Per Executed Instruction           cycle        29.40
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.47%                                                                                          
          On average, each warp of this kernel spends 15.2 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 51.7% of the total average of 29.4 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    223838.81
    Executed Instructions                           inst     96698368
    Avg. Issued Instructions Per Scheduler          inst    223925.36
    Issued Instructions                             inst     96735755
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                1.19
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 30.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.12
    Achieved Active Warps Per SM           warp        44.24
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.47%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6966.80
    Total DRAM Elapsed Cycles        cycle     36030464
    Average L1 Active Cycles         cycle    595048.80
    Total L1 Elapsed Cycles          cycle     68506194
    Average L2 Active Cycles         cycle    565930.19
    Total L2 Elapsed Cycles          cycle     48605680
    Average SM Active Cycles         cycle    595048.80
    Total SM Elapsed Cycles          cycle     68506194
    Average SMSP Active Cycles       cycle    594691.92
    Total SMSP Elapsed Cycles        cycle    274024776
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.487%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.85% above the average, while the minimum instance value is 16.36% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.511%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.88% above the average, while the minimum instance value is 16.24% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.487%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.85% above the average, while the minimum instance value is 16.36% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 17, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle      5797281
    Memory Throughput                 %        91.37
    DRAM Throughput                   %         1.84
    Duration                         ms         5.44
    L1/TEX Cache Throughput           %        96.35
    L2 Cache Throughput               %        43.49
    SM Active Cycles              cycle   5497885.94
    Compute (SM) Throughput           %        16.17
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.88
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       320000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.41
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.14
    Issued Ipc Active     inst/cycle         0.41
    SM Busy                        %        10.14
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.67%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        35.56
    Mem Busy                              %        91.37
    Max Bandwidth                         %        40.73
    L1/TEX Hit Rate                       %        65.79
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        94.94
    Mem Pipes Busy                        %        16.17
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.69%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.13
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.87
    Active Warps Per Scheduler          warp         7.84
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.626%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          7.84 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        77.37
    Warp Cycles Per Executed Instruction           cycle        77.38
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.626%                                                                                          
          On average, each warp of this kernel spends 73.5 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.9% of the total average of 77.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    557207.70
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst    557255.59
    Issued Instructions                             inst    240734413
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                4.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.02
    Achieved Active Warps Per SM           warp        31.37
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.626%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    151237.70
    Total DRAM Elapsed Cycles        cycle    329221888
    Average L1 Active Cycles         cycle   5497885.94
    Total L1 Elapsed Cycles          cycle    626102024
    Average L2 Active Cycles         cycle   5556291.85
    Total L2 Elapsed Cycles          cycle    444183920
    Average SM Active Cycles         cycle   5497885.94
    Total SM Elapsed Cycles          cycle    626102024
    Average SMSP Active Cycles       cycle   5502866.78
    Total SMSP Elapsed Cycles        cycle   2504408096
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.99%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       634405
    Memory Throughput                 %        73.53
    DRAM Throughput                   %         0.77
    Duration                         us       595.78
    L1/TEX Cache Throughput           %        78.35
    L2 Cache Throughput               %         8.06
    SM Active Cycles              cycle    595240.94
    Compute (SM) Throughput           %        49.02
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 12% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.82
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       160000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.50
    Executed Ipc Elapsed  inst/cycle         1.41
    Issue Slots Busy               %        37.62
    Issued Ipc Active     inst/cycle         1.50
    SM Busy                        %        37.62
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.48%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        14.97
    Mem Busy                              %        73.53
    Max Bandwidth                         %        49.26
    L1/TEX Hit Rate                       %        92.54
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        91.87
    Mem Pipes Busy                        %        49.02
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.96%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.66
    Issued Warp Per Scheduler                        0.38
    No Eligible                            %        62.34
    Active Warps Per Scheduler          warp        11.06
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.47%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          11.06 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.37
    Warp Cycles Per Executed Instruction           cycle        29.38
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.47%                                                                                          
          On average, each warp of this kernel spends 15.2 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 51.7% of the total average of 29.4 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    223838.81
    Executed Instructions                           inst     96698368
    Avg. Issued Instructions Per Scheduler          inst    223919.93
    Issued Instructions                             inst     96733411
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                1.19
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 31.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.05
    Achieved Active Warps Per SM           warp        44.19
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.47%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6966.80
    Total DRAM Elapsed Cycles        cycle     36028672
    Average L1 Active Cycles         cycle    595240.94
    Total L1 Elapsed Cycles          cycle     68503336
    Average L2 Active Cycles         cycle    567256.25
    Total L2 Elapsed Cycles          cycle     48599040
    Average SM Active Cycles         cycle    595240.94
    Total SM Elapsed Cycles          cycle     68503336
    Average SMSP Active Cycles       cycle    594636.63
    Total SMSP Elapsed Cycles        cycle    274013344
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.439%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.80% above the average, while the minimum instance value is 16.26% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.414%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.77% above the average, while the minimum instance value is 16.01% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.439%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.80% above the average, while the minimum instance value is 16.26% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 15, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle      5810779
    Memory Throughput                 %        91.16
    DRAM Throughput                   %         1.82
    Duration                         ms         5.46
    L1/TEX Cache Throughput           %        96.22
    L2 Cache Throughput               %        43.30
    SM Active Cycles              cycle   5505376.49
    Compute (SM) Throughput           %        16.14
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.88
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       320000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.40
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.12
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        10.12
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.68%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        35.23
    Mem Busy                              %        91.16
    Max Bandwidth                         %        40.41
    L1/TEX Hit Rate                       %        65.67
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        94.52
    Mem Pipes Busy                        %        16.14
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.58%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.14
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.86
    Active Warps Per Scheduler          warp         7.86
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.838%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          7.86 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        77.55
    Warp Cycles Per Executed Instruction           cycle        77.55
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.838%                                                                                          
          On average, each warp of this kernel spends 73.5 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.8% of the total average of 77.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    557207.70
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst    557255.75
    Issued Instructions                             inst    240734484
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                4.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.06
    Achieved Active Warps Per SM           warp        31.40
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.838%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    150188.90
    Total DRAM Elapsed Cycles        cycle    329987072
    Average L1 Active Cycles         cycle   5505376.49
    Total L1 Elapsed Cycles          cycle    627559682
    Average L2 Active Cycles         cycle   5549054.42
    Total L2 Elapsed Cycles          cycle    445217680
    Average SM Active Cycles         cycle   5505376.49
    Total SM Elapsed Cycles          cycle    627559682
    Average SMSP Active Cycles       cycle   5497164.38
    Total SMSP Elapsed Cycles        cycle   2510238728
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.81%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  hgemm_normal(const __half *, const __half *, float *, int, int) (128, 8, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle       634797
    Memory Throughput                 %        73.49
    DRAM Throughput                   %         0.77
    Duration                         us       596.13
    L1/TEX Cache Throughput           %        78.43
    L2 Cache Throughput               %         8.05
    SM Active Cycles              cycle    594657.95
    Compute (SM) Throughput           %        48.99
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the L1 
          bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes      
          transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or        
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 12% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.75
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       160000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.51
    Executed Ipc Elapsed  inst/cycle         1.41
    Issue Slots Busy               %        37.66
    Issued Ipc Active     inst/cycle         1.51
    SM Busy                        %        37.66
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.46%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        14.96
    Mem Busy                              %        73.49
    Max Bandwidth                         %        49.24
    L1/TEX Hit Rate                       %        92.54
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        91.77
    Mem Pipes Busy                        %        48.99
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.93%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 12.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        37.66
    Issued Warp Per Scheduler                        0.38
    No Eligible                            %        62.34
    Active Warps Per Scheduler          warp        11.07
    Eligible Warps Per Scheduler        warp         1.84
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.51%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          11.07 active warps per scheduler, but only an average of 1.84 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.39
    Warp Cycles Per Executed Instruction           cycle        29.40
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.99
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.51%                                                                                          
          On average, each warp of this kernel spends 15.2 cycles being stalled waiting for the L1 instruction queue    
          for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing  
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 51.7% of the total average of 29.4 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    223838.81
    Executed Instructions                           inst     96698368
    Avg. Issued Instructions Per Scheduler          inst    223920.77
    Issued Instructions                             inst     96733771
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              31
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                1.19
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 159 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 30.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.16
    Achieved Active Warps Per SM           warp        44.27
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.51%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6966.70
    Total DRAM Elapsed Cycles        cycle     36050176
    Average L1 Active Cycles         cycle    594657.95
    Total L1 Elapsed Cycles          cycle     68543518
    Average L2 Active Cycles         cycle    567118.39
    Total L2 Elapsed Cycles          cycle     48631200
    Average SM Active Cycles         cycle    594657.95
    Total SM Elapsed Cycles          cycle     68543518
    Average SMSP Active Cycles       cycle    594640.76
    Total SMSP Elapsed Cycles        cycle    274174072
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.588%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.96% above the average, while the minimum instance value is 16.17% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.442%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.81% above the average, while the minimum instance value is 16.50% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.588%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.96% above the average, while the minimum instance value is 16.17% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1064960
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  hgemm_tensor_core(const __half *, const __half *, float *, int, int) (32, 512, 1)x(32, 1, 1), Context 1, Stream 17, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.51
    SM Frequency                    Ghz         1.06
    Elapsed Cycles                cycle      5806443
    Memory Throughput                 %        91.23
    DRAM Throughput                   %         1.85
    Duration                         ms         5.45
    L1/TEX Cache Throughput           %        96.44
    L2 Cache Throughput               %        41.84
    SM Active Cycles              cycle   5492561.11
    Compute (SM) Throughput           %        16.15
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing L1 in the Memory Workload Analysis section.                                                

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.88
    Dropped Samples                sample            0
    Maximum Sampling Interval       cycle       320000
    # Pass Groups                                    4
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.41
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.15
    Issued Ipc Active     inst/cycle         0.41
    SM Busy                        %        10.15
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.66%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s        35.81
    Mem Busy                              %        91.23
    Max Bandwidth                         %        41.32
    L1/TEX Hit Rate                       %        65.54
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.20
    Mem Pipes Busy                        %        16.15
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 45.62%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        10.15
    Issued Warp Per Scheduler                        0.10
    No Eligible                            %        89.85
    Active Warps Per Scheduler          warp         7.87
    Eligible Warps Per Scheduler        warp         0.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 8.77%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 9.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          7.87 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        77.55
    Warp Cycles Per Executed Instruction           cycle        77.56
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.93
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.77%                                                                                           
          On average, each warp of this kernel spends 73.1 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 94.3% of the total average of 77.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    557207.70
    Executed Instructions                           inst    240713728
    Avg. Issued Instructions Per Scheduler          inst    557255.77
    Issued Instructions                             inst    240734493
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                4.74
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        49.18
    Achieved Active Warps Per SM           warp        31.47
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 8.77%                                                                                           
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM. This kernel's theoretical occupancy (50.0%) is limited by the required amount of shared    
          memory.                                                                                                       

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    152522.20
    Total DRAM Elapsed Cycles        cycle    329742848
    Average L1 Active Cycles         cycle   5492561.11
    Total L1 Elapsed Cycles          cycle    627091484
    Average L2 Active Cycles         cycle      5554874
    Total L2 Elapsed Cycles          cycle    444886720
    Average SM Active Cycles         cycle   5492561.11
    Total SM Elapsed Cycles          cycle    627091484
    Average SMSP Active Cycles       cycle   5492389.28
    Total SMSP Elapsed Cycles        cycle   2508365936
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.074%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.36% above the average, while the minimum instance value is 15.36% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.038%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 5.33% above the average, while the minimum instance value is 15.33% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.074%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.36% above the average, while the minimum instance value is 15.36% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst      1605632
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 49.9%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 268435456 excessive sectors (50% of the   
          total 537395200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

